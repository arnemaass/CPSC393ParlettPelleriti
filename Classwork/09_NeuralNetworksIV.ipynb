{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import keras as kb\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression # Linear Regression Model\n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in NNs\n",
    "\n",
    "\n",
    "In CPSC 392, we talked about Regularization through things like LASSO/Ridge penalties, Bagging in Random Forests, and Limiting `max_depth` or `min_samples_leaf` in Decision Trees. All of these methods aim to make our models *less complex* which can move us from an area of high variance/overfitting, to an area where we are NOT overfitting nor underfitting (our main goal.)\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1akEMD7_STAkqJcZUI5vc3CW5aU5AeTWR\"></center>\n",
    "\n",
    "However, the models we learned in 392 tended to be pretty simple. Deep Neural Networks are typically *NOT* simple. This complexity allows them to learn complicated relationships between our inputs and our outputs. For example, with linear regression, you're limited to linear relationships between inputs and outputs, neural networks allow you to have non-linear relationships, and the deeper the network the more complicated those relationships can be (remember we talked about Universal Function Approximation?)\n",
    "\n",
    "Remember the whole *goal* of Machine Learning is to approximate the function $y = f(x)$ that tells you how to take our inputs $x$ and turn them into an accurate guess at our output(s) $y$. That function is $f(x)$. The more complicated $f(x)$ is, the more flexible/complex our model needs to be to approximate it. Models like regression, decision trees...etc. often aren't flexible enough to solve complicated problems like the one's we'll encounter later in the class. BUT Neural Networks can be!\n",
    "\n",
    "However, we know that when a model is TOO complex/flexible, it can overfit...\n",
    "\n",
    "<center><img src=\"https://jashrathod.github.io/assets/img/optimal-fit.png\" alt = \"from:https://jashrathod.github.io/2021-09-30-underfitting-overfitting-and-regularization/\" width = \"400px\" ></center>\n",
    "\n",
    "\n",
    "So we often need regularization to make sure that our models are not overfitting. Remember that **overfitting is when the model is so complex that it can pick up on noise/quirks in the training data that aren't there in other samples of data** so our goal when regularizing is to prevent this from happening. \n",
    "\n",
    "We talked about *multiple* methods of Regularization for NNs:\n",
    "\n",
    "- Penalization\n",
    "- Bagging or Model Averaging\n",
    "- Dropout\n",
    "- Early Stopping\n",
    "- Batch Normalization\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization in keras\n",
    "\n",
    "## Penalization\n",
    "\n",
    "We can add penalization to any/all our layers using the `kernel_regularizer` argument. While you can create your own custom regularizers, we'll focus on `L1`, `L2` and ElasticNet aka `L1L2`. Let's take the MNIST model we built in Classwork 5 and add penalties to the layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and rescale the data so that the pixel values are between 0-1\n",
    "\n",
    "# load data\n",
    "((trainX, trainY), (testX, testY)) = mnist.load_data()\n",
    "\n",
    "# reshape datta to be individual columns instead of image matrix\n",
    "trainX = trainX.reshape((trainX.shape[0], 28 * 28 * 1))\n",
    "testX = testX.reshape((testX.shape[0], 28 * 28 * 1))\n",
    "\n",
    "# rescale data to be 0-1 instead of 0-255\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "\n",
    "# change the labels to be in the correct format\n",
    "lb = LabelBinarizer()\n",
    "trainY = lb.fit_transform(trainY)\n",
    "testY = lb.transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "# build structure of the model\n",
    "model = kb.Sequential([\n",
    "    kb.layers.Dense(250, input_shape =[784]), #input\n",
    "    kb.layers.Dense(100),\n",
    "    kb.layers.Dense(50),\n",
    "    kb.layers.Dense(30),\n",
    "    kb.layers.Dense(20),\n",
    "    kb.layers.Dense(10),\n",
    "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "#fit the model (same as SKlearn)\n",
    "model.fit(trainX,trainY, epochs = 100, validation_data=(testX, testY))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method of adding a penalty on the weights will use the default **regularization strength**. Remember that regularization strength tells the model how MUCH to penalize large weights. If regularization strength is 0, then there's no penalty. If it's greater than 0, there will be more and more penalization as regularization strength gets bigger. \n",
    "\n",
    "Typically these defaults are pretty good (check them out [here](https://keras.io/api/layers/regularizers/)). BUT if you'd like more control over the regularization strength, you can also pass a `regulizers` object to the `kernel_regularizer` argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "# build structure of the model\n",
    "model = kb.Sequential([\n",
    "    kb.layers.Dense(250, input_shape =[784]), #input\n",
    "    kb.layers.Dense(100),\n",
    "    kb.layers.Dense(50),\n",
    "    kb.layers.Dense(30),\n",
    "    kb.layers.Dense(20),\n",
    "    kb.layers.Dense(10),\n",
    "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "#fit the model (same as SKlearn)\n",
    "model.fit(trainX,trainY, epochs = 100, validation_data=(testX, testY))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging \n",
    "\n",
    "When we use Bagging, our goal is to have many *slightly different* models that we then average together to make a prediction. Traditionaly (like in Random Forests) we implement this by **bootstrapping** samples of data to train on. Bootstrapping is sampling *with replacement* so that each bootstrapped sample may have some data poitns that occur multiple times, and others that do not occur at all. Thus each model is trained on *slightly* different data.\n",
    "\n",
    "This helps prevent overfitting because while each model might overfit to the noise/quirks in the sample it sees, no two models will overfit to the SAME noise/quirks because they're seeing different data. Thus, what the models have in common are the broad/general patterns that apply to *any* sample of data. \n",
    "\n",
    "It turns out that NNs don't *necessarily* need bagging in order to get several *slightly different* networks. NNs are sensitive to things like:\n",
    "\n",
    "- the initial random weights selected before we start training\n",
    "- which points end up in which mini-batch when training\n",
    "- etc...\n",
    "\n",
    "And thus it may be possible to train several networks without bagging and still end up with an ensemble of slightly different networks (you can still use bagging though, but its not easy with Keras).\n",
    "\n",
    "\n",
    "We won't build these too often, so I won't spend time going over it here, but [here's](https://www.adriangb.com/scikeras/stable/notebooks/Meta_Estimators.html#4.-Bagging-ensemble) an example.\n",
    "\n",
    "## Dropout\n",
    "\n",
    "Dropout is one of the most POPULAR methods of regularization in NNs because it is easy to implement, works with pretty much any NN structure, and is very effective at reducing overfitting. Dropout works on the same principles as *Bagging* but is more efficiently implemented.\n",
    "\n",
    "In Dropout we mask--aka set to 0--some of the outputs of a layer before feeding them into the next layer. We do this randomly and it changes each time we take a pass through the network, so as the model is training, it basically never knows which inputs will not be there...so it can't \"put all its eggs in one basket\" so to speak. In other words, the model cannot over-rely on any connections, because sometimes those nodes will just be gone. \n",
    "\n",
    "In keras, [Dropout](https://keras.io/api/layers/regularization_layers/dropout/) is implemented through a `Dropout()` layer, which you can stick between any two layers in your network. When data passes through the Dropout layer, it will randomly set some of the inputs into the `Dropout()` layer to 0 based on the `rate` argument which is a value between 0 and 1 indicating the fraction of nodes in that layer to set to 0/drop. 0 would mean we do not drop any values. 1 means we drop all values. Typically we'll set this value to something like 0.2, 0.3, or 0.5. \n",
    "\n",
    "**Note**: Dropout is only applied during *training*. When making predictions with the model, we do not dropout any nodes. \n",
    "\n",
    " Let's take the MNIST model we built in Classwork 5 and add Dropout to the layers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build structure of the model\n",
    "model = kb.Sequential([\n",
    "    kb.layers.Dense(250, input_shape =[784]), #input\n",
    "    kb.layers.Dense(100),\n",
    "    kb.layers.Dense(50),\n",
    "    kb.layers.Dense(30),\n",
    "    kb.layers.Dense(20),\n",
    "    kb.layers.Dense(10),\n",
    "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "#fit the model (same as SKlearn)\n",
    "model.fit(trainX,trainY, epochs = 100, validation_data=(testX, testY))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping \n",
    "\n",
    "Early Stopping regularizes our model by stopping training once the validation loss no longer improves. Remember that the pattern of overfitting shows that overfitting happens when the training loss continues to improve while the validation loss stops improving or gets worse. \n",
    "\n",
    "In Keras, [Early Stopping](https://keras.io/api/callbacks/early_stopping/) is implemented through a `callback` which is an object that has access to your model information during training and can make changes to the training process during training. \n",
    "\n",
    " Let's take the MNIST model we built in Classwork 5 and add Early Stopping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tells keras to stop training when the validation loss no longer improves for 3 epochs in a row\n",
    "\n",
    "from keras import regularizers\n",
    "\n",
    "# build structure of the model\n",
    "model = kb.Sequential([\n",
    "    kb.layers.Dense(250, input_shape =[784]), #input\n",
    "    kb.layers.Dense(100),\n",
    "    kb.layers.Dense(50),\n",
    "    kb.layers.Dense(30),\n",
    "    kb.layers.Dense(20),\n",
    "    kb.layers.Dense(10),\n",
    "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "#fit the model (same as SKlearn)\n",
    "model.fit(trainX,trainY, epochs = 100, validation_data=(testX, testY))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm\n",
    "\n",
    "Batch Normalization was actually proposed as a way to handle something called \"covariate shift\" but much like many things in Deep Learning, we discovered by accident that it had a regularizing effect on models. We don't really know why it works, but hey if it works, it works. Some propose that the \"noise\" inserted into our model by using the *estimate* of the mean and variance for each batch is what causes this regularization. It also can speed up training and allow us to use larger learning rates because it smooths out the loss surface we're trying to descend. \n",
    "\n",
    "Batch Norm is similar to z-scoring, but we do it *between* layers. This ensures that the input to each layer in our network is stable. \n",
    "\n",
    "Batch Norm first takes the output and centers it based on the estimated mean of the outputs of a layer, scales it based on the estimated variance/standard deviation of the outputs of a layer, and then scaling and shifting it based on learned parameters $\\gamma$ and $\\beta$ which allow the output to have a different mean ($\\beta$) and standard deviation ($\\gamma$) than 0 and 1.\n",
    "\n",
    "Thus we take the outputs $\\mathbf{x}$ and turn them into\n",
    "\n",
    "$$\\beta + \\gamma * \\frac{\\mathbf{x} - \\mu_B}{\\sqrt{(\\sigma_B + \\epsilon)}}$$\n",
    "\n",
    "Where $\\beta$ is the new mean of the outputs and $\\gamma$ is the new standard deviation of the outputs. $\\epsilon$ is just a tiny value that helps prevent us from dividing by 0. \n",
    "\n",
    "$\\gamma$ and $\\beta$ are learned during training through backpropagation/gradient descent just like the weights and biases in our model. \n",
    "\n",
    "In Keras, we can use a [`BatchNormalization()`](https://keras.io/api/layers/normalization_layers/batch_normalization/) layer in order to implement Batch Normalization.  Let's take the MNIST model we built in Classwork 5 and add Batch Normalization to the layers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build structure of the model\n",
    "model = kb.Sequential([\n",
    "    kb.layers.Dense(250, input_shape =[784]), #input\n",
    "    kb.layers.Dense(100),\n",
    "    kb.layers.Dense(50),\n",
    "    kb.layers.Dense(30),\n",
    "    kb.layers.Dense(20),\n",
    "    kb.layers.Dense(10),\n",
    "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "#fit the model (same as SKlearn)\n",
    "model.fit(trainX,trainY, epochs = 100, validation_data=(testX, testY))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Your Own\n",
    "\n",
    "We can use multiple methods of regularization at once (although there are some rules of thumb, like use penalties when your network is small, and Dropout when it is large...). \n",
    "\n",
    "Take the MNIST NN and add multiple methods of regularization as you see fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# build structure of the model\n",
    "model = kb.Sequential([\n",
    "    kb.layers.Dense(250, input_shape =[784]), #input\n",
    "    kb.layers.Dense(100),\n",
    "    kb.layers.Dense(50),\n",
    "    kb.layers.Dense(30),\n",
    "    kb.layers.Dense(20),\n",
    "    kb.layers.Dense(10),\n",
    "    kb.layers.Dense(10, activation = \"softmax\") #output\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=kb.optimizers.SGD(0.01),\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "#fit the model (same as SKlearn)\n",
    "model.fit(trainX,trainY, epochs = 100, validation_data=(testX, testY))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
