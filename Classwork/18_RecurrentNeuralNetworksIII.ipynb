{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "Recurrent architechtures (like the LSTM and GRU) are often used to process sequences of text. But Neural Netoworks can't understand strings; they need numbers. So how do we turn words into numbers?\n",
    "\n",
    "## Standardization\n",
    "\n",
    "We often want to change our string to make the characters more \"standard\". For example by making everything lower case so that `\"The\"` and `\"the\"` aren't counted as different tokens or `\"zoe\"` and `\"z√∂e\"`. We also may want to remove punctuation (unless we want to count them as their own tokens). \n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Then we need to break down our now standardized sequence into tokens. Tokens can be characters (like in our Pride and Prejudice LSTM from last classwork), words (most common), ngrams, or even parts of words!\n",
    "\n",
    "\n",
    "Let's try to process this `text` by hand, then with a `TextVectorization()` layer from keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Fox\n",
    "Socks\n",
    "Box\n",
    "Knox\n",
    "\n",
    "Knox in box.\n",
    "Fox in socks.\n",
    "\n",
    "Knox on fox in socks in box.\n",
    "\n",
    "Socks on Knox and Knox in box.\n",
    "\n",
    "Fox in socks on box on Knox.\n",
    "\n",
    "Chicks with bricks come.\n",
    "Chicks with blocks come.\n",
    "Chicks with bricks and blocks and clocks come.\n",
    "\n",
    "Look, sir.  Look, sir.  Mr. Knox, sir.\n",
    "Let's do tricks with bricks and blocks, sir.\n",
    "Let's do tricks with chicks and clocks, sir.\n",
    "\n",
    "First, I'll make a quick trick brick stack.\n",
    "Then I'll make a quick trick block stack.\n",
    "\n",
    "You can make a quick trick chick stack.\n",
    "You can make a quick trick clock stack.\n",
    "\n",
    "And here's a new trick, Mr. Knox....\n",
    "Socks on chicks and chicks on fox.\n",
    "Fox on clocks on bricks and blocks.\n",
    "Bricks and blocks on Knox on box.\n",
    "\n",
    "Now we come to ticks and tocks, sir.\n",
    "Try to say this Mr. Knox, sir....\n",
    "\n",
    "Clocks on fox tick.\n",
    "Clocks on Knox tock.\n",
    "Six sick bricks tick.\n",
    "Six sick chicks tock.\n",
    "\n",
    "Please, sir.  I don't like this trick, sir.\n",
    "My tongue isn't quick or slick, sir.\n",
    "I get all those ticks and clocks, sir, \n",
    "mixed up with the chicks and tocks, sir.\n",
    "I can't do it, Mr. Fox, sir.\n",
    "\n",
    "I'm so sorry, Mr. Knox, sir.\n",
    "\n",
    "Here's an easy game to play.\n",
    "Here's an easy thing to say....\n",
    "\n",
    "New socks.\n",
    "Two socks.\n",
    "Whose socks?\n",
    "Sue's socks.\n",
    "\n",
    "Who sews whose socks?\n",
    "Sue sews Sue's socks.\n",
    "\n",
    "Who sees who sew whose new socks, sir?\n",
    "You see Sue sew Sue's new socks, sir.\n",
    "\n",
    "That's not easy, Mr. Fox, sir.\n",
    "\n",
    "Who comes? ...\n",
    "Crow comes.\n",
    "Slow Joe Crow comes.\n",
    "\n",
    "Who sews crow's clothes?\n",
    "Sue sews crow's clothes.\n",
    "Slow Joe Crow sews whose clothes?\n",
    "Sue's clothes.\n",
    "\n",
    "Sue sews socks of fox in socks now.\n",
    "\n",
    "Slow Joe Crow sews Knox in box now.\n",
    "\n",
    "Sue sews rose on Slow Joe Crow's clothes.\n",
    "Fox sews hose on Slow Joe Crow's nose.\n",
    "\n",
    "Hose goes.\n",
    "Rose grows.\n",
    "Nose hose goes some.\n",
    "Crow's rose grows some.\n",
    "\n",
    "Mr. Fox!\n",
    "I hate this game, sir.\n",
    "This game makes my tongue quite lame, sir.\n",
    "\n",
    "Mr. Knox, sir, what a shame, sir.\n",
    "\n",
    "We'll find something new to do now.\n",
    "Here is lots of new blue goo now.\n",
    "New goo.  Blue goo.\n",
    "Gooey.  Gooey.\n",
    "Blue goo.  New goo.\n",
    "Gluey. Gluey.\n",
    "\n",
    "Gooey goo for chewy chewing!\n",
    "That's what that Goo-Goose is doing.\n",
    "Do you choose to chew goo, too, sir?\n",
    "If, sir, you, sir, choose to chew, sir, \n",
    "with the Goo-Goose, chew, sir.\n",
    "Do, sir.\n",
    "\n",
    "Mr. Fox, sir, \n",
    "I won't do it.  \n",
    "I can't say.  \n",
    "I won't chew it.\n",
    "\n",
    "Very well, sir.\n",
    "Step this way.\n",
    "We'll find another game to play.\n",
    "\n",
    "Bim comes.\n",
    "Ben comes.\n",
    "Bim brings Ben broom.\n",
    "Ben brings Bim broom.\n",
    "\n",
    "Ben bends Bim's broom.\n",
    "Bim bends Ben's broom.\n",
    "Bim's bends.\n",
    "Ben's bends.\n",
    "Ben's bent broom breaks.\n",
    "Bim's bent broom breaks.\n",
    "\n",
    "Ben's band.  Bim's band.\n",
    "Big bands.  Pig bands.\n",
    "\n",
    "Bim and Ben lead bands with brooms.\n",
    "Ben's band bangs and Bim's band booms.\n",
    "\n",
    "Pig band!  Boom band!\n",
    "Big band!  Broom band!\n",
    "My poor mouth can't say that.  No, sir.\n",
    "My poor mouth is much too slow, sir.\n",
    "\n",
    "Well then... bring your mouth this way.\n",
    "I'll find it something it can say.\n",
    "\n",
    "Luke Luck likes lakes.\n",
    "Luke's duck likes lakes.\n",
    "Luke Luck licks lakes.\n",
    "Luck's duck licks lakes.\n",
    "\n",
    "Duck takes licks in lakes Luke Luck likes.\n",
    "Luke Luck takes licks in lakes duck likes.\n",
    "\n",
    "I can't blab such blibber blubber!\n",
    "My tongue isn't make of rubber.\n",
    "\n",
    "Mr. Knox.  Now come now.  Come now.\n",
    "You don't have to be so dumb now....\n",
    "\n",
    "Try to say this, Mr. Knox, please....\n",
    "\n",
    "Through three cheese trees three free fleas flew.\n",
    "While these fleas flew, freezy breeze blew.\n",
    "Freezy breeze made these three trees freeze.\n",
    "Freezy trees made these trees' cheese freeze.\n",
    "That's what made these three free fleas sneeze.\n",
    "\n",
    "Stop it!  Stop it!\n",
    "That's enough, sir.\n",
    "I can't say such silly stuff, sir.\n",
    "\n",
    "Very well, then, Mr. Knox, sir.\n",
    "\n",
    "Let's have a little talk about tweetle beetles....\n",
    "\n",
    "What do you know about tweetle beetles?  Well...\n",
    "\n",
    "When tweetle beetles fight, \n",
    "it's called a tweetle beetle battle.\n",
    "\n",
    "And when they battle in a puddle, \n",
    "it's a tweetle beetle puddle battle.\n",
    "\n",
    "AND when tweetle beetles battle with paddles in a puddle, \n",
    "they call it a tweetle beetle puddle paddle battle.\n",
    "\n",
    "AND...\n",
    "\n",
    "When beetles battle beetles in a puddle paddle battle \n",
    "and the beetle battle puddle is a puddle in a bottle...\n",
    "...they call this a tweetle beetle bottle puddle paddle battle muddle.\n",
    "\n",
    "AND...\n",
    "\n",
    "When beetles fight these battles in a bottle with their paddles \n",
    "and the bottle's on a poodle and the poodle's eating noodles...\n",
    "...they call this a muddle puddle tweetle poodle beetle noodle \n",
    "bottle paddle battle.\n",
    "\n",
    "AND...\n",
    "\n",
    "Now wait a minute, Mr. Socks Fox!\n",
    "\n",
    "When a fox is in the bottle where the tweetle beetles battle \n",
    "with their paddles in a puddle on a noodle-eating poodle, \n",
    "THIS is what they call...\n",
    "\n",
    "...a tweetle beetle noodle poodle bottled paddled \n",
    "muddled duddled fuddled wuddled fox in socks, sir!\n",
    "\n",
    "Fox in socks, our game is done, sir.\n",
    "Thank you for a lot of fun, sir.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 1,\n",
       " b'[UNK]': 0,\n",
       " b'a': 2,\n",
       " b'about': 3,\n",
       " b'all': 4,\n",
       " b'an': 5,\n",
       " b'and': 6,\n",
       " b'another': 7,\n",
       " b'band': 8,\n",
       " b'bands': 9,\n",
       " b'bangs': 10,\n",
       " b'battle': 11,\n",
       " b'battles': 12,\n",
       " b'be': 13,\n",
       " b'beetle': 14,\n",
       " b'beetles': 15,\n",
       " b'ben': 16,\n",
       " b'bends': 17,\n",
       " b'bens': 18,\n",
       " b'bent': 19,\n",
       " b'big': 20,\n",
       " b'bim': 21,\n",
       " b'bims': 22,\n",
       " b'blab': 23,\n",
       " b'blew': 24,\n",
       " b'blibber': 25,\n",
       " b'block': 26,\n",
       " b'blocks': 27,\n",
       " b'blubber': 28,\n",
       " b'blue': 29,\n",
       " b'boom': 30,\n",
       " b'booms': 31,\n",
       " b'bottle': 32,\n",
       " b'bottled': 33,\n",
       " b'bottles': 34,\n",
       " b'box': 35,\n",
       " b'breaks': 36,\n",
       " b'breeze': 37,\n",
       " b'brick': 38,\n",
       " b'bricks': 39,\n",
       " b'bring': 40,\n",
       " b'brings': 41,\n",
       " b'broom': 42,\n",
       " b'brooms': 43,\n",
       " b'call': 44,\n",
       " b'called': 45,\n",
       " b'can': 46,\n",
       " b'cant': 47,\n",
       " b'cheese': 48,\n",
       " b'chew': 49,\n",
       " b'chewing': 50,\n",
       " b'chewy': 51,\n",
       " b'chick': 52,\n",
       " b'chicks': 53,\n",
       " b'choose': 54,\n",
       " b'clock': 55,\n",
       " b'clocks': 56,\n",
       " b'clothes': 57,\n",
       " b'come': 58,\n",
       " b'comes': 59,\n",
       " b'crow': 60,\n",
       " b'crows': 61,\n",
       " b'do': 62,\n",
       " b'doing': 63,\n",
       " b'done': 64,\n",
       " b'dont': 65,\n",
       " b'duck': 66,\n",
       " b'duddled': 67,\n",
       " b'dumb': 68,\n",
       " b'easy': 69,\n",
       " b'eating': 70,\n",
       " b'enough': 71,\n",
       " b'fight': 72,\n",
       " b'find': 73,\n",
       " b'first': 74,\n",
       " b'fleas': 75,\n",
       " b'flew': 76,\n",
       " b'for': 77,\n",
       " b'fox': 78,\n",
       " b'free': 79,\n",
       " b'freeze': 80,\n",
       " b'freezy': 81,\n",
       " b'fuddled': 82,\n",
       " b'fun': 83,\n",
       " b'game': 84,\n",
       " b'get': 85,\n",
       " b'gluey': 86,\n",
       " b'goes': 87,\n",
       " b'goo': 88,\n",
       " b'gooey': 89,\n",
       " b'googoose': 90,\n",
       " b'grows': 91,\n",
       " b'hate': 92,\n",
       " b'have': 93,\n",
       " b'here': 94,\n",
       " b'heres': 95,\n",
       " b'hose': 96,\n",
       " b'i': 97,\n",
       " b'if': 98,\n",
       " b'ill': 99,\n",
       " b'im': 100,\n",
       " b'in': 101,\n",
       " b'is': 102,\n",
       " b'isnt': 103,\n",
       " b'it': 104,\n",
       " b'its': 105,\n",
       " b'joe': 106,\n",
       " b'know': 107,\n",
       " b'knox': 108,\n",
       " b'lakes': 109,\n",
       " b'lame': 110,\n",
       " b'lead': 111,\n",
       " b'lets': 112,\n",
       " b'licks': 113,\n",
       " b'like': 114,\n",
       " b'likes': 115,\n",
       " b'little': 116,\n",
       " b'look': 117,\n",
       " b'lot': 118,\n",
       " b'lots': 119,\n",
       " b'luck': 120,\n",
       " b'lucks': 121,\n",
       " b'luke': 122,\n",
       " b'lukes': 123,\n",
       " b'made': 124,\n",
       " b'make': 125,\n",
       " b'makes': 126,\n",
       " b'minute': 127,\n",
       " b'mixed': 128,\n",
       " b'mouth': 129,\n",
       " b'mr': 130,\n",
       " b'much': 131,\n",
       " b'muddle': 132,\n",
       " b'muddled': 133,\n",
       " b'my': 134,\n",
       " b'new': 135,\n",
       " b'no': 136,\n",
       " b'noodle': 137,\n",
       " b'noodleeating': 138,\n",
       " b'noodles': 139,\n",
       " b'nose': 140,\n",
       " b'not': 141,\n",
       " b'now': 142,\n",
       " b'of': 143,\n",
       " b'on': 144,\n",
       " b'or': 145,\n",
       " b'our': 146,\n",
       " b'paddle': 147,\n",
       " b'paddled': 148,\n",
       " b'paddles': 149,\n",
       " b'pig': 150,\n",
       " b'play': 151,\n",
       " b'please': 152,\n",
       " b'poodle': 153,\n",
       " b'poodles': 154,\n",
       " b'poor': 155,\n",
       " b'puddle': 156,\n",
       " b'quick': 157,\n",
       " b'quite': 158,\n",
       " b'rose': 159,\n",
       " b'rubber': 160,\n",
       " b'say': 161,\n",
       " b'see': 162,\n",
       " b'sees': 163,\n",
       " b'sew': 164,\n",
       " b'sews': 165,\n",
       " b'shame': 166,\n",
       " b'sick': 167,\n",
       " b'silly': 168,\n",
       " b'sir': 169,\n",
       " b'six': 170,\n",
       " b'slick': 171,\n",
       " b'slow': 172,\n",
       " b'sneeze': 173,\n",
       " b'so': 174,\n",
       " b'socks': 175,\n",
       " b'some': 176,\n",
       " b'something': 177,\n",
       " b'sorry': 178,\n",
       " b'stack': 179,\n",
       " b'step': 180,\n",
       " b'stop': 181,\n",
       " b'stuff': 182,\n",
       " b'such': 183,\n",
       " b'sue': 184,\n",
       " b'sues': 185,\n",
       " b'takes': 186,\n",
       " b'talk': 187,\n",
       " b'thank': 188,\n",
       " b'that': 189,\n",
       " b'thats': 190,\n",
       " b'the': 191,\n",
       " b'their': 192,\n",
       " b'then': 193,\n",
       " b'these': 194,\n",
       " b'they': 195,\n",
       " b'thing': 196,\n",
       " b'this': 197,\n",
       " b'those': 198,\n",
       " b'three': 199,\n",
       " b'through': 200,\n",
       " b'tick': 201,\n",
       " b'ticks': 202,\n",
       " b'to': 203,\n",
       " b'tock': 204,\n",
       " b'tocks': 205,\n",
       " b'tongue': 206,\n",
       " b'too': 207,\n",
       " b'trees': 208,\n",
       " b'trick': 209,\n",
       " b'tricks': 210,\n",
       " b'try': 211,\n",
       " b'tweetle': 212,\n",
       " b'two': 213,\n",
       " b'up': 214,\n",
       " b'very': 215,\n",
       " b'wait': 216,\n",
       " b'way': 217,\n",
       " b'we': 218,\n",
       " b'well': 219,\n",
       " b'what': 220,\n",
       " b'when': 221,\n",
       " b'where': 222,\n",
       " b'while': 223,\n",
       " b'who': 224,\n",
       " b'whose': 225,\n",
       " b'with': 226,\n",
       " b'wont': 227,\n",
       " b'wuddled': 228,\n",
       " b'you': 229,\n",
       " b'your': 230}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standardize \n",
    "\n",
    "import re\n",
    "import string\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# put text into list of lines\n",
    "text_list = text.split(\"\\n\")\n",
    "text_list = [sub for sub in text_list if len(sub) > 0]\n",
    "\n",
    "# make everything lowercase\n",
    "text_list = tf.strings.lower(text_list)\n",
    "\n",
    "# replace all puncuation with nothing\n",
    "text_list = tf.strings.regex_replace(text_list,\n",
    "            f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "# split into word level tokens\n",
    "text_list = tf.strings.split(text_list)\n",
    "\n",
    "# get vocabulary\n",
    "vocab = np.unique(np.hstack(text_list))\n",
    "\n",
    "vocab_d = {1: \"\", 0: b\"[UNK]\"}\n",
    "for i, j in enumerate(vocab):\n",
    "    vocab_d[i+2] = j\n",
    "\n",
    "vocab_inv_d = {v: k for k, v in vocab_d.items()}\n",
    "\n",
    "vocab_inv_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[191, 78, 12, 1]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"The Fox Battles Chelsea\"\n",
    "# make everything lowercase\n",
    "s = tf.strings.lower(s)\n",
    "\n",
    "# replace all puncuation with nothing\n",
    "s = tf.strings.regex_replace(s,\n",
    "            f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "# split into word level tokens\n",
    "s = tf.strings.split(s)\n",
    "\n",
    "encoding = [vocab_inv_d[word] if word in vocab_inv_d else 1 for word in s.numpy()]\n",
    "\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the fox battles [UNK]'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoding = [vocab_d[i].decode() for i in encoding]\n",
    "\n",
    "\" \".join(decoding)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "\n",
    "Now that we understand what's going on, let's do all of this using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'sir',\n",
       " 'a',\n",
       " 'and',\n",
       " 'socks',\n",
       " 'in',\n",
       " 'knox',\n",
       " 'fox',\n",
       " 'on',\n",
       " 'mr',\n",
       " 'with',\n",
       " 'tweetle',\n",
       " 'battle',\n",
       " 'to',\n",
       " 'this',\n",
       " 'puddle',\n",
       " 'now',\n",
       " 'sews',\n",
       " 'i',\n",
       " 'you',\n",
       " 'new',\n",
       " 'it',\n",
       " 'do',\n",
       " 'chicks',\n",
       " 'beetles',\n",
       " 'band',\n",
       " 'the',\n",
       " 'say',\n",
       " 'is',\n",
       " 'goo',\n",
       " 'broom',\n",
       " 'box',\n",
       " 'beetle',\n",
       " 'when',\n",
       " 'well',\n",
       " 'trick',\n",
       " 'slow',\n",
       " 'lakes',\n",
       " 'come',\n",
       " 'clocks',\n",
       " 'bricks',\n",
       " 'who',\n",
       " 'what',\n",
       " 'they',\n",
       " 'these',\n",
       " 'sue',\n",
       " 'quick',\n",
       " 'my',\n",
       " 'make',\n",
       " 'joe',\n",
       " 'game',\n",
       " 'crows',\n",
       " 'comes',\n",
       " 'clothes',\n",
       " 'cant',\n",
       " 'bottle',\n",
       " 'blocks',\n",
       " 'bims',\n",
       " 'bim',\n",
       " 'bens',\n",
       " 'ben',\n",
       " 'whose',\n",
       " 'trees',\n",
       " 'three',\n",
       " 'thats',\n",
       " 'sues',\n",
       " 'stack',\n",
       " 'poodle',\n",
       " 'paddle',\n",
       " 'of',\n",
       " 'luke',\n",
       " 'luck',\n",
       " 'likes',\n",
       " 'licks',\n",
       " 'duck',\n",
       " 'crow',\n",
       " 'chew',\n",
       " 'call',\n",
       " 'bends',\n",
       " 'tongue',\n",
       " 'then',\n",
       " 'rose',\n",
       " 'paddles',\n",
       " 'mouth',\n",
       " 'made',\n",
       " 'lets',\n",
       " 'ill',\n",
       " 'hose',\n",
       " 'heres',\n",
       " 'gooey',\n",
       " 'freezy',\n",
       " 'fleas',\n",
       " 'find',\n",
       " 'easy',\n",
       " 'can',\n",
       " 'blue',\n",
       " 'bands',\n",
       " 'wont',\n",
       " 'way',\n",
       " 'very',\n",
       " 'try',\n",
       " 'tricks',\n",
       " 'too',\n",
       " 'tocks',\n",
       " 'tock',\n",
       " 'ticks',\n",
       " 'tick',\n",
       " 'their',\n",
       " 'that',\n",
       " 'takes',\n",
       " 'such',\n",
       " 'stop',\n",
       " 'something',\n",
       " 'some',\n",
       " 'so',\n",
       " 'six',\n",
       " 'sick',\n",
       " 'sew',\n",
       " 'poor',\n",
       " 'please',\n",
       " 'play',\n",
       " 'pig',\n",
       " 'nose',\n",
       " 'noodle',\n",
       " 'muddle',\n",
       " 'look',\n",
       " 'its',\n",
       " 'isnt',\n",
       " 'have',\n",
       " 'grows',\n",
       " 'googoose',\n",
       " 'goes',\n",
       " 'gluey',\n",
       " 'freeze',\n",
       " 'free',\n",
       " 'for',\n",
       " 'flew',\n",
       " 'fight',\n",
       " 'dont',\n",
       " 'choose',\n",
       " 'cheese',\n",
       " 'brings',\n",
       " 'breeze',\n",
       " 'breaks',\n",
       " 'big',\n",
       " 'bent',\n",
       " 'an',\n",
       " 'about',\n",
       " 'your',\n",
       " 'wuddled',\n",
       " 'while',\n",
       " 'where',\n",
       " 'we',\n",
       " 'wait',\n",
       " 'up',\n",
       " 'two',\n",
       " 'through',\n",
       " 'those',\n",
       " 'thing',\n",
       " 'thank',\n",
       " 'talk',\n",
       " 'stuff',\n",
       " 'step',\n",
       " 'sorry',\n",
       " 'sneeze',\n",
       " 'slick',\n",
       " 'silly',\n",
       " 'shame',\n",
       " 'sees',\n",
       " 'see',\n",
       " 'rubber',\n",
       " 'quite',\n",
       " 'poodles',\n",
       " 'paddled',\n",
       " 'our',\n",
       " 'or',\n",
       " 'not',\n",
       " 'noodles',\n",
       " 'noodleeating',\n",
       " 'no',\n",
       " 'muddled',\n",
       " 'much',\n",
       " 'mixed',\n",
       " 'minute',\n",
       " 'makes',\n",
       " 'lukes',\n",
       " 'lucks',\n",
       " 'lots',\n",
       " 'lot',\n",
       " 'little',\n",
       " 'like',\n",
       " 'lead',\n",
       " 'lame',\n",
       " 'know',\n",
       " 'im',\n",
       " 'if',\n",
       " 'here',\n",
       " 'hate',\n",
       " 'get',\n",
       " 'fun',\n",
       " 'fuddled',\n",
       " 'first',\n",
       " 'enough',\n",
       " 'eating',\n",
       " 'dumb',\n",
       " 'duddled',\n",
       " 'done',\n",
       " 'doing',\n",
       " 'clock',\n",
       " 'chick',\n",
       " 'chewy',\n",
       " 'chewing',\n",
       " 'called',\n",
       " 'brooms',\n",
       " 'bring',\n",
       " 'brick',\n",
       " 'bottles',\n",
       " 'bottled',\n",
       " 'booms',\n",
       " 'boom',\n",
       " 'blubber',\n",
       " 'block',\n",
       " 'blibber',\n",
       " 'blew',\n",
       " 'blab',\n",
       " 'be',\n",
       " 'battles',\n",
       " 'bangs',\n",
       " 'another',\n",
       " 'all']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put text into list of lines\n",
    "text_list = text.split(\"\\n\")\n",
    "text_list = [sub for sub in text_list if len(sub) > 0]\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    output_mode = \"int\",\n",
    "    standardize = \"lower_and_strip_punctuation\",\n",
    "    split = \"whitespace\"\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(text_list)\n",
    "text_vectorization.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([ 27,   8, 227,   1])>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = text_vectorization(\"The Fox Battles Chelsea\")\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: '[UNK]',\n",
       " 2: 'sir',\n",
       " 3: 'a',\n",
       " 4: 'and',\n",
       " 5: 'socks',\n",
       " 6: 'in',\n",
       " 7: 'knox',\n",
       " 8: 'fox',\n",
       " 9: 'on',\n",
       " 10: 'mr',\n",
       " 11: 'with',\n",
       " 12: 'tweetle',\n",
       " 13: 'battle',\n",
       " 14: 'to',\n",
       " 15: 'this',\n",
       " 16: 'puddle',\n",
       " 17: 'now',\n",
       " 18: 'sews',\n",
       " 19: 'i',\n",
       " 20: 'you',\n",
       " 21: 'new',\n",
       " 22: 'it',\n",
       " 23: 'do',\n",
       " 24: 'chicks',\n",
       " 25: 'beetles',\n",
       " 26: 'band',\n",
       " 27: 'the',\n",
       " 28: 'say',\n",
       " 29: 'is',\n",
       " 30: 'goo',\n",
       " 31: 'broom',\n",
       " 32: 'box',\n",
       " 33: 'beetle',\n",
       " 34: 'when',\n",
       " 35: 'well',\n",
       " 36: 'trick',\n",
       " 37: 'slow',\n",
       " 38: 'lakes',\n",
       " 39: 'come',\n",
       " 40: 'clocks',\n",
       " 41: 'bricks',\n",
       " 42: 'who',\n",
       " 43: 'what',\n",
       " 44: 'they',\n",
       " 45: 'these',\n",
       " 46: 'sue',\n",
       " 47: 'quick',\n",
       " 48: 'my',\n",
       " 49: 'make',\n",
       " 50: 'joe',\n",
       " 51: 'game',\n",
       " 52: 'crows',\n",
       " 53: 'comes',\n",
       " 54: 'clothes',\n",
       " 55: 'cant',\n",
       " 56: 'bottle',\n",
       " 57: 'blocks',\n",
       " 58: 'bims',\n",
       " 59: 'bim',\n",
       " 60: 'bens',\n",
       " 61: 'ben',\n",
       " 62: 'whose',\n",
       " 63: 'trees',\n",
       " 64: 'three',\n",
       " 65: 'thats',\n",
       " 66: 'sues',\n",
       " 67: 'stack',\n",
       " 68: 'poodle',\n",
       " 69: 'paddle',\n",
       " 70: 'of',\n",
       " 71: 'luke',\n",
       " 72: 'luck',\n",
       " 73: 'likes',\n",
       " 74: 'licks',\n",
       " 75: 'duck',\n",
       " 76: 'crow',\n",
       " 77: 'chew',\n",
       " 78: 'call',\n",
       " 79: 'bends',\n",
       " 80: 'tongue',\n",
       " 81: 'then',\n",
       " 82: 'rose',\n",
       " 83: 'paddles',\n",
       " 84: 'mouth',\n",
       " 85: 'made',\n",
       " 86: 'lets',\n",
       " 87: 'ill',\n",
       " 88: 'hose',\n",
       " 89: 'heres',\n",
       " 90: 'gooey',\n",
       " 91: 'freezy',\n",
       " 92: 'fleas',\n",
       " 93: 'find',\n",
       " 94: 'easy',\n",
       " 95: 'can',\n",
       " 96: 'blue',\n",
       " 97: 'bands',\n",
       " 98: 'wont',\n",
       " 99: 'way',\n",
       " 100: 'very',\n",
       " 101: 'try',\n",
       " 102: 'tricks',\n",
       " 103: 'too',\n",
       " 104: 'tocks',\n",
       " 105: 'tock',\n",
       " 106: 'ticks',\n",
       " 107: 'tick',\n",
       " 108: 'their',\n",
       " 109: 'that',\n",
       " 110: 'takes',\n",
       " 111: 'such',\n",
       " 112: 'stop',\n",
       " 113: 'something',\n",
       " 114: 'some',\n",
       " 115: 'so',\n",
       " 116: 'six',\n",
       " 117: 'sick',\n",
       " 118: 'sew',\n",
       " 119: 'poor',\n",
       " 120: 'please',\n",
       " 121: 'play',\n",
       " 122: 'pig',\n",
       " 123: 'nose',\n",
       " 124: 'noodle',\n",
       " 125: 'muddle',\n",
       " 126: 'look',\n",
       " 127: 'its',\n",
       " 128: 'isnt',\n",
       " 129: 'have',\n",
       " 130: 'grows',\n",
       " 131: 'googoose',\n",
       " 132: 'goes',\n",
       " 133: 'gluey',\n",
       " 134: 'freeze',\n",
       " 135: 'free',\n",
       " 136: 'for',\n",
       " 137: 'flew',\n",
       " 138: 'fight',\n",
       " 139: 'dont',\n",
       " 140: 'choose',\n",
       " 141: 'cheese',\n",
       " 142: 'brings',\n",
       " 143: 'breeze',\n",
       " 144: 'breaks',\n",
       " 145: 'big',\n",
       " 146: 'bent',\n",
       " 147: 'an',\n",
       " 148: 'about',\n",
       " 149: 'your',\n",
       " 150: 'wuddled',\n",
       " 151: 'while',\n",
       " 152: 'where',\n",
       " 153: 'we',\n",
       " 154: 'wait',\n",
       " 155: 'up',\n",
       " 156: 'two',\n",
       " 157: 'through',\n",
       " 158: 'those',\n",
       " 159: 'thing',\n",
       " 160: 'thank',\n",
       " 161: 'talk',\n",
       " 162: 'stuff',\n",
       " 163: 'step',\n",
       " 164: 'sorry',\n",
       " 165: 'sneeze',\n",
       " 166: 'slick',\n",
       " 167: 'silly',\n",
       " 168: 'shame',\n",
       " 169: 'sees',\n",
       " 170: 'see',\n",
       " 171: 'rubber',\n",
       " 172: 'quite',\n",
       " 173: 'poodles',\n",
       " 174: 'paddled',\n",
       " 175: 'our',\n",
       " 176: 'or',\n",
       " 177: 'not',\n",
       " 178: 'noodles',\n",
       " 179: 'noodleeating',\n",
       " 180: 'no',\n",
       " 181: 'muddled',\n",
       " 182: 'much',\n",
       " 183: 'mixed',\n",
       " 184: 'minute',\n",
       " 185: 'makes',\n",
       " 186: 'lukes',\n",
       " 187: 'lucks',\n",
       " 188: 'lots',\n",
       " 189: 'lot',\n",
       " 190: 'little',\n",
       " 191: 'like',\n",
       " 192: 'lead',\n",
       " 193: 'lame',\n",
       " 194: 'know',\n",
       " 195: 'im',\n",
       " 196: 'if',\n",
       " 197: 'here',\n",
       " 198: 'hate',\n",
       " 199: 'get',\n",
       " 200: 'fun',\n",
       " 201: 'fuddled',\n",
       " 202: 'first',\n",
       " 203: 'enough',\n",
       " 204: 'eating',\n",
       " 205: 'dumb',\n",
       " 206: 'duddled',\n",
       " 207: 'done',\n",
       " 208: 'doing',\n",
       " 209: 'clock',\n",
       " 210: 'chick',\n",
       " 211: 'chewy',\n",
       " 212: 'chewing',\n",
       " 213: 'called',\n",
       " 214: 'brooms',\n",
       " 215: 'bring',\n",
       " 216: 'brick',\n",
       " 217: 'bottles',\n",
       " 218: 'bottled',\n",
       " 219: 'booms',\n",
       " 220: 'boom',\n",
       " 221: 'blubber',\n",
       " 222: 'block',\n",
       " 223: 'blibber',\n",
       " 224: 'blew',\n",
       " 225: 'blab',\n",
       " 226: 'be',\n",
       " 227: 'battles',\n",
       " 228: 'bangs',\n",
       " 229: 'another',\n",
       " 230: 'all'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode = dict(enumerate(text_vectorization.get_vocabulary()))\n",
    "decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the fox battles [UNK]'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sentence = \" \".join(decode[int(i)] for i in sentence)\n",
    "\n",
    "decoded_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we process text, we can either add this processing step to a tensorflow pipeline, or we can add text processing as part of our model (as a Layer).\n",
    "\n",
    "However, the text tokenization process cannot take advantage of GPUs, and therefore is trained on the CPU. If you include it as a layer in your network, every iteration will need to wait for the tokenization layer to process on the CPU before passing off the results to the GPU to run the rest of the model, which can slow things down.\n",
    "\n",
    "Let's process the IMDB reviews dataset this way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  12.4M      0  0:00:06  0:00:06 --:--:-- 14.4M    0  0:00:06  0:00:04  0:00:02 11.6M\n",
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "# load in data from URL\n",
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# unzip file\n",
    "!tar -xf aclImdb_v1.tar.gz\n",
    "\n",
    "# remove extra files that are used for unsupervised learning\n",
    "!rm -r aclImdb/train/unsup\n",
    "\n",
    "# show us one file\n",
    "!cat aclImdb/train/pos/4077_10.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, pathlib, shutil, random\n",
    "\n",
    "# base_dir = pathlib.Path(\"aclImdb\")\n",
    "# val_dir = base_dir / \"val\"\n",
    "# train_dir = base_dir / \"train\"\n",
    "# for category in (\"neg\", \"pos\"):\n",
    "#     os.makedirs(val_dir / category)\n",
    "#     files = os.listdir(train_dir / category)\n",
    "#     random.Random(1337).shuffle(files)\n",
    "#     num_val_samples = int(0.2 * len(files))\n",
    "#     val_files = files[-num_val_samples:]\n",
    "#     for fname in val_files:\n",
    "#         shutil.move(train_dir / category / fname,\n",
    "#                     val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'I saw this on TV the other night\\xc2\\x85 or rather I flicked over to another channel every so often to watch infomercials when I couldn\\'t stand watching it any longer. It was bad. Really, really bad. Not \"so bad it\\'s good\" just flat out bad. How did it get funded? Who thought this was a good idea? An actor friend of mine auditioned and was told he wasn\\'t good enough to play a bad guy but I think what they meant was \"save yourself and runaway from this steaming pile of @#$%.\" I bet the rest of the cast had been given the option. To be fair the acting was hard to judge because of the appalling fake American ascents. The shooting was dullllllllllll. The action was awkward and stilted. The dialog was inane. By far the saddest thing was ship. In real life the Interislander ferry is a shabby boat and on film it doesn\\'t scrub up well. Instead of trying very unsuccessfully to make it look like a new crews liner with bits of tinsel wrapped around rusting polls, I kid you not, they could have change the script to explain or even celebrate the shabbiness. Dumb, Dumb, Dumb. Don\\'t watch this movie, not even as a joke.', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    # print(targets)\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-grams\n",
    "\n",
    "Let's fit our text classification model using 1-grams (single words as tokens). Let's train a model on this and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# take in raw text, standardize and tokenize\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# create binary 1-grams\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# define a model building function so that we can\n",
    "# use the same architechture over and over with different\n",
    "# forms of pre-processing\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 39s 49ms/step - loss: 0.3939 - accuracy: 0.8365 - val_loss: 0.2098 - val_accuracy: 0.9224\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2773 - accuracy: 0.8986 - val_loss: 0.1750 - val_accuracy: 0.9384\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2542 - accuracy: 0.9126 - val_loss: 0.1565 - val_accuracy: 0.9478\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2398 - accuracy: 0.9211 - val_loss: 0.1497 - val_accuracy: 0.9508\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2356 - accuracy: 0.9249 - val_loss: 0.1480 - val_accuracy: 0.9552\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2361 - accuracy: 0.9248 - val_loss: 0.1394 - val_accuracy: 0.9564\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2264 - accuracy: 0.9291 - val_loss: 0.1327 - val_accuracy: 0.9596\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2214 - accuracy: 0.9305 - val_loss: 0.1271 - val_accuracy: 0.9620\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2211 - accuracy: 0.9313 - val_loss: 0.1280 - val_accuracy: 0.9610\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2144 - accuracy: 0.9338 - val_loss: 0.1260 - val_accuracy: 0.9638\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.3636 - accuracy: 0.8800\n",
      "Test acc: 0.880\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 31s 38ms/step - loss: 0.3710 - accuracy: 0.8466 - val_loss: 0.1733 - val_accuracy: 0.9422\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2572 - accuracy: 0.9108 - val_loss: 0.1349 - val_accuracy: 0.9582\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2222 - accuracy: 0.9287 - val_loss: 0.1218 - val_accuracy: 0.9630\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2092 - accuracy: 0.9357 - val_loss: 0.1065 - val_accuracy: 0.9702\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2100 - accuracy: 0.9374 - val_loss: 0.1008 - val_accuracy: 0.9712\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1953 - accuracy: 0.9422 - val_loss: 0.0993 - val_accuracy: 0.9736\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2021 - accuracy: 0.9447 - val_loss: 0.0918 - val_accuracy: 0.9740\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1914 - accuracy: 0.9450 - val_loss: 0.0952 - val_accuracy: 0.9724\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1909 - accuracy: 0.9458 - val_loss: 0.0865 - val_accuracy: 0.9748\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.1830 - accuracy: 0.9464 - val_loss: 0.0819 - val_accuracy: 0.9774\n",
      "782/782 [==============================] - 30s 38ms/step - loss: 0.3531 - accuracy: 0.8872\n",
      "Test acc: 0.887\n"
     ]
    }
   ],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "binary_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "782/782 [==============================] - 32s 40ms/step - loss: 0.4793 - accuracy: 0.7907 - val_loss: 0.2100 - val_accuracy: 0.9248\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.3229 - accuracy: 0.8745 - val_loss: 0.1582 - val_accuracy: 0.9444\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2867 - accuracy: 0.8914 - val_loss: 0.1575 - val_accuracy: 0.9434\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.2618 - accuracy: 0.9004 - val_loss: 0.1434 - val_accuracy: 0.9508\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2496 - accuracy: 0.9046 - val_loss: 0.1310 - val_accuracy: 0.9514\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2385 - accuracy: 0.9080 - val_loss: 0.1244 - val_accuracy: 0.9570\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2320 - accuracy: 0.9094 - val_loss: 0.1213 - val_accuracy: 0.9532\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2275 - accuracy: 0.9124 - val_loss: 0.1210 - val_accuracy: 0.9550\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2273 - accuracy: 0.9137 - val_loss: 0.1200 - val_accuracy: 0.9556\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.2243 - accuracy: 0.9126 - val_loss: 0.1152 - val_accuracy: 0.9590\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.3443 - accuracy: 0.8739\n",
      "Test acc: 0.874\n"
     ]
    }
   ],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")\n",
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "tfidf_2gram_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TextVectorization Layer after Training\n",
    "\n",
    "Once we're done training our model, we may want to create a NEW model where we have an added text vectorization layer so that we can input raw data into the model and still make predictions.\n",
    "\n",
    "To do that, we use the functional API from keras. First, we create an input layer that is expecting text of variable dimensions.\n",
    "\n",
    "Then we send that text through the `text_vectorization()` layer we created. This will take our text and process it.\n",
    "\n",
    "Then we feed this processed output into our model `model()` to make an actual prediction. \n",
    "\n",
    "By shoving all of these things into a new model, `inference_model()` we've created a useful model object that does both our pre-processing and predictions for us, while not creating a bottleneck of pre-processing during each iteration of training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "outputs = model(processed_inputs)\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.86 percent positive\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On Your Own\n",
    "\n",
    "Build a simple text classification model using this email data (on Canvas).\n",
    "\n",
    "Upload the .zip file to your working directory and then run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip emails.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"Data\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"Crime\", \"Entertainment\"):\n",
    "    os.makedirs(val_dir / category)\n",
    "    os.makedirs(train_dir / category)\n",
    "    files = os.listdir(base_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    train_files = files[0:num_val_samples]\n",
    "    for fname in val_files:\n",
    "        shutil.move(base_dir / category / fname,\n",
    "                    val_dir / category / fname)\n",
    "    for fname in train_files:\n",
    "        shutil.move(base_dir / category / fname,\n",
    "                    train_dir / category / fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"Data/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"Data/val\", batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take in raw text, standardize and tokenize\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=7500,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "# create binary 1-grams\n",
    "binary_1gram_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "binary_1gram_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "for i in binary_1gram_train_ds.take(1):\n",
    "  print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layers\n",
    "We also learned about word embeddings which are non-sparse, low dimensional vectors that represent the semantic meaning of words.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1Ef4ZAxOuifpeodt8AZbQISNbOWQ_Bvu9\" alt=\"Q\" width = \"400\"/>\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1WRx3J3bc95lcJAObQ7FwVCdvbEX0jHn3\" alt=\"Q\" width = \"400\"/>\n",
    "\n",
    "While we can use pre-trained word embeddings (such as GloVe and word2vec; see tutorial [here](https://keras.io/examples/nlp/pretrained_word_embeddings/)), we can also learn our own embeddings as a part of our models.\n",
    "\n",
    "We can do so using Keras' [`Embedding()`](https://keras.io/api/layers/core_layers/embedding/) layer. \n",
    "\n",
    "## Feedback Phrases Sentiment\n",
    "\n",
    "Let's look at a really simple example which is trying to classify different feedback phrases as positive or negative.\n",
    "\n",
    "To process our text we:\n",
    "\n",
    "- create one hot encodings of our words, and store the *indices* for each word\n",
    "\n",
    "(For example if the word `\"hello\"` was represented as `[0,0,0,0,0,1,0]` we'd store the index `5` instead of the whole vector `[0,0,0,0,0,1,0]`)\n",
    "\n",
    "- pad the sequences so that they're all the same length. If a sequence is too short, we add `0`'s at the end until it's the right length\n",
    "- feed the sequences into an `Embedding()` layer.\n",
    "    - `input_dim`: tells us the length of the embedding vector, ours will be `vocab_size` because we have `vocab_size`-dimensional one-hot vectors\n",
    "    - `output_dim`: the dimension of our embedded vectors\n",
    "    - `input_length`: the length of all our sequences (if it's the same for each sequence)\n",
    "\n",
    "- Flatten our embeddings into a single vector that can be fed to `Dense()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One Hot Indices------------------------------------\n",
      "[[32, 42], [39, 21], [23, 35], [29, 21], [8], [38], [20, 35], [26, 39], [20, 21], [5, 48, 42, 56], [12, 11], [56, 59, 47, 32]]\n",
      "---------------------------------------------------\n",
      "\n",
      "\n",
      "Documents with padding to make them the same length\n",
      "[[32 42  0  0]\n",
      " [39 21  0  0]\n",
      " [23 35  0  0]\n",
      " [29 21  0  0]\n",
      " [ 8  0  0  0]\n",
      " [38  0  0  0]\n",
      " [20 35  0  0]\n",
      " [26 39  0  0]\n",
      " [20 21  0  0]\n",
      " [ 5 48 42 56]\n",
      " [12 11  0  0]\n",
      " [56 59 47 32]]\n",
      "---------------------------------------------------\n",
      "\n",
      "\n",
      "Model Summary------------------------------------\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 8)              480       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513\n",
      "Trainable params: 513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "---------------------------------------------------\n",
      "\n",
      "Accuracy: 91.666669\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "# define documents\n",
    "docs = ['Well done!',\n",
    " 'Good work',\n",
    " 'Great effort',\n",
    " 'nice work',\n",
    " 'Excellent!',\n",
    " 'Weak',\n",
    " 'Poor effort!',\n",
    " 'not good',\n",
    " 'poor work',\n",
    " 'Could have done better.',\n",
    " 'Gorgeous job',\n",
    " 'Better luck next time']\n",
    "\n",
    "# define class labels\n",
    "labels = array([1,1,1,1,1,0,0,0,0,0,1,0])\n",
    "\n",
    "# pre-process\n",
    "\n",
    "# integer encode the documents\n",
    "vocab_size = 60\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(\"\\nOne Hot Indices------------------------------------\")\n",
    "print(encoded_docs)\n",
    "print(\"---------------------------------------------------\\n\")\n",
    "\n",
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(\"\\nDocuments with padding to make them the same length\")\n",
    "print(padded_docs)\n",
    "print(\"---------------------------------------------------\\n\")\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 8, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# summarize the model\n",
    "print(\"\\nModel Summary------------------------------------\")\n",
    "print(model.summary())\n",
    "print(\"---------------------------------------------------\\n\")\n",
    "# fit the model\n",
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pride and Prejudice vs. Frankenstein\n",
    "\n",
    "Now, let's use the same Pride and Prejudice text we used last week, as well as the text of Frankenstein (on canvas) to build a model that can classify whether a snippet of 100 words is from Pride and Prejudice or Frankenstein.\n",
    "\n",
    "First, we need to load in our two text files, get rid of punctuation, and split them into word lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"pandp.txt\"\n",
    "raw_text_p = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text_p = re.sub(r'[^\\w\\s]', '', raw_text_p)\n",
    "raw_text_p = raw_text_p.lower().split()\n",
    "\n",
    "\n",
    "filename = \"frankenstein.txt\"\n",
    "raw_text_f = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text_f = re.sub(r'[^\\w\\s]', '', raw_text_f)\n",
    "raw_text_f = raw_text_f.lower().split()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll create a dictionary that maps our word tokens into indices for one hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  121529\n",
      "Total Vocab:  14118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text_p))) + sorted(list(set(raw_text_f)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "char_to_int\n",
    "# text info\n",
    "n_chars = len(raw_text_p) \n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll pull data from each file by creating a bunch of sequences of 100 words from each text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns PandP:  121509\n",
      "Total Characters:  74941\n",
      "Total Vocab:  14118\n",
      "Total Patterns Frank:  74921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 20 # 100 words as input\n",
    "dataX_p = []\n",
    "dataY_p = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    " seq_in = raw_text_p[i:i + seq_length] # generate 100 character input\n",
    " seq_out = raw_text_p[i + seq_length] # grab next character\n",
    " \n",
    " dataX_p.append([char_to_int[char] for char in seq_in])\n",
    " dataY_p.append(0)\n",
    "n_patterns = len(dataX_p)\n",
    "print(\"Total Patterns PandP: \", n_patterns)\n",
    "\n",
    "\n",
    "# text info\n",
    "n_chars = len(raw_text_f) \n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 20 # 100 words as input\n",
    "dataX_f = []\n",
    "dataY_f = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    " seq_in = raw_text_f[i:i + seq_length] # generate 100 character input\n",
    " seq_out = raw_text_f[i + seq_length] # grab next character\n",
    " \n",
    " dataX_f.append([char_to_int[char] for char in seq_in])\n",
    " dataY_f.append(1)\n",
    "n_patterns = len(dataX_f)\n",
    "print(\"Total Patterns Frank: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the pandp and frank training data\n",
    "dataX = dataX_p + dataX_f\n",
    "dataY = dataY_p + dataY_f"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use this data to build a model that predicts whether or not a text is from pride and prejudice or frankenstein! Use `Embedding`, `Dense`, and `Flatten` layers to build a model similar to the one in the previous section, but using this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "\n",
    "# compile the model\n",
    "\n",
    "# summarize the model\n",
    "\n",
    "# fit the model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
