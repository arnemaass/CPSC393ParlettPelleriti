{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import keras as kb\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "ITpol7esk-dU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0tDoQQeO_oO"
      },
      "source": [
        "# Transfer Learning\n",
        "Transfer Learning is the typical way MOST people will use Deep Learning.\n",
        "\n",
        "Transfer Learning involves using models that someone else **already trained** and either:\n",
        "\n",
        "- building on top of the model\n",
        "OR\n",
        "- fine tuining some or all of the weights of the model\n",
        "\n",
        "Transfer learning is so important because state of the art deep learning models are very **expensive** to train (both computationally expensive, time expensive, and concomitantly money expensive because that compute time costs money! Some of these models cost $50,000 to train).\n",
        "\n",
        "You will likely not be training complex models yourself, you'll use open source models and tweak them to fit whatever needs you have.\n",
        "\n",
        "## Load Data From Kaggle\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J4WJM1WO_oQ",
        "outputId": "89da60ab-6f86-4ff8-de1e-a736d4e196b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnvyHcqFO_oQ",
        "outputId": "bece8d29-d8e2-46b4-acb2-c32289b4c730"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dogs-vs-cats.zip to /content\n",
            "100% 810M/812M [00:13<00:00, 49.7MB/s]\n",
            "100% 812M/812M [00:13<00:00, 61.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GaiqZZPaO_oR"
      },
      "outputs": [],
      "source": [
        "!unzip -qq dogs-vs-cats.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kfI_1s1tO_oR"
      },
      "outputs": [],
      "source": [
        "!unzip -qq train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c6tElgPRO_oR"
      },
      "outputs": [],
      "source": [
        "# load data for next part\n",
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1000)\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
        "make_subset(\"test\", start_index=1500, end_index=2500)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfZgTUwXO_oS",
        "outputId": "616ac6c2-3f98-4eed-82a0-b0d7137fd4df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 files belonging to 2 classes.\n",
            "Found 1000 files belonging to 2 classes.\n",
            "Found 2000 files belonging to 2 classes.\n",
            "(16,)\n",
            "(16,)\n",
            "(16,)\n",
            "(32, 16)\n",
            "(32, 16)\n",
            "(32, 16)\n",
            "(4, 4)\n",
            "(4, 4)\n",
            "(4, 4)\n",
            "data batch shape: (32, 180, 180, 3)\n",
            "labels batch shape: (32,)\n"
          ]
        }
      ],
      "source": [
        "from keras.utils import image_dataset_from_directory\n",
        "import numpy as np\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "\n",
        "random_numbers = np.random.normal(size=(1000, 16))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(random_numbers)\n",
        "\n",
        "for i, element in enumerate(dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break\n",
        "\n",
        "batched_dataset = dataset.batch(32)\n",
        "for i, element in enumerate(batched_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break\n",
        "\n",
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break\n",
        "\n",
        "for data_batch, labels_batch in train_dataset:\n",
        "    print(\"data batch shape:\", data_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfWwNZ96O_oT"
      },
      "source": [
        "\n",
        "## Building FF Layers on Top of a Pre-Trained CNN\n",
        "\n",
        "There are many pre-trained CNNs that are trained on very large repositories of images that include every-day objects and animals. Remember that the purpose of Convolutional and Pooling Layers is to *learn features* in the image (like a wheel, an eye, a petal...). If the pre-trained CNN learns enough useful features, we can use these features for a *bunch* of different applications.\n",
        "\n",
        "Some common pre-trained CNNs you might hear about are [**VGG**](https://keras.io/api/applications/vgg/), **ResNet** (the one that popularized residual connections), **Inception**, and **Xception**.\n",
        "\n",
        "We're going to use **VGG16** and build our own classifier on top of its convolutional layers.  \n",
        "\n",
        "We'll take the full model and remove the \"head\" (the FeedForward layers), and create our own FeedForward layers.\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20200219152327/conv-layers-vgg16.jpg\" alt=\"Q\" width = “400”/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gNrbtOxO_oU",
        "outputId": "c08ffb31-39eb-4920-85bd-dc4f9773e26f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 180, 180, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 180, 180, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 90, 90, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 90, 90, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 90, 90, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 45, 45, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 45, 45, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 45, 45, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 45, 45, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 22, 22, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 22, 22, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 22, 22, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 11, 11, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 11, 11, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 5, 5, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vgg_conv = kb.applications.vgg16.VGG16(\n",
        "    weights = \"imagenet\",\n",
        "    include_top = False,\n",
        "    input_shape = (180,180,3)\n",
        ")\n",
        "\n",
        "vgg_conv.trainable = False\n",
        "vgg_conv.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6Y6YqyiO_oU"
      },
      "source": [
        "LOOK HOW MANY parameters there are?! Do you want to train that model? I sure don't.\n",
        "\n",
        "Now that we have the base, we can add FF layers to the model.\n",
        "\n",
        "The function `kb.applications.vgg16.preprocess_input()` applies some pre-processing steps to the data before feeding it into the VGG model base."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aJQ_PTuzO_oU"
      },
      "outputs": [],
      "source": [
        "inputs = kb.Input(shape=(180, 180, 3))\n",
        "x = kb.applications.vgg16.preprocess_input(inputs)\n",
        "x = vgg_conv(x)\n",
        "x = kb.layers.Flatten()(x)\n",
        "x = kb.layers.Dense(256)(x)\n",
        "x = kb.layers.Dropout(0.5)(x)\n",
        "outputs = kb.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "custom_ff = kb.Model(inputs, outputs)\n",
        "custom_ff.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5THxb_2bO_oV",
        "outputId": "514a61a8-5743-482b-ac80-a5d40b10892d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "63/63 [==============================] - 23s 215ms/step - loss: 13.9781 - accuracy: 0.9260 - val_loss: 16.1519 - val_accuracy: 0.9080\n",
            "Epoch 2/5\n",
            "63/63 [==============================] - 9s 146ms/step - loss: 3.7454 - accuracy: 0.9720 - val_loss: 4.9354 - val_accuracy: 0.9630\n",
            "Epoch 3/5\n",
            "63/63 [==============================] - 9s 138ms/step - loss: 2.5758 - accuracy: 0.9840 - val_loss: 6.3043 - val_accuracy: 0.9660\n",
            "Epoch 4/5\n",
            "63/63 [==============================] - 10s 147ms/step - loss: 1.1188 - accuracy: 0.9905 - val_loss: 4.5770 - val_accuracy: 0.9670\n",
            "Epoch 5/5\n",
            "63/63 [==============================] - 11s 173ms/step - loss: 1.0140 - accuracy: 0.9925 - val_loss: 3.5877 - val_accuracy: 0.9770\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7badacf27070>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "custom_ff.fit(train_dataset, epochs = 5, validation_data = validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJI8EZpJTKiH",
        "outputId": "e732f318-232c-4bc5-d12c-c7bb853a932e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
            "                                                                 \n",
            " tf.__operators__.getitem (  (None, 180, 180, 3)       0         \n",
            " SlicingOpLambda)                                                \n",
            "                                                                 \n",
            " tf.nn.bias_add (TFOpLambda  (None, 180, 180, 3)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 5, 5, 512)         14714688  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12800)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               3277056   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17992001 (68.63 MB)\n",
            "Trainable params: 3277313 (12.50 MB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "custom_ff.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wegbb-24O_oV"
      },
      "source": [
        "## Fine Tuning a Network\n",
        "\n",
        "While adding a new FF portion to a pre-trained CNN is simple and often works well, we *sometimes* want to tweak the weights from the pre-trained model. In order to do this we **Freeze** and **Unfreeze** various layers of the network.\n",
        "\n",
        "Layers that are **Frozen** will not be updated, their weights will stay as is.\n",
        "\n",
        "Layers that are **NOT Frozen** are trainable and their weights will be updated using backprop + gradient descent while the FF layers we added are trained. This allows the network to learn *slightly* different features that are better suited to our purposes. Typically we will unfreeze the last few convolutional/pooling layers from the pre-trained network.\n",
        "\n",
        "NOTE: it would be a bad idea to unfreeze middle layers without unfreezing the layers after it.\n",
        "\n",
        "\n",
        "Now let's use the same base model **VGG** but we'll unfreeze the last 4 layers, and train them along with the feed forward layers that we added.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1BPCDzYXeN-ERqhEBKUml48fh8uEJljvW\"/>\n",
        "\n",
        "(image from Deep Learning with Python by Chollet)\n",
        "\n",
        "\n",
        "\n",
        "(activity modified from [this example](https://towardsdatascience.com/fine-tuning-pre-trained-model-vgg-16-1277268c537f))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT7jEkrCO_oV",
        "outputId": "eabcbad9-b907-4964-bea4-afb2301897a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_3 False\n",
            "1 block1_conv1 False\n",
            "2 block1_conv2 False\n",
            "3 block1_pool False\n",
            "4 block2_conv1 False\n",
            "5 block2_conv2 False\n",
            "6 block2_pool False\n",
            "7 block3_conv1 False\n",
            "8 block3_conv2 False\n",
            "9 block3_conv3 False\n",
            "10 block3_pool False\n",
            "11 block4_conv1 False\n",
            "12 block4_conv2 False\n",
            "13 block4_conv3 False\n",
            "14 block4_pool False\n",
            "15 block5_conv1 True\n",
            "16 block5_conv2 True\n",
            "17 block5_conv3 True\n",
            "18 block5_pool True\n"
          ]
        }
      ],
      "source": [
        "vgg_conv = kb.applications.vgg16.VGG16(\n",
        "    weights = \"imagenet\",\n",
        "    include_top = False,\n",
        "    input_shape = (180,180,3)\n",
        ")\n",
        "\n",
        "# Freeze four convolution blocks\n",
        "for layer in vgg_conv.layers[:15]:\n",
        "    layer.trainable = False\n",
        "# Make sure you have frozen the correct layers\n",
        "for i, layer in enumerate(vgg_conv.layers):\n",
        "    print(i, layer.name, layer.trainable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8JTA3Gi9O_oV"
      },
      "outputs": [],
      "source": [
        "inputs = kb.Input(shape=(180, 180, 3))\n",
        "x = kb.applications.vgg16.preprocess_input(inputs)\n",
        "x = vgg_conv(x)\n",
        "x = kb.layers.Flatten()(x)\n",
        "x = kb.layers.Dense(256)(x)\n",
        "x = kb.layers.Dropout(0.5)(x)\n",
        "outputs = kb.layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "transfer_model = kb.Model(inputs, outputs)\n",
        "transfer_model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=kb.optimizers.RMSprop(learning_rate=1e-5),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9-fOSM9O_oV",
        "outputId": "7e6d20b2-e7f9-45b7-bbed-8aa5082a33a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "63/63 [==============================] - 16s 214ms/step - loss: 3.9175 - accuracy: 0.7955 - val_loss: 0.5782 - val_accuracy: 0.9355\n",
            "Epoch 2/5\n",
            "63/63 [==============================] - 13s 209ms/step - loss: 0.8977 - accuracy: 0.9250 - val_loss: 0.3756 - val_accuracy: 0.9530\n",
            "Epoch 3/5\n",
            "63/63 [==============================] - 14s 213ms/step - loss: 0.3523 - accuracy: 0.9610 - val_loss: 0.3025 - val_accuracy: 0.9615\n",
            "Epoch 4/5\n",
            "63/63 [==============================] - 14s 210ms/step - loss: 0.1840 - accuracy: 0.9760 - val_loss: 0.3008 - val_accuracy: 0.9625\n",
            "Epoch 5/5\n",
            "63/63 [==============================] - 14s 216ms/step - loss: 0.1802 - accuracy: 0.9795 - val_loss: 0.2533 - val_accuracy: 0.9690\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7badacf26740>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "transfer_model.fit(train_dataset, epochs=5, validation_data=(test_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWJVC8BwTCku",
        "outputId": "3afb0c59-b292-4e5a-efe1-dc74cdae3053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 180, 180, 3)]     0         \n",
            "                                                                 \n",
            " tf.__operators__.getitem_1  (None, 180, 180, 3)       0         \n",
            "  (SlicingOpLambda)                                              \n",
            "                                                                 \n",
            " tf.nn.bias_add_1 (TFOpLamb  (None, 180, 180, 3)       0         \n",
            " da)                                                             \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 5, 5, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 12800)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               3277056   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17992001 (68.63 MB)\n",
            "Trainable params: 10356737 (39.51 MB)\n",
            "Non-trainable params: 7635264 (29.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "transfer_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiNyrECCUR2Z"
      },
      "source": [
        "Compare the number of `Non-trainable params:` in this model compared to the last one we trained."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}