{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# data and plotting\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "from sklearn.model_selection import train_test_split\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, ConfusionMatrixDisplay, roc_auc_score, recall_score, precision_score\n",
    "\n",
    "# models\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review from Last Time\n",
    "Last time we learned about two models: The **Maximal Margin Classifier** and the **Support Vector Classifier**. Maximial Margin Classfiers help us solve the problem of picking between infinitely many hyperplanes when our data is linearly separable.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1zAWlFxIOJchpnJRF3DGiIpv3VdFfq-ci\" style=\"background-color:white;\"  alt=\"hyperplane plot\" width = \"600\" class=\"center\"/> </center>\n",
    "\n",
    "\n",
    "But sometimes, data are *not* linearly separable. In come SVCs. SVCs introduce *slack variables* ($\\xi_i$) that allow points to be inside or even on the wrong side of the margin.\n",
    "\n",
    "But now we have a new problem. Data isn't always linearly separable, even *with* slack variables.\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1BTwdaw4ZiY3AU-sl_7lnRgxz2oBz3bVG\"  style=\"background-color:white;\"  alt=\"hyperplane plot\" width = \"600\" class=\"center\"/> </center>\n",
    "\n",
    "The good news is we can just \"project\" our data into multiple dimensions, and find a separating hyperplane there!\n",
    "\n",
    "<center><img src=\"https://drive.google.com/uc?export=view&id=1J1aOlA3OUB39yFnOKmUkFtR6kOrzuKYp\"  style=\"background-color:white;\"  alt=\"hyperplane plot\" width = \"600\" class=\"center\"/> </center>\n",
    "\n",
    "# Review from Lecture\n",
    "In the last lecture we learned about kernels and The Kernel Trick that allow us to use a linear classifier like SVCs on non-linearly separable data. \n",
    "\n",
    "Remember, a Kernel is a function that calculates the relationship (dot product) between two vectors *as if* they are in a higher dimensional space *without actually having to calculate that higher dimensional space*.\n",
    "\n",
    "We talked about two main kernels:\n",
    "\n",
    "## Polynomial Kernel\n",
    "\n",
    "$$K(x,y) = (x*y + r)^d$$\n",
    "\n",
    "The hyperparameters $r$ and $d$ can be chosen via hyperparameter tuning. $d$ controls the maximum dimensions of the space we're projecting our data to (e.g. when $d=2$ we're at most projecting to a 2D space). \n",
    "\n",
    "For example:\n",
    "\n",
    "$$K(x,y) = (x*y + \\frac{1}{2})^2$$\n",
    "$$(x*y + \\frac{1}{2})^2 = (x*y + \\frac{1}{2})(x*y + \\frac{1}{2})$$\n",
    "$$ = xy + x^2y^2 + \\frac{1}{4}$$\n",
    "\n",
    "Notice that $xy + x^2y^2 + \\frac{1}{4}$ is the same value we'd get if we took the dot product of the two vectors: $x' = (x,x^2, \\frac{1}{2})$ and $y' = (y,y^2, \\frac{1}{2})$. Thus, the kernel $K(x,y)$ calculates the same value as the dot product between two *transformed* points $x'$ and $y'$ which have 2 dimensions. The first dimension is the original variable, the second dimension is the variable squared (we ignore the 3rd dimension because it is the same for all data points, and thus does not give us any useful information).\n",
    "\n",
    "BUT NOTICE, when we plug $x$ and $y$ into the kernel function $K(x,y)$, we NEVER actually had to calculate $x'$ and $y'$!! Which saves us a lot of time. This is true no matter the $r$ and $d$ values you use for the polynomial kernel.\n",
    "\n",
    "\n",
    "## Radial Kernel (Radial Basis Function)\n",
    "\n",
    "$$ K(x,y) = e^{-\\gamma(x-y)^2}$$\n",
    "\n",
    "This whole calculates-the-dot-product-without-calculating-the-transformed-data thing is especially useful when the transformation transforms the data into *infinite* dimensions like the Radial Kernel. \n",
    "\n",
    "We proved using Taylor Series Expansion that the radial kernel calculates the dot product of two data points *as if* they were projected into infinite dimensions. This causes the SVM to act like a weighted KNN model, and gives us a huge amount of flexibility with what our groups of data look like.\n",
    "\n",
    "## Why is the *Dot Product* so important?\n",
    "\n",
    "$$\n",
    "\\overbrace{y(\\mathbf{w}) = \\sum_{n=1}^N a_n t_n \\;\\; \\phi(x) \\cdot \\phi(x_n) \\;\\;+b}^{\\text{formula to classify new data point } x} $$\n",
    "\n",
    "$$\\overbrace{y(\\mathbf{w}) = \\sum_{n=1}^N a_n t_n \\;\\; \\underbrace{K(x,y)}_\\text{kernel} \\;\\;+b}^{\\text{formula to classify new data point } x} \n",
    "$$\n",
    "\n",
    "# SVC's in sklearn\n",
    "\n",
    "We'll use this Pulsar Star Data from [Kaggle](https://www.kaggle.com/datasets/colearninglounge/predicting-pulsar-starintermediate?resource=download).\n",
    "\n",
    "Download the data, then upload it using the upload button to Colab. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "star = pd.read_csv(\"pulsar_data_train.csv\")\n",
    "star.head()\n",
    "star = star.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# organize and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build parts of pipeline\n",
    "\n",
    "# build pipeline\n",
    "\n",
    "# parameters dict\n",
    "\n",
    "# grid search\n",
    "\n",
    "# fit and check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The possible distributions you can use for your hyperparameters are listed [here](https://docs.scipy.org/doc/scipy/reference/stats.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an SVM with radial kernel\n",
    "\n",
    "Let's look at that data we tried (and failed) to classify using a linear kernel last time.\n",
    "- load in the data\n",
    "- plot the data using a scatter plot\n",
    "- do an 80/20 TTS\n",
    "- zscore your data\n",
    "- build an `SVC()` model with `kernel = \"rbf\"` and `C = 1`\n",
    "- grab the train/test accuracy and ROC/AUC\n",
    "- use the `plotSVM2D()` function I wrote for you to plot the decision boundary for your model for both train and test (just change the first two arguments, no need to refit the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.203055</td>\n",
       "      <td>-1.309099</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.486382</td>\n",
       "      <td>-0.159741</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.832017</td>\n",
       "      <td>0.246909</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.411230</td>\n",
       "      <td>1.565836</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.154422</td>\n",
       "      <td>-0.144092</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2  y\n",
       "0 -1.203055 -1.309099  a\n",
       "1 -1.486382 -0.159741  a\n",
       "2 -0.832017  0.246909  a\n",
       "3 -1.411230  1.565836  a\n",
       "4 -1.154422 -0.144092  a"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "d = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC393ParlettPelleriti/main/Data/svmcw.csv\")\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your train test split, and z score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT CHANGE ANYTHING JUST RUN\n",
    "\n",
    "def plotSVM2D(Xdf,y,model):\n",
    "    x0_name = Xdf.columns[0]\n",
    "    x1_name = Xdf.columns[1]\n",
    "    #grab the range of features for each feature\n",
    "    x0_range = np.linspace(min(Xdf[x0_name]) - np.std(Xdf[x0_name]),\n",
    "                           max(Xdf[x0_name]) + np.std(Xdf[x0_name]), num = 100)\n",
    "    x1_range = np.linspace(min(Xdf[x1_name]) - np.std(Xdf[x1_name]),\n",
    "                           max(Xdf[x1_name]) + np.std(Xdf[x1_name]), num = 100)\n",
    "\n",
    "    #get all possible points on graph\n",
    "    x0 = np.repeat(x0_range,100)\n",
    "    x1 = np.tile(x1_range,100)\n",
    "    x_grid = pd.DataFrame({x0_name: x0,x1_name: x1})\n",
    "\n",
    "    # bredict all background points\n",
    "    p = model.predict(x_grid)\n",
    "    x_grid[\"p\"] = p #add to dataframe\n",
    "    \n",
    "    #build the plot\n",
    "    bound = (ggplot(x_grid, aes(x = x0_name, y = x1_name, color = \"factor(p)\")) +\n",
    "                 geom_point(alpha = 0.2, size = 0.2) + theme_minimal() +\n",
    "                 scale_color_manual(name = \"Class\", values = [\"#E69F00\", \"#0072B2\"]) +\n",
    "                 geom_point(data = Xdf, mapping = aes(x = x0_name, y = x1_name, color = \"factor(y)\"), size = 2))\n",
    "    print(bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotSVM2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "How does the model do? How does it differ from the model you built using a linear kernel last class?\n",
    "\n",
    "\n",
    "## Hyperparameters in `sklearn`\n",
    "\n",
    "### `gamma`\n",
    "for a RBF kernel, the Gamma ($\\gamma$) parameter scales the amount of influence two data points have on each other. The bigger $\\gamma$ is, the smaller the influence two data points have on each other:\n",
    "\n",
    "$$ e^{(-\\gamma \\lVert x-y \\rVert^2)} $$\n",
    "\n",
    "NOTE: for Polynomial Kernels, Gamma ($\\gamma$) is just a coefficient that we multiply our dot-product by:\n",
    "\n",
    "$$(\\gamma\\langle x,y \\rangle  + r)^d$$\n",
    "\n",
    "### `coef0`\n",
    "`coef0` is the name given to $r$ in the Polynomial kernel.\n",
    "\n",
    "$$(\\gamma\\langle x,y \\rangle  + r)^d$$\n",
    "\n",
    "### `degree`\n",
    "`degree` is the degree of your polynomial kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test it Out\n",
    "Now that we're familiar with sklearn syntax:\n",
    "\n",
    "- Re-run your model above but set `gamma = 25`  \n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "How did changing `gamma` change the decision boundary? Did the boundary get smoother or more jagged? Do jagged boundaries more often lead to **overfitting** or **underfitting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotSVM2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now re-run your model with `gamma = 0.1`.\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "How is your model doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotSVM2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an SVM with a polynomial kernel\n",
    "Let's see if we can classify this data with a polynomial kernel.\n",
    "\n",
    "- build an `SVC()` model with `kernel = \"poly\"`, `degree = 2`, `gamma = 1`, and `C = 1`\n",
    "- grab the train/test accuracy and ROC/AUC\n",
    "- use the `plotSVM2D()` function I wrote for you to plot the decision boundary for your model for both train and test (just change the first two arguments, no need to refit the model)\n",
    "\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "How does your model do? What does the decision boundary look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotSVM2D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Play around with the `C`, `degree` and `gamma` parameters.\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "How do they change the performance and decision boundary of the model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now with Messier Data\n",
    "\n",
    "While the previous data was CLEARLY non-linear and needed the help of a kernel, it was still fairly separable. Let's try a messier dataset [here](https://raw.githubusercontent.com/cmparlettpelleriti/CPSC393ParlettPelleriti/main/Data/svmcw2.csv).\n",
    "\n",
    "- load the data\n",
    "- plot the data with a scatterplot\n",
    "- do a 80/20 TTS\n",
    "- use a `rbf` kernel, and use GridSearch to chooose `C` (options should be `[0.001, 0.01, 0.1, 0.5, 1]`), and `gamma` (options should be `[0.001, 0.01, 0.1, 0.5, 1,2,5,10,25,50]`)\n",
    "- (make sure you z score *appropriately*)\n",
    "- print out the train/test accuracy and ROC/AUC.\n",
    "- plot the decision boundary for the full model for train and test using `plotSVM2D()`\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "What hyperparameter values did GridSearch choose? How did your model perform? What does the decision boundary look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do your train test split, and z score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotSVM2D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
