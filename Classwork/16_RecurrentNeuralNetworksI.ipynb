{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Architechture\n",
    "In previous lectures we learned about Convolutional Neural Networks which take into account the spatial relationships in spatial data like images. \n",
    "\n",
    "Recurrent Architechtures take into account *temporal* relationships in sequence data (such as a time series or text). While we can handle sequential data using densely connected feed forward or 1D convolutional architechtures, they often don't work as well as recurrent architechtures which are specifically built for sequential data.\n",
    "\n",
    "Recurrent Neural Networks use **cells** which take in an input (such as a word, or a stock price) and generate an output, just like a node in a feed forward neural network. However, unlike a feed forward neural network, recurrent cells also take in the output of the cell at the *previous time step*. We call this output the **hidden state**.\n",
    "\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1CJOsMv2MAoU5ooTGEBrlMX6UwmDAuSG6\" alt=\"Q\" width = \"400px\"/>\n",
    "\n",
    "In a Simple/Vanilla RNN, the inputs ($x_t$) and the previous hidden state ($h_{t-1}$) and combine them using weights ($W$) and a bias ($b$). We then send that combination through a hyperbolic tangent activation function which takes values and squishes them between -1 and 1. This output is our current hidden state ($h_t$).\n",
    "\n",
    "\n",
    "## RNNs as Deep Networks\n",
    "\n",
    "Even though our recurrent architechture *technically* only has one cell, we're feeding values through it over and over, thus it's as if we have a very deep network. RNN's suffer from many of the same issues as very deep feed forward neural networks, and the problems get more serious the longer our sequences are. \n",
    "\n",
    "\n",
    "# Preparing Time Series Data\n",
    "When we have time-series/sequential data, we need to decide *how* to input it into the model. Unlike tabular data that we used in FFNNs, or image data that we used with CNNs, time-series data requires a little bit of...finessing...in order to get it ready to put into a model. \n",
    "\n",
    "Say we have 20 values in a sequence (say my heartrate over time.) In order to train an RNN to predict my heart rate, I need to take my long sequence and break it into bit sized chunks. Let's say we want to train our model on the past 5 heart rates, and predict the current heartrate. \n",
    "\n",
    "We then need to break this sequence of 20 up into smaller sequences, each with a sequence of 5 input heart rates, and 1 output heart rate (what we want to predict.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Example\n",
    "Today we're going to walk through an example of using a `SimpleRNN` layer in Keras to build a recurrent neural network that predicts the weather. \n",
    "\n",
    "This was adapted based off of a Tensorflow exercise, I've changed the architechture to use a Vanilla RNN (rather than the ones we'll learn next class), and added a TON of comments so you can understand what's going on.\n",
    "\n",
    "First, we'll look at how to break our time series (weather over time) into smaller sequences, and how to do a train/test split when we have time series data. Then we'll feed this data into a simple RNN that we'll build using the Functional API.\n",
    "\n",
    "[Notebook](https://colab.research.google.com/drive/1fvzQ0nSd9j6Hqk7pEvXgRwv_z9bu_58h?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
