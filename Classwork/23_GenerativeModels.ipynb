{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Models\n",
    "\n",
    "In the previous Generative Modeling classwork, we talked about how the goal of generative modelling is to learn how to create new inputs (images, tabular data...etc) that have similar properties to the training data. Generally this can be accomplished in two ways:\n",
    "\n",
    "- **Density Estimation**: where the model learns an explicit probability density function that it can sample from (e.g. Naive Bayes, Gaussian Mixture Models, and Variational Autoencoders)\n",
    "- **Sample Generation**: where the model learns a function that can generate new stimuli without explicity density estimation (such as Generative Adversarial Networks)\n",
    "\n",
    "\n",
    "In this classwork, we'll focus on **Sample Generation** models which do not explicitly learn a probability distribtion to sample from (unlike VAEs, GMM, and NB in the last section), rather we just ask them to create new inputs and reward it when the inputs look like our training data and penalize it when they don't. \n",
    "\n",
    "The most common for of this type of model is a **Generative Adversarial Network** (GAN). GANs consist of two parts:\n",
    "\n",
    "- **Generator**: whose job is to create convincing new images/inputs from random noise\n",
    "- **Discriminator**: whose job it is to detect fake (made with the Generator) vs. real (training data) inputs\n",
    "\n",
    "The Generator and Discriminator act like a counterfitter and cop: the Generator learns to make more fakes over time, and the Discriminator learns how to detect those fakes better over time. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "$$  E_x[log(D(x))] + E_z[log(1-D(G(z)))] $$\n",
    "\n",
    "- $E_x$ refers to the expected value accross all real examples\n",
    "- $E_z$ refers to the expected value accross all fake examples\n",
    "- $D(x)$ is the predicted probability from the Discriminator ($D$) on *real* samples ($x$)\n",
    "- $G(z)$ is a fake output from the Generator ($G$) given some random noise ($z$)\n",
    "- $D(G(z))$ is the predicted probability from the Discriminator ($D$) on *fake* samples ($G(z)$)\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "- Given that predicted probabilities can only be between 0-1, think through with your group, what values of $D(x)$ and $D(G(z))$ would *minimize* and *maximize* the loss.\n",
    "- What do the values you calculated above tell you about the performance of the Generator/Discriminator when the loss is minimized? maximized?\n",
    "\n",
    "\n",
    "Because the Generator and Discriminator are largely at odds, the Generator wants to minimize the loss function, and the Discriminator wants to maximize it. \n",
    "\n",
    "## Problems\n",
    "\n",
    "However, because GANs are really two competing models in a trench coat, they can be difficult to train. Some major issues are:\n",
    "\n",
    "- Mode Collapse: The generator simply learns a single image that will fool the discriminator and maps all input noise to that image.\n",
    "- Vanishing Gradients: when the discriminator is too good, the generator's gradient goes to 0 (or near zero), causing it to not be able to learn\n",
    "- Lack of Convergence: sometimes the Generator and Discriminator will reach a type of equilibrium where the generator is creating good fakes. But sometimes they just oscillate, undoing each others' progress.\n",
    "\n",
    "\n",
    "# Generative Models in Your Daily Life\n",
    "- Star Wars\n",
    "- [TikTok](https://www.theverge.com/2023/3/2/23621751/bold-glamour-tiktok-face-filter-beauty-ai-ar-body-dismorphia)\n",
    "\n",
    "# MNIST and Pokemon GANs\n",
    "\n",
    "First, let's just take a look at Tensorflow's example GAN on MNIST data ([here](https://www.tensorflow.org/tutorials/generative/dcgan))\n",
    "\n",
    "Then, let's open my modified version which uses pokemon images (converted to grayscale) in a similar GAN ([here](https://colab.research.google.com/drive/1-cZOaHL5hREzpXc3Kw4F_BFCZRf_TuW3?usp=sharing))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
