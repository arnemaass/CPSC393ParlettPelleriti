{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-09 19:37:00.580483: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from keras.datasets import mnist, fashion_mnist\n",
    "import tensorflow.keras as kb\n",
    "from tensorflow.keras import backend\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "Autoencoders are our first experience with *Unsupervised* Neural Networks. The goal of an Autoencoder is to learn how to compress the data with the **encoder** portion of the network, and then uncompress it with the **decoder** portion. \n",
    "\n",
    "In order to enforce this compression we typically either:\n",
    "\n",
    "- have a hidden layer with **low dimensionality** (very few nodes)\n",
    "OR\n",
    "- enforce **sparcity** (many nodes are 0) in the hidden layer\n",
    "\n",
    "## Denoising as Regularization\n",
    "In addition to the two types of penalties we talked about (penalties on the hidden layer and penalties on the derivative) which both help regularize our Autoencoders, we also talked about **Denoising Autoencoders**. DAE's add a little bit of noise (i.e. adding small random values to our input) to our data but *still ask the model to return the original input*. Adding a little bit of noise to the input forces the AE to learn that small changes (noise) to the input should not drastically change what the output should be.\n",
    "\n",
    "\n",
    "## Autoencoders as Non-Linear Principal Component Analysis\n",
    "We often say that autoencoders do non-linear PCA, because like PCA, autoencoders learn to represent *most* of the information in the data using a smaller set of variables. However unlike PCA, the \"components\" or compressed representation of the data doesn't have to be a *linear combination* of the input variables. We can add non-linear activation funcitons, or have a *deep* encoder which allows more complex combinations of the input variables. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1q670OFb8hLd6FhGcBmJcqwfcmw5uz9Ag\" alt=\"Q\" width = \"200px\"/>\n",
    "\n",
    "## Convolutional Autoencoders\n",
    "And the encoder and decoder don't have to be just Dense, Feed Forward layers. We can also use Convolutional and Pooling layers in order to make Convolutional Autoencoders which can compress images.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1aJ2MXRfHqMt07Z4ukICUkRqc2t9Dr8y9\" alt=\"Q\" width = \"400px\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a FF Autoencoder in Keras\n",
    "We can theoretically build an autoencoder using a simple stack of layers (e.g. through `Sequential`), however, we're often interested in using the encoder and decoder *separately* once the model is trained, so we'll often use the Functional API to build our Autoencoders so that we can separate them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 17:30:54.647921: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "inshape = 30\n",
    "encoding_dim = 5\n",
    "\n",
    "input_data = kb.Input(shape = (inshape,), name = \"ae_input\")\n",
    "encoded = kb.layers.Dense(encoding_dim, name = \"encoding\")(input_data)\n",
    "decoded = kb.layers.Dense(inshape, name = \"decoding\")(encoded)\n",
    "\n",
    "# AE\n",
    "autoencoder = kb.Model(inputs = input_data,\n",
    "                       outputs = decoded)\n",
    "\n",
    "# encoder\n",
    "encoder = kb.Model(inputs = input_data,\n",
    "                   outputs = encoded)\n",
    "\n",
    "\n",
    "# decoder\n",
    "encoded_input = kb.Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = kb.Model(inputs = encoded_input,\n",
    "                   outputs = decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " ae_input (InputLayer)       [(None, 30)]              0         \n",
      "                                                                 \n",
      " encoding (Dense)            (None, 5)                 155       \n",
      "                                                                 \n",
      " decoding (Dense)            (None, 30)                180       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 335\n",
      "Trainable params: 335\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " ae_input (InputLayer)       [(None, 30)]              0         \n",
      "                                                                 \n",
      " encoding (Dense)            (None, 5)                 155       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 155\n",
      "Trainable params: 155\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 5)]               0         \n",
      "                                                                 \n",
      " decoding (Dense)            (None, 30)                180       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 180\n",
      "Trainable params: 180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Data\n",
    "\n",
    "Loading in Data to use in Tensorflow can be a little tricky when it's not data that comes pre-loaded with tf. \n",
    "\n",
    "First we're going to load in a Breast Cancer dataset using pandas, and select the columns that we want to put into our autoencoder (all the different features of the breast cancer biopsy).\n",
    "\n",
    "Then we convert it to a tensor...because that's what **tensor**flow expects as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "bc = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/BreastCancer.csv\")\n",
    "\n",
    "# grab necessary columns\n",
    "feat = ['radius_mean', 'texture_mean', 'perimeter_mean',\n",
    "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
    "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
    "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
    "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
    "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
    "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
    "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
    "       'symmetry_worst', 'fractal_dimension_worst']\n",
    "bc_compress = bc[feat]\n",
    "\n",
    "# convert to tensorflow data\n",
    "bc_compress_tf = tf.convert_to_tensor(bc_compress)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll do a train/test split.\n",
    "\n",
    "First we calculate how many data points go in train, and how many go in test. \n",
    "\n",
    "Then we use tensorflow's `split()` function to do the train test split. We need to shuffle our data using `tf.random.shuffle()` first, otherwise it will split our data so that the first rows are in the train and the last rows are in test (this is like setting the `shuffle = True` argument in a `train_test_split()` or `KFold()` in `sklearn`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(456, 30) (113, 30)\n"
     ]
    }
   ],
   "source": [
    "# TTS\n",
    "\n",
    "# calculate the number of data points in each set\n",
    "test_prop = bc_compress_tf.shape[0]//5\n",
    "train_prop = bc_compress_tf.shape[0] - test_prop\n",
    "\n",
    "# split\n",
    "x_train, x_test = tf.split(tf.random.shuffle(bc_compress_tf), #shuffle before splitting\n",
    "                           [train_prop, test_prop], # number of data points in each set\n",
    "                           0) # split the rows, not columns\n",
    "\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "15/15 [==============================] - 1s 24ms/step - loss: 69184.4453 - val_loss: 65629.9453\n",
      "Epoch 2/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 57674.1445 - val_loss: 55410.5898\n",
      "Epoch 3/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 48905.1016 - val_loss: 46991.7070\n",
      "Epoch 4/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 41572.5273 - val_loss: 40188.7930\n",
      "Epoch 5/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 35582.4766 - val_loss: 34381.1289\n",
      "Epoch 6/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 30189.6328 - val_loss: 28873.1895\n",
      "Epoch 7/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 25037.9980 - val_loss: 23488.0391\n",
      "Epoch 8/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 20001.2148 - val_loss: 18454.9961\n",
      "Epoch 9/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 15284.3945 - val_loss: 13829.4482\n",
      "Epoch 10/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 11233.8135 - val_loss: 10026.0684\n",
      "Epoch 11/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 7980.0483 - val_loss: 6966.0566\n",
      "Epoch 12/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5449.9023 - val_loss: 4711.3843\n",
      "Epoch 13/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 3636.3909 - val_loss: 3272.0437\n",
      "Epoch 14/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2485.9114 - val_loss: 2301.6797\n",
      "Epoch 15/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1780.7460 - val_loss: 1783.1450\n",
      "Epoch 16/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1392.3937 - val_loss: 1475.4952\n",
      "Epoch 17/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1167.6241 - val_loss: 1300.0670\n",
      "Epoch 18/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1042.5378 - val_loss: 1194.5098\n",
      "Epoch 19/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 962.8529 - val_loss: 1129.3044\n",
      "Epoch 20/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 916.3347 - val_loss: 1084.4127\n",
      "Epoch 21/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 883.5078 - val_loss: 1057.2679\n",
      "Epoch 22/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 858.5230 - val_loss: 1031.2657\n",
      "Epoch 23/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 843.1089 - val_loss: 1011.8168\n",
      "Epoch 24/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 832.5911 - val_loss: 998.1524\n",
      "Epoch 25/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 819.2738 - val_loss: 989.8397\n",
      "Epoch 26/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 812.7498 - val_loss: 979.9459\n",
      "Epoch 27/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 806.5043 - val_loss: 973.6516\n",
      "Epoch 28/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 797.9788 - val_loss: 965.7115\n",
      "Epoch 29/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 791.0177 - val_loss: 955.4058\n",
      "Epoch 30/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 785.0032 - val_loss: 948.2582\n",
      "Epoch 31/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 782.4108 - val_loss: 942.8094\n",
      "Epoch 32/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 774.7025 - val_loss: 933.6335\n",
      "Epoch 33/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 766.9612 - val_loss: 927.7361\n",
      "Epoch 34/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 761.7762 - val_loss: 922.1129\n",
      "Epoch 35/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 755.4031 - val_loss: 914.1486\n",
      "Epoch 36/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 749.5370 - val_loss: 906.8953\n",
      "Epoch 37/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 745.3892 - val_loss: 899.8107\n",
      "Epoch 38/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 737.5270 - val_loss: 896.9249\n",
      "Epoch 39/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 733.3105 - val_loss: 888.4225\n",
      "Epoch 40/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 727.8503 - val_loss: 880.5840\n",
      "Epoch 41/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 722.5104 - val_loss: 874.6207\n",
      "Epoch 42/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 717.8335 - val_loss: 867.9891\n",
      "Epoch 43/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 712.3285 - val_loss: 863.8908\n",
      "Epoch 44/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 709.2786 - val_loss: 858.1095\n",
      "Epoch 45/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 712.1202 - val_loss: 850.2603\n",
      "Epoch 46/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 699.6654 - val_loss: 843.0156\n",
      "Epoch 47/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 692.0164 - val_loss: 837.5546\n",
      "Epoch 48/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 687.9685 - val_loss: 831.6923\n",
      "Epoch 49/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 693.6840 - val_loss: 828.5364\n",
      "Epoch 50/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 679.8748 - val_loss: 819.9691\n",
      "Epoch 51/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 672.3378 - val_loss: 813.5827\n",
      "Epoch 52/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 666.6838 - val_loss: 808.8083\n",
      "Epoch 53/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 662.9254 - val_loss: 805.8406\n",
      "Epoch 54/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 659.0911 - val_loss: 796.9467\n",
      "Epoch 55/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 654.1268 - val_loss: 794.0229\n",
      "Epoch 56/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 653.7805 - val_loss: 787.4056\n",
      "Epoch 57/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 647.0059 - val_loss: 782.5425\n",
      "Epoch 58/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 641.0917 - val_loss: 778.4276\n",
      "Epoch 59/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 636.5289 - val_loss: 773.3594\n",
      "Epoch 60/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 633.5137 - val_loss: 768.8279\n",
      "Epoch 61/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 630.1043 - val_loss: 763.9537\n",
      "Epoch 62/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 628.0212 - val_loss: 759.6213\n",
      "Epoch 63/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 623.0112 - val_loss: 756.3615\n",
      "Epoch 64/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 619.4260 - val_loss: 751.1161\n",
      "Epoch 65/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 614.2519 - val_loss: 749.6184\n",
      "Epoch 66/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 612.6380 - val_loss: 744.9205\n",
      "Epoch 67/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 609.4911 - val_loss: 740.0432\n",
      "Epoch 68/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 604.8685 - val_loss: 735.4196\n",
      "Epoch 69/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 600.9705 - val_loss: 731.4052\n",
      "Epoch 70/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 596.7685 - val_loss: 726.2644\n",
      "Epoch 71/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 594.8953 - val_loss: 722.4798\n",
      "Epoch 72/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 597.2817 - val_loss: 718.9139\n",
      "Epoch 73/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 592.3146 - val_loss: 716.4807\n",
      "Epoch 74/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 587.3331 - val_loss: 711.4580\n",
      "Epoch 75/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 582.4543 - val_loss: 708.3678\n",
      "Epoch 76/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 580.2094 - val_loss: 705.6811\n",
      "Epoch 77/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 576.4639 - val_loss: 702.5574\n",
      "Epoch 78/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 573.2495 - val_loss: 697.3264\n",
      "Epoch 79/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 571.0143 - val_loss: 693.8197\n",
      "Epoch 80/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 570.3171 - val_loss: 689.8699\n",
      "Epoch 81/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 565.5394 - val_loss: 686.8785\n",
      "Epoch 82/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 563.3165 - val_loss: 684.9178\n",
      "Epoch 83/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 559.8577 - val_loss: 680.5173\n",
      "Epoch 84/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 559.2458 - val_loss: 678.4730\n",
      "Epoch 85/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 555.4066 - val_loss: 673.9655\n",
      "Epoch 86/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 554.0348 - val_loss: 671.4816\n",
      "Epoch 87/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 549.6068 - val_loss: 667.3734\n",
      "Epoch 88/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 547.3657 - val_loss: 664.2666\n",
      "Epoch 89/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 545.3250 - val_loss: 661.6104\n",
      "Epoch 90/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 541.8209 - val_loss: 658.5677\n",
      "Epoch 91/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 540.1237 - val_loss: 655.8046\n",
      "Epoch 92/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 537.3066 - val_loss: 653.0886\n",
      "Epoch 93/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 537.8524 - val_loss: 651.6623\n",
      "Epoch 94/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 533.9846 - val_loss: 648.1548\n",
      "Epoch 95/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 530.5111 - val_loss: 645.3271\n",
      "Epoch 96/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 532.1539 - val_loss: 644.5919\n",
      "Epoch 97/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 531.2012 - val_loss: 640.0441\n",
      "Epoch 98/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 527.0230 - val_loss: 638.8613\n",
      "Epoch 99/500\n",
      "15/15 [==============================] - 0s 10ms/step - loss: 525.2331 - val_loss: 634.6462\n",
      "Epoch 100/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 520.7205 - val_loss: 632.4529\n",
      "Epoch 101/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 519.1512 - val_loss: 629.5029\n",
      "Epoch 102/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 516.7949 - val_loss: 626.6731\n",
      "Epoch 103/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 516.3139 - val_loss: 625.3417\n",
      "Epoch 104/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 512.9901 - val_loss: 622.6562\n",
      "Epoch 105/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 510.6870 - val_loss: 620.1234\n",
      "Epoch 106/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 510.0174 - val_loss: 619.4088\n",
      "Epoch 107/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 508.2993 - val_loss: 616.0168\n",
      "Epoch 108/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 507.1373 - val_loss: 615.9713\n",
      "Epoch 109/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 503.8203 - val_loss: 612.5819\n",
      "Epoch 110/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 501.9383 - val_loss: 609.7170\n",
      "Epoch 111/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 500.4556 - val_loss: 606.4807\n",
      "Epoch 112/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 498.2667 - val_loss: 604.6358\n",
      "Epoch 113/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 497.2238 - val_loss: 602.4160\n",
      "Epoch 114/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 494.0997 - val_loss: 600.5747\n",
      "Epoch 115/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 492.6762 - val_loss: 599.5610\n",
      "Epoch 116/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 492.6514 - val_loss: 598.3852\n",
      "Epoch 117/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 490.9680 - val_loss: 596.6074\n",
      "Epoch 118/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 489.6502 - val_loss: 593.5148\n",
      "Epoch 119/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 487.1007 - val_loss: 592.5024\n",
      "Epoch 120/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 485.9333 - val_loss: 590.7576\n",
      "Epoch 121/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 486.6527 - val_loss: 589.3209\n",
      "Epoch 122/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 482.6942 - val_loss: 586.4266\n",
      "Epoch 123/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 481.1972 - val_loss: 584.2059\n",
      "Epoch 124/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 480.4572 - val_loss: 582.4446\n",
      "Epoch 125/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 479.8509 - val_loss: 582.7045\n",
      "Epoch 126/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 487.5356 - val_loss: 579.2131\n",
      "Epoch 127/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 477.4085 - val_loss: 579.1944\n",
      "Epoch 128/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 480.5915 - val_loss: 576.1978\n",
      "Epoch 129/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 474.2662 - val_loss: 576.5845\n",
      "Epoch 130/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 472.4807 - val_loss: 574.4043\n",
      "Epoch 131/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 470.1384 - val_loss: 571.8297\n",
      "Epoch 132/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 470.1898 - val_loss: 570.1340\n",
      "Epoch 133/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 466.9269 - val_loss: 569.5676\n",
      "Epoch 134/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 468.0639 - val_loss: 567.4391\n",
      "Epoch 135/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 468.3760 - val_loss: 565.2808\n",
      "Epoch 136/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 467.2933 - val_loss: 564.6904\n",
      "Epoch 137/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 462.7409 - val_loss: 562.5360\n",
      "Epoch 138/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 462.0367 - val_loss: 561.0964\n",
      "Epoch 139/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 461.0673 - val_loss: 559.1464\n",
      "Epoch 140/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 460.4205 - val_loss: 558.6345\n",
      "Epoch 141/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 459.5586 - val_loss: 556.1967\n",
      "Epoch 142/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 459.6494 - val_loss: 556.3988\n",
      "Epoch 143/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 459.9304 - val_loss: 553.0896\n",
      "Epoch 144/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 454.7046 - val_loss: 553.0051\n",
      "Epoch 145/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 454.8849 - val_loss: 550.2825\n",
      "Epoch 146/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 453.9670 - val_loss: 549.3921\n",
      "Epoch 147/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 452.5759 - val_loss: 547.5423\n",
      "Epoch 148/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 451.1538 - val_loss: 546.6702\n",
      "Epoch 149/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 450.9794 - val_loss: 545.8847\n",
      "Epoch 150/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 449.9233 - val_loss: 544.7223\n",
      "Epoch 151/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 447.2629 - val_loss: 543.1934\n",
      "Epoch 152/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 448.1397 - val_loss: 542.3798\n",
      "Epoch 153/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 451.8326 - val_loss: 544.9922\n",
      "Epoch 154/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 446.1232 - val_loss: 540.1669\n",
      "Epoch 155/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 445.7915 - val_loss: 539.2084\n",
      "Epoch 156/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 443.7570 - val_loss: 537.0618\n",
      "Epoch 157/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 440.8455 - val_loss: 535.8738\n",
      "Epoch 158/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 440.2831 - val_loss: 534.4501\n",
      "Epoch 159/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 439.9504 - val_loss: 534.3164\n",
      "Epoch 160/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 438.8065 - val_loss: 532.9113\n",
      "Epoch 161/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 438.6360 - val_loss: 530.8624\n",
      "Epoch 162/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 436.7616 - val_loss: 530.1837\n",
      "Epoch 163/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 442.2272 - val_loss: 534.3453\n",
      "Epoch 164/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 440.4547 - val_loss: 531.0128\n",
      "Epoch 165/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 434.9018 - val_loss: 527.4269\n",
      "Epoch 166/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 435.1267 - val_loss: 526.9261\n",
      "Epoch 167/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 433.9099 - val_loss: 525.7158\n",
      "Epoch 168/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 435.7845 - val_loss: 524.0161\n",
      "Epoch 169/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 431.4300 - val_loss: 523.1947\n",
      "Epoch 170/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 431.1074 - val_loss: 522.9604\n",
      "Epoch 171/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 430.3801 - val_loss: 520.9911\n",
      "Epoch 172/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 429.8028 - val_loss: 520.7366\n",
      "Epoch 173/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 430.1507 - val_loss: 520.5203\n",
      "Epoch 174/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 426.9166 - val_loss: 518.2974\n",
      "Epoch 175/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 425.8879 - val_loss: 517.5338\n",
      "Epoch 176/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 426.0246 - val_loss: 516.5607\n",
      "Epoch 177/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 429.7614 - val_loss: 515.6190\n",
      "Epoch 178/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 424.0499 - val_loss: 515.1611\n",
      "Epoch 179/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 424.5737 - val_loss: 513.9592\n",
      "Epoch 180/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 424.8286 - val_loss: 513.3002\n",
      "Epoch 181/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 422.5308 - val_loss: 513.3400\n",
      "Epoch 182/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 421.7286 - val_loss: 511.4947\n",
      "Epoch 183/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 420.1184 - val_loss: 510.3224\n",
      "Epoch 184/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 420.7525 - val_loss: 509.8806\n",
      "Epoch 185/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 419.6558 - val_loss: 507.9395\n",
      "Epoch 186/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 420.5680 - val_loss: 508.4452\n",
      "Epoch 187/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 420.9368 - val_loss: 508.1013\n",
      "Epoch 188/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 417.7945 - val_loss: 506.5086\n",
      "Epoch 189/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 416.5369 - val_loss: 504.8481\n",
      "Epoch 190/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 417.5786 - val_loss: 504.3942\n",
      "Epoch 191/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 416.0433 - val_loss: 504.6465\n",
      "Epoch 192/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 415.0903 - val_loss: 502.4727\n",
      "Epoch 193/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 415.4688 - val_loss: 501.7604\n",
      "Epoch 194/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 414.3499 - val_loss: 501.2144\n",
      "Epoch 195/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 411.9899 - val_loss: 499.8449\n",
      "Epoch 196/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 411.7427 - val_loss: 499.8871\n",
      "Epoch 197/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 411.8287 - val_loss: 498.7024\n",
      "Epoch 198/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 410.1828 - val_loss: 497.8287\n",
      "Epoch 199/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 409.2842 - val_loss: 496.7968\n",
      "Epoch 200/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 408.4057 - val_loss: 495.9105\n",
      "Epoch 201/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 410.1688 - val_loss: 495.9220\n",
      "Epoch 202/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 411.0680 - val_loss: 497.2191\n",
      "Epoch 203/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 408.7272 - val_loss: 495.1660\n",
      "Epoch 204/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 406.6686 - val_loss: 494.6933\n",
      "Epoch 205/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 405.7914 - val_loss: 493.7639\n",
      "Epoch 206/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 407.9770 - val_loss: 492.1592\n",
      "Epoch 207/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 406.2126 - val_loss: 492.1587\n",
      "Epoch 208/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 406.1281 - val_loss: 491.5182\n",
      "Epoch 209/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 406.2280 - val_loss: 492.0029\n",
      "Epoch 210/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 403.9512 - val_loss: 490.4132\n",
      "Epoch 211/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 403.1367 - val_loss: 488.9993\n",
      "Epoch 212/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 403.3013 - val_loss: 487.9618\n",
      "Epoch 213/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 400.4732 - val_loss: 487.4593\n",
      "Epoch 214/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 401.1355 - val_loss: 486.9060\n",
      "Epoch 215/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 399.6938 - val_loss: 486.5276\n",
      "Epoch 216/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 398.6436 - val_loss: 485.0351\n",
      "Epoch 217/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 397.9954 - val_loss: 484.0191\n",
      "Epoch 218/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 398.0508 - val_loss: 484.0446\n",
      "Epoch 219/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 398.9223 - val_loss: 483.3881\n",
      "Epoch 220/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 397.1244 - val_loss: 483.8107\n",
      "Epoch 221/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 396.5611 - val_loss: 482.1487\n",
      "Epoch 222/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 396.0805 - val_loss: 481.5695\n",
      "Epoch 223/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 395.0478 - val_loss: 480.0823\n",
      "Epoch 224/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 395.1118 - val_loss: 479.5263\n",
      "Epoch 225/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 392.9214 - val_loss: 479.5399\n",
      "Epoch 226/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 404.3889 - val_loss: 480.2458\n",
      "Epoch 227/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 394.6809 - val_loss: 478.8661\n",
      "Epoch 228/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 399.1954 - val_loss: 478.9325\n",
      "Epoch 229/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 393.5763 - val_loss: 475.6956\n",
      "Epoch 230/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 391.6053 - val_loss: 476.6428\n",
      "Epoch 231/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 390.4183 - val_loss: 473.9518\n",
      "Epoch 232/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 392.7083 - val_loss: 474.5865\n",
      "Epoch 233/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 388.4427 - val_loss: 473.1461\n",
      "Epoch 234/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 388.4725 - val_loss: 472.9735\n",
      "Epoch 235/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 386.5202 - val_loss: 471.9917\n",
      "Epoch 236/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 390.6523 - val_loss: 471.2567\n",
      "Epoch 237/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 385.2638 - val_loss: 472.2385\n",
      "Epoch 238/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 386.0255 - val_loss: 470.1512\n",
      "Epoch 239/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 383.9190 - val_loss: 469.3263\n",
      "Epoch 240/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 384.1499 - val_loss: 468.7425\n",
      "Epoch 241/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 389.2220 - val_loss: 469.5297\n",
      "Epoch 242/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 382.5403 - val_loss: 467.0712\n",
      "Epoch 243/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 381.6747 - val_loss: 465.8680\n",
      "Epoch 244/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 381.4208 - val_loss: 465.7435\n",
      "Epoch 245/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 384.1990 - val_loss: 467.8847\n",
      "Epoch 246/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 381.4014 - val_loss: 466.9855\n",
      "Epoch 247/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 379.8727 - val_loss: 464.6456\n",
      "Epoch 248/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 379.3444 - val_loss: 462.3271\n",
      "Epoch 249/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 377.8208 - val_loss: 461.3867\n",
      "Epoch 250/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 377.1194 - val_loss: 461.6591\n",
      "Epoch 251/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 376.4245 - val_loss: 461.2220\n",
      "Epoch 252/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 376.5312 - val_loss: 460.9172\n",
      "Epoch 253/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 374.1932 - val_loss: 459.0698\n",
      "Epoch 254/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 375.0592 - val_loss: 458.1183\n",
      "Epoch 255/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 373.9542 - val_loss: 457.4096\n",
      "Epoch 256/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 375.0325 - val_loss: 456.4619\n",
      "Epoch 257/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 373.0080 - val_loss: 456.0178\n",
      "Epoch 258/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 372.6522 - val_loss: 456.1437\n",
      "Epoch 259/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 370.7883 - val_loss: 455.0479\n",
      "Epoch 260/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 370.6348 - val_loss: 453.0807\n",
      "Epoch 261/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 368.4030 - val_loss: 452.3481\n",
      "Epoch 262/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 367.5154 - val_loss: 451.4603\n",
      "Epoch 263/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 367.5000 - val_loss: 450.6118\n",
      "Epoch 264/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 366.6599 - val_loss: 449.8456\n",
      "Epoch 265/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 366.4637 - val_loss: 449.0146\n",
      "Epoch 266/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 364.7081 - val_loss: 448.3378\n",
      "Epoch 267/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 364.0832 - val_loss: 447.0084\n",
      "Epoch 268/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 362.5997 - val_loss: 447.4321\n",
      "Epoch 269/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 364.5474 - val_loss: 445.6672\n",
      "Epoch 270/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 363.3507 - val_loss: 444.1800\n",
      "Epoch 271/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 360.1404 - val_loss: 443.9905\n",
      "Epoch 272/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 359.2572 - val_loss: 443.1003\n",
      "Epoch 273/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 361.9994 - val_loss: 441.3044\n",
      "Epoch 274/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 361.3998 - val_loss: 444.9230\n",
      "Epoch 275/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 357.6414 - val_loss: 441.9907\n",
      "Epoch 276/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 356.4143 - val_loss: 439.3505\n",
      "Epoch 277/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 354.6202 - val_loss: 437.6428\n",
      "Epoch 278/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 353.1826 - val_loss: 436.3342\n",
      "Epoch 279/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 357.3119 - val_loss: 435.9409\n",
      "Epoch 280/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 352.0112 - val_loss: 434.1434\n",
      "Epoch 281/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 350.1132 - val_loss: 432.7417\n",
      "Epoch 282/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 350.0113 - val_loss: 431.2915\n",
      "Epoch 283/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 349.4557 - val_loss: 430.5734\n",
      "Epoch 284/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 347.7508 - val_loss: 430.6028\n",
      "Epoch 285/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 347.8147 - val_loss: 427.6635\n",
      "Epoch 286/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 344.0045 - val_loss: 427.4208\n",
      "Epoch 287/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 344.3622 - val_loss: 427.6420\n",
      "Epoch 288/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 344.4756 - val_loss: 423.6418\n",
      "Epoch 289/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 340.2604 - val_loss: 422.8383\n",
      "Epoch 290/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 340.1766 - val_loss: 422.8894\n",
      "Epoch 291/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 338.2366 - val_loss: 420.3091\n",
      "Epoch 292/500\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 336.6785 - val_loss: 418.4181\n",
      "Epoch 293/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 337.0060 - val_loss: 417.1048\n",
      "Epoch 294/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 334.5399 - val_loss: 415.7408\n",
      "Epoch 295/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 332.7304 - val_loss: 414.9438\n",
      "Epoch 296/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 332.2014 - val_loss: 414.4528\n",
      "Epoch 297/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 333.6725 - val_loss: 411.7408\n",
      "Epoch 298/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 328.8972 - val_loss: 409.6683\n",
      "Epoch 299/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 327.6942 - val_loss: 408.5470\n",
      "Epoch 300/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 327.1846 - val_loss: 406.8024\n",
      "Epoch 301/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 325.0961 - val_loss: 405.4521\n",
      "Epoch 302/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 326.3833 - val_loss: 404.2174\n",
      "Epoch 303/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 322.8731 - val_loss: 402.2359\n",
      "Epoch 304/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 320.7758 - val_loss: 400.9252\n",
      "Epoch 305/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 318.5645 - val_loss: 399.2456\n",
      "Epoch 306/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 316.0670 - val_loss: 397.8550\n",
      "Epoch 307/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 315.8350 - val_loss: 397.8708\n",
      "Epoch 308/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 315.2822 - val_loss: 393.8083\n",
      "Epoch 309/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 312.5650 - val_loss: 392.0441\n",
      "Epoch 310/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 310.3857 - val_loss: 390.4759\n",
      "Epoch 311/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 310.4106 - val_loss: 389.0267\n",
      "Epoch 312/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 308.7097 - val_loss: 386.8857\n",
      "Epoch 313/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 305.4738 - val_loss: 386.0349\n",
      "Epoch 314/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 305.0499 - val_loss: 384.6572\n",
      "Epoch 315/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 302.3055 - val_loss: 381.1520\n",
      "Epoch 316/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 300.3626 - val_loss: 379.7961\n",
      "Epoch 317/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 298.4850 - val_loss: 377.5704\n",
      "Epoch 318/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 298.0144 - val_loss: 376.1606\n",
      "Epoch 319/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 300.8615 - val_loss: 374.8797\n",
      "Epoch 320/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 295.7393 - val_loss: 373.0505\n",
      "Epoch 321/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 294.7366 - val_loss: 372.3841\n",
      "Epoch 322/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 292.9706 - val_loss: 369.0036\n",
      "Epoch 323/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 290.0602 - val_loss: 366.9202\n",
      "Epoch 324/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 295.5824 - val_loss: 367.6347\n",
      "Epoch 325/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 287.1419 - val_loss: 361.1438\n",
      "Epoch 326/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 284.3922 - val_loss: 359.1307\n",
      "Epoch 327/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 281.6354 - val_loss: 357.3868\n",
      "Epoch 328/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 280.1033 - val_loss: 355.7174\n",
      "Epoch 329/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 277.5768 - val_loss: 354.2921\n",
      "Epoch 330/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 276.3912 - val_loss: 351.4828\n",
      "Epoch 331/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 274.5140 - val_loss: 349.6965\n",
      "Epoch 332/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 274.8526 - val_loss: 348.0294\n",
      "Epoch 333/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 273.0838 - val_loss: 346.5338\n",
      "Epoch 334/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 270.6550 - val_loss: 343.7823\n",
      "Epoch 335/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 269.2205 - val_loss: 346.3575\n",
      "Epoch 336/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 268.7872 - val_loss: 342.0601\n",
      "Epoch 337/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 264.8351 - val_loss: 337.8879\n",
      "Epoch 338/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 263.7520 - val_loss: 335.9760\n",
      "Epoch 339/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 261.8176 - val_loss: 333.7116\n",
      "Epoch 340/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 260.2190 - val_loss: 332.5672\n",
      "Epoch 341/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 259.4918 - val_loss: 331.4329\n",
      "Epoch 342/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 260.1252 - val_loss: 327.8081\n",
      "Epoch 343/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 257.1499 - val_loss: 326.9353\n",
      "Epoch 344/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 255.2540 - val_loss: 324.8945\n",
      "Epoch 345/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 252.9060 - val_loss: 321.4485\n",
      "Epoch 346/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 251.6610 - val_loss: 319.9301\n",
      "Epoch 347/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 249.3125 - val_loss: 317.5477\n",
      "Epoch 348/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 250.4728 - val_loss: 317.9045\n",
      "Epoch 349/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 246.0541 - val_loss: 313.4231\n",
      "Epoch 350/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 244.1968 - val_loss: 311.1211\n",
      "Epoch 351/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 243.7535 - val_loss: 310.2784\n",
      "Epoch 352/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 241.7030 - val_loss: 308.0156\n",
      "Epoch 353/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 240.2068 - val_loss: 305.9458\n",
      "Epoch 354/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 238.7957 - val_loss: 303.1836\n",
      "Epoch 355/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 236.7067 - val_loss: 300.8866\n",
      "Epoch 356/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 235.3944 - val_loss: 299.5467\n",
      "Epoch 357/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 237.1663 - val_loss: 296.8132\n",
      "Epoch 358/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 234.5988 - val_loss: 296.1135\n",
      "Epoch 359/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 230.2222 - val_loss: 292.6236\n",
      "Epoch 360/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 229.9945 - val_loss: 290.8647\n",
      "Epoch 361/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 228.4005 - val_loss: 287.3681\n",
      "Epoch 362/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 224.8678 - val_loss: 285.9009\n",
      "Epoch 363/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 222.8691 - val_loss: 283.0574\n",
      "Epoch 364/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 220.8367 - val_loss: 280.3592\n",
      "Epoch 365/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 221.1927 - val_loss: 278.4607\n",
      "Epoch 366/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 219.8036 - val_loss: 277.5623\n",
      "Epoch 367/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 219.0117 - val_loss: 273.9148\n",
      "Epoch 368/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 216.4248 - val_loss: 272.8947\n",
      "Epoch 369/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 215.1121 - val_loss: 269.3878\n",
      "Epoch 370/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 212.0014 - val_loss: 267.6217\n",
      "Epoch 371/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 211.2010 - val_loss: 266.6394\n",
      "Epoch 372/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 209.9266 - val_loss: 262.5211\n",
      "Epoch 373/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 206.9682 - val_loss: 260.3441\n",
      "Epoch 374/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 205.1580 - val_loss: 257.6738\n",
      "Epoch 375/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 204.2624 - val_loss: 255.5895\n",
      "Epoch 376/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 206.1700 - val_loss: 255.5700\n",
      "Epoch 377/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 201.1631 - val_loss: 250.9706\n",
      "Epoch 378/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 198.6113 - val_loss: 248.7666\n",
      "Epoch 379/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 197.6478 - val_loss: 246.5106\n",
      "Epoch 380/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 202.9203 - val_loss: 244.3246\n",
      "Epoch 381/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 195.0477 - val_loss: 240.9635\n",
      "Epoch 382/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 191.3156 - val_loss: 238.1643\n",
      "Epoch 383/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 190.9520 - val_loss: 236.2283\n",
      "Epoch 384/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 189.8226 - val_loss: 235.3928\n",
      "Epoch 385/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 189.1887 - val_loss: 233.9203\n",
      "Epoch 386/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 184.6468 - val_loss: 229.7607\n",
      "Epoch 387/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 184.5344 - val_loss: 227.1546\n",
      "Epoch 388/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 183.3763 - val_loss: 225.6944\n",
      "Epoch 389/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 181.0934 - val_loss: 223.1431\n",
      "Epoch 390/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 180.1846 - val_loss: 220.4733\n",
      "Epoch 391/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 178.3802 - val_loss: 218.3793\n",
      "Epoch 392/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 179.1256 - val_loss: 217.2002\n",
      "Epoch 393/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 185.5841 - val_loss: 224.6517\n",
      "Epoch 394/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 180.0299 - val_loss: 213.0443\n",
      "Epoch 395/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 171.7982 - val_loss: 210.2076\n",
      "Epoch 396/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 171.8504 - val_loss: 208.6024\n",
      "Epoch 397/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 170.2542 - val_loss: 205.9246\n",
      "Epoch 398/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 167.9704 - val_loss: 203.4448\n",
      "Epoch 399/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 167.1395 - val_loss: 201.5476\n",
      "Epoch 400/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 164.6515 - val_loss: 199.8188\n",
      "Epoch 401/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 175.9849 - val_loss: 200.9891\n",
      "Epoch 402/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 161.3116 - val_loss: 195.4076\n",
      "Epoch 403/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 161.3966 - val_loss: 192.7623\n",
      "Epoch 404/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 159.7713 - val_loss: 190.2534\n",
      "Epoch 405/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 158.0834 - val_loss: 187.9757\n",
      "Epoch 406/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 155.9352 - val_loss: 185.2022\n",
      "Epoch 407/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 156.3416 - val_loss: 183.3674\n",
      "Epoch 408/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 152.6869 - val_loss: 181.5488\n",
      "Epoch 409/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 152.6505 - val_loss: 180.1680\n",
      "Epoch 410/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 150.4191 - val_loss: 178.4435\n",
      "Epoch 411/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 150.5092 - val_loss: 175.7175\n",
      "Epoch 412/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 148.4302 - val_loss: 173.9553\n",
      "Epoch 413/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 146.5024 - val_loss: 171.8450\n",
      "Epoch 414/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 144.8367 - val_loss: 169.7754\n",
      "Epoch 415/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 143.4878 - val_loss: 168.3358\n",
      "Epoch 416/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 142.5103 - val_loss: 166.0851\n",
      "Epoch 417/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 142.1258 - val_loss: 164.3431\n",
      "Epoch 418/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 141.1840 - val_loss: 162.2839\n",
      "Epoch 419/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 138.6591 - val_loss: 160.3826\n",
      "Epoch 420/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 137.1862 - val_loss: 158.0964\n",
      "Epoch 421/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 135.9859 - val_loss: 156.2193\n",
      "Epoch 422/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 134.5453 - val_loss: 154.4374\n",
      "Epoch 423/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 133.6801 - val_loss: 152.6546\n",
      "Epoch 424/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 133.2841 - val_loss: 151.3816\n",
      "Epoch 425/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 131.6468 - val_loss: 149.4276\n",
      "Epoch 426/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 130.6653 - val_loss: 147.9663\n",
      "Epoch 427/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 128.7771 - val_loss: 145.6574\n",
      "Epoch 428/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 128.2658 - val_loss: 144.2195\n",
      "Epoch 429/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 127.0448 - val_loss: 142.4281\n",
      "Epoch 430/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 126.6837 - val_loss: 141.2504\n",
      "Epoch 431/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 124.9325 - val_loss: 139.5409\n",
      "Epoch 432/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 123.3405 - val_loss: 137.1295\n",
      "Epoch 433/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 125.4885 - val_loss: 136.4270\n",
      "Epoch 434/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 124.6357 - val_loss: 135.5534\n",
      "Epoch 435/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 123.3677 - val_loss: 133.4200\n",
      "Epoch 436/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 119.7764 - val_loss: 131.4415\n",
      "Epoch 437/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 120.1772 - val_loss: 129.9682\n",
      "Epoch 438/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 121.4232 - val_loss: 129.1747\n",
      "Epoch 439/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 118.8516 - val_loss: 128.4531\n",
      "Epoch 440/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 116.5955 - val_loss: 124.9408\n",
      "Epoch 441/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 114.2275 - val_loss: 124.0455\n",
      "Epoch 442/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 113.3790 - val_loss: 122.4289\n",
      "Epoch 443/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 113.8826 - val_loss: 122.0461\n",
      "Epoch 444/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 112.6387 - val_loss: 119.9946\n",
      "Epoch 445/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 111.7251 - val_loss: 118.2373\n",
      "Epoch 446/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 110.7928 - val_loss: 117.8977\n",
      "Epoch 447/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 108.6795 - val_loss: 115.6293\n",
      "Epoch 448/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 110.9432 - val_loss: 114.8106\n",
      "Epoch 449/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 108.0518 - val_loss: 113.0003\n",
      "Epoch 450/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 105.6063 - val_loss: 112.2870\n",
      "Epoch 451/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 106.8534 - val_loss: 110.2627\n",
      "Epoch 452/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 105.0555 - val_loss: 108.4915\n",
      "Epoch 453/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 103.9602 - val_loss: 107.3149\n",
      "Epoch 454/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 102.2439 - val_loss: 105.7095\n",
      "Epoch 455/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 101.3525 - val_loss: 104.6046\n",
      "Epoch 456/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 102.1314 - val_loss: 103.4921\n",
      "Epoch 457/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 99.2040 - val_loss: 101.9336\n",
      "Epoch 458/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 99.9251 - val_loss: 101.6155\n",
      "Epoch 459/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 97.5938 - val_loss: 99.6394\n",
      "Epoch 460/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 97.9661 - val_loss: 98.5921\n",
      "Epoch 461/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 96.1936 - val_loss: 97.4335\n",
      "Epoch 462/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 96.4643 - val_loss: 96.5731\n",
      "Epoch 463/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 95.8945 - val_loss: 95.5287\n",
      "Epoch 464/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 94.6638 - val_loss: 94.1167\n",
      "Epoch 465/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 94.1206 - val_loss: 93.5311\n",
      "Epoch 466/500\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 92.6183 - val_loss: 91.9578\n",
      "Epoch 467/500\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 92.9407 - val_loss: 90.8618\n",
      "Epoch 468/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 93.8421 - val_loss: 90.2163\n",
      "Epoch 469/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 91.0760 - val_loss: 93.4052\n",
      "Epoch 470/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 88.9072 - val_loss: 88.2466\n",
      "Epoch 471/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 92.7393 - val_loss: 86.5076\n",
      "Epoch 472/500\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 88.9832 - val_loss: 86.3262\n",
      "Epoch 473/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 87.7934 - val_loss: 84.4045\n",
      "Epoch 474/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 87.8025 - val_loss: 83.3243\n",
      "Epoch 475/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 85.0034 - val_loss: 82.5285\n",
      "Epoch 476/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 85.3326 - val_loss: 81.2307\n",
      "Epoch 477/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 85.6498 - val_loss: 81.1509\n",
      "Epoch 478/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 84.2183 - val_loss: 80.2664\n",
      "Epoch 479/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 85.7173 - val_loss: 79.0183\n",
      "Epoch 480/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 82.7239 - val_loss: 77.5971\n",
      "Epoch 481/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 80.8571 - val_loss: 76.4345\n",
      "Epoch 482/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 80.7720 - val_loss: 75.7877\n",
      "Epoch 483/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 79.9614 - val_loss: 74.7174\n",
      "Epoch 484/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 80.2673 - val_loss: 73.9204\n",
      "Epoch 485/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 78.7415 - val_loss: 73.0419\n",
      "Epoch 486/500\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 77.9653 - val_loss: 72.1903\n",
      "Epoch 487/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 78.1762 - val_loss: 71.8745\n",
      "Epoch 488/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 78.2750 - val_loss: 71.7860\n",
      "Epoch 489/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 78.1391 - val_loss: 70.3364\n",
      "Epoch 490/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 77.1657 - val_loss: 70.0796\n",
      "Epoch 491/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 76.3023 - val_loss: 68.9188\n",
      "Epoch 492/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 75.8128 - val_loss: 68.3145\n",
      "Epoch 493/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 73.8239 - val_loss: 67.2891\n",
      "Epoch 494/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 75.1418 - val_loss: 66.6293\n",
      "Epoch 495/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 73.0506 - val_loss: 66.0230\n",
      "Epoch 496/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 72.5242 - val_loss: 64.7755\n",
      "Epoch 497/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 72.3747 - val_loss: 64.4180\n",
      "Epoch 498/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 71.4692 - val_loss: 63.6318\n",
      "Epoch 499/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 71.0935 - val_loss: 62.7445\n",
      "Epoch 500/500\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 70.7833 - val_loss: 63.0616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3e55cdf70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile and train\n",
    "\n",
    "autoencoder.compile(loss = \"mean_squared_error\", optimizer = \"adam\")\n",
    "autoencoder.fit(x_train, x_train, \n",
    "                epochs = 500,\n",
    "                validation_data = (x_test, x_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! We trained an autoencoder to compress our breast cancer data. \n",
    "\n",
    "We now have a few options of what to do with it. One option is to use the `encoder` to compress all our our data and then feed that data through a predictive model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 5) (114, 5)\n",
      "(455, 1) (114, 1)\n"
     ]
    }
   ],
   "source": [
    "# one way to do this using sklearn's tts\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(bc[feat],\n",
    "                                                    bc[\"diagnosis\"],\n",
    "                                                    test_size = 0.2)\n",
    "\n",
    "x_train_compressed = encoder(tf.convert_to_tensor(x_train))\n",
    "x_test_compressed = encoder(tf.convert_to_tensor(x_test))\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "y_train = tf.convert_to_tensor(y_train)\n",
    "y_test = tf.convert_to_tensor(y_test)\n",
    "\n",
    "print(x_train_compressed.shape, x_test_compressed.shape)\n",
    "\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 12ms/step - loss: 1.2223 - accuracy: 0.5714 - val_loss: 0.6896 - val_accuracy: 0.6754\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6901 - accuracy: 0.6154 - val_loss: 0.6871 - val_accuracy: 0.6754\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6885 - accuracy: 0.6154 - val_loss: 0.6846 - val_accuracy: 0.6754\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6870 - accuracy: 0.6154 - val_loss: 0.6822 - val_accuracy: 0.6754\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6855 - accuracy: 0.6154 - val_loss: 0.6798 - val_accuracy: 0.6754\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6841 - accuracy: 0.6154 - val_loss: 0.6773 - val_accuracy: 0.6754\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6826 - accuracy: 0.6154 - val_loss: 0.6752 - val_accuracy: 0.6754\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6814 - accuracy: 0.6154 - val_loss: 0.6734 - val_accuracy: 0.6754\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6804 - accuracy: 0.6154 - val_loss: 0.6716 - val_accuracy: 0.6754\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6794 - accuracy: 0.6154 - val_loss: 0.6700 - val_accuracy: 0.6754\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6785 - accuracy: 0.6154 - val_loss: 0.6688 - val_accuracy: 0.6754\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6778 - accuracy: 0.6154 - val_loss: 0.6673 - val_accuracy: 0.6754\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6770 - accuracy: 0.6154 - val_loss: 0.6655 - val_accuracy: 0.6754\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6761 - accuracy: 0.6154 - val_loss: 0.6641 - val_accuracy: 0.6754\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6753 - accuracy: 0.6154 - val_loss: 0.6630 - val_accuracy: 0.6754\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6748 - accuracy: 0.6154 - val_loss: 0.6617 - val_accuracy: 0.6754\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6741 - accuracy: 0.6154 - val_loss: 0.6605 - val_accuracy: 0.6754\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 0.6154 - val_loss: 0.6593 - val_accuracy: 0.6754\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6730 - accuracy: 0.6154 - val_loss: 0.6588 - val_accuracy: 0.6754\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6727 - accuracy: 0.6154 - val_loss: 0.6577 - val_accuracy: 0.6754\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6722 - accuracy: 0.6154 - val_loss: 0.6570 - val_accuracy: 0.6754\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6719 - accuracy: 0.6154 - val_loss: 0.6561 - val_accuracy: 0.6754\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6715 - accuracy: 0.6154 - val_loss: 0.6551 - val_accuracy: 0.6754\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6710 - accuracy: 0.6154 - val_loss: 0.6545 - val_accuracy: 0.6754\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6708 - accuracy: 0.6154 - val_loss: 0.6536 - val_accuracy: 0.6754\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6704 - accuracy: 0.6154 - val_loss: 0.6530 - val_accuracy: 0.6754\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6702 - accuracy: 0.6154 - val_loss: 0.6521 - val_accuracy: 0.6754\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6698 - accuracy: 0.6154 - val_loss: 0.6516 - val_accuracy: 0.6754\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6696 - accuracy: 0.6154 - val_loss: 0.6510 - val_accuracy: 0.6754\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6694 - accuracy: 0.6154 - val_loss: 0.6504 - val_accuracy: 0.6754\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6692 - accuracy: 0.6154 - val_loss: 0.6502 - val_accuracy: 0.6754\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6691 - accuracy: 0.6154 - val_loss: 0.6498 - val_accuracy: 0.6754\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6690 - accuracy: 0.6154 - val_loss: 0.6494 - val_accuracy: 0.6754\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6688 - accuracy: 0.6154 - val_loss: 0.6490 - val_accuracy: 0.6754\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6687 - accuracy: 0.6154 - val_loss: 0.6486 - val_accuracy: 0.6754\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6685 - accuracy: 0.6154 - val_loss: 0.6481 - val_accuracy: 0.6754\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6683 - accuracy: 0.6154 - val_loss: 0.6474 - val_accuracy: 0.6754\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.6154 - val_loss: 0.6469 - val_accuracy: 0.6754\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6680 - accuracy: 0.6154 - val_loss: 0.6465 - val_accuracy: 0.6754\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6678 - accuracy: 0.6154 - val_loss: 0.6461 - val_accuracy: 0.6754\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6677 - accuracy: 0.6154 - val_loss: 0.6459 - val_accuracy: 0.6754\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6677 - accuracy: 0.6154 - val_loss: 0.6454 - val_accuracy: 0.6754\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6675 - accuracy: 0.6154 - val_loss: 0.6449 - val_accuracy: 0.6754\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6674 - accuracy: 0.6154 - val_loss: 0.6445 - val_accuracy: 0.6754\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6673 - accuracy: 0.6154 - val_loss: 0.6442 - val_accuracy: 0.6754\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6672 - accuracy: 0.6154 - val_loss: 0.6439 - val_accuracy: 0.6754\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6671 - accuracy: 0.6154 - val_loss: 0.6437 - val_accuracy: 0.6754\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6671 - accuracy: 0.6154 - val_loss: 0.6434 - val_accuracy: 0.6754\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.6154 - val_loss: 0.6431 - val_accuracy: 0.6754\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.6154 - val_loss: 0.6430 - val_accuracy: 0.6754\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6669 - accuracy: 0.6154 - val_loss: 0.6429 - val_accuracy: 0.6754\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6669 - accuracy: 0.6154 - val_loss: 0.6427 - val_accuracy: 0.6754\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6668 - accuracy: 0.6154 - val_loss: 0.6425 - val_accuracy: 0.6754\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6668 - accuracy: 0.6154 - val_loss: 0.6422 - val_accuracy: 0.6754\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6667 - accuracy: 0.6154 - val_loss: 0.6423 - val_accuracy: 0.6754\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6668 - accuracy: 0.6154 - val_loss: 0.6421 - val_accuracy: 0.6754\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6667 - accuracy: 0.6154 - val_loss: 0.6418 - val_accuracy: 0.6754\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6667 - accuracy: 0.6154 - val_loss: 0.6416 - val_accuracy: 0.6754\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6414 - val_accuracy: 0.6754\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6414 - val_accuracy: 0.6754\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6416 - val_accuracy: 0.6754\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6415 - val_accuracy: 0.6754\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6413 - val_accuracy: 0.6754\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6412 - val_accuracy: 0.6754\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6411 - val_accuracy: 0.6754\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6411 - val_accuracy: 0.6754\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6411 - val_accuracy: 0.6754\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6412 - val_accuracy: 0.6754\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6412 - val_accuracy: 0.6754\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6412 - val_accuracy: 0.6754\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6409 - val_accuracy: 0.6754\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6408 - val_accuracy: 0.6754\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6407 - val_accuracy: 0.6754\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6406 - val_accuracy: 0.6754\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6403 - val_accuracy: 0.6754\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6403 - val_accuracy: 0.6754\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6403 - val_accuracy: 0.6754\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6403 - val_accuracy: 0.6754\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6400 - val_accuracy: 0.6754\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6398 - val_accuracy: 0.6754\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6398 - val_accuracy: 0.6754\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6397 - val_accuracy: 0.6754\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6395 - val_accuracy: 0.6754\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6395 - val_accuracy: 0.6754\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6395 - val_accuracy: 0.6754\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6395 - val_accuracy: 0.6754\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6394 - val_accuracy: 0.6754\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6394 - val_accuracy: 0.6754\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6395 - val_accuracy: 0.6754\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6393 - val_accuracy: 0.6754\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6393 - val_accuracy: 0.6754\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6393 - val_accuracy: 0.6754\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6392 - val_accuracy: 0.6754\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6390 - val_accuracy: 0.6754\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6388 - val_accuracy: 0.6754\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6388 - val_accuracy: 0.6754\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6388 - val_accuracy: 0.6754\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6387 - val_accuracy: 0.6754\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6387 - val_accuracy: 0.6754\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6386 - val_accuracy: 0.6754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3c6c38820>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = kb.Sequential([\n",
    "    kb.layers.Dense(5, input_shape = (5,), activation = \"relu\"),\n",
    "    kb.layers.Dense(3, activation = \"relu\"),\n",
    "    kb.layers.Dense(2, activation = \"relu\"),\n",
    "    kb.layers.Dense(1, activation = \"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"sgd\", metrics = [\"accuracy\"])\n",
    "model.fit(x_train_compressed, y_train,\n",
    "          epochs = 100,\n",
    "          validation_data = (x_test_compressed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 5)                 30        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 18        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 8         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59\n",
      "Trainable params: 59\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option we have is to add the encoder as a **non-trainable** layer to our neural network. This way the NN will take in raw data, the encoder will compress it, and then pass it along to the next layer of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell our model NOT to change the encoder\n",
    "encoder.trainable = False\n",
    "\n",
    "input = kb.Input(shape = (30,))\n",
    "x = encoder(input)\n",
    "x = kb.layers.Dense(5, input_shape = (5,), activation = \"relu\", name = \"5Dense\")(x)\n",
    "x = kb.layers.Dense(3, activation = \"relu\", name = \"3Dense\")(x)\n",
    "x = kb.layers.Dense(2, activation = \"relu\", name = \"2Dense\")(x)\n",
    "output = kb.layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "\n",
    "model_encoded = kb.Model(inputs = input,\n",
    "                         outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 30)]              0         \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 5)                 155       \n",
      "                                                                 \n",
      " 5Dense (Dense)              (None, 5)                 30        \n",
      "                                                                 \n",
      " 3Dense (Dense)              (None, 3)                 18        \n",
      "                                                                 \n",
      " 2Dense (Dense)              (None, 2)                 8         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 214\n",
      "Trainable params: 59\n",
      "Non-trainable params: 155\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_encoded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "15/15 [==============================] - 1s 15ms/step - loss: 0.6923 - accuracy: 0.6154 - val_loss: 0.6901 - val_accuracy: 0.6754\n",
      "Epoch 2/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.6154 - val_loss: 0.6870 - val_accuracy: 0.6754\n",
      "Epoch 3/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6885 - accuracy: 0.6154 - val_loss: 0.6841 - val_accuracy: 0.6754\n",
      "Epoch 4/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6867 - accuracy: 0.6154 - val_loss: 0.6816 - val_accuracy: 0.6754\n",
      "Epoch 5/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6852 - accuracy: 0.6154 - val_loss: 0.6797 - val_accuracy: 0.6754\n",
      "Epoch 6/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6841 - accuracy: 0.6154 - val_loss: 0.6775 - val_accuracy: 0.6754\n",
      "Epoch 7/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6827 - accuracy: 0.6154 - val_loss: 0.6758 - val_accuracy: 0.6754\n",
      "Epoch 8/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6818 - accuracy: 0.6154 - val_loss: 0.6743 - val_accuracy: 0.6754\n",
      "Epoch 9/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6809 - accuracy: 0.6154 - val_loss: 0.6725 - val_accuracy: 0.6754\n",
      "Epoch 10/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6799 - accuracy: 0.6154 - val_loss: 0.6708 - val_accuracy: 0.6754\n",
      "Epoch 11/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6789 - accuracy: 0.6154 - val_loss: 0.6691 - val_accuracy: 0.6754\n",
      "Epoch 12/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6780 - accuracy: 0.6154 - val_loss: 0.6671 - val_accuracy: 0.6754\n",
      "Epoch 13/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6769 - accuracy: 0.6154 - val_loss: 0.6655 - val_accuracy: 0.6754\n",
      "Epoch 14/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6761 - accuracy: 0.6154 - val_loss: 0.6646 - val_accuracy: 0.6754\n",
      "Epoch 15/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6756 - accuracy: 0.6154 - val_loss: 0.6632 - val_accuracy: 0.6754\n",
      "Epoch 16/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6749 - accuracy: 0.6154 - val_loss: 0.6618 - val_accuracy: 0.6754\n",
      "Epoch 17/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6742 - accuracy: 0.6154 - val_loss: 0.6610 - val_accuracy: 0.6754\n",
      "Epoch 18/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6738 - accuracy: 0.6154 - val_loss: 0.6599 - val_accuracy: 0.6754\n",
      "Epoch 19/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6733 - accuracy: 0.6154 - val_loss: 0.6590 - val_accuracy: 0.6754\n",
      "Epoch 20/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6728 - accuracy: 0.6154 - val_loss: 0.6582 - val_accuracy: 0.6754\n",
      "Epoch 21/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6725 - accuracy: 0.6154 - val_loss: 0.6573 - val_accuracy: 0.6754\n",
      "Epoch 22/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6721 - accuracy: 0.6154 - val_loss: 0.6565 - val_accuracy: 0.6754\n",
      "Epoch 23/200\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.6717 - accuracy: 0.6154 - val_loss: 0.6558 - val_accuracy: 0.6754\n",
      "Epoch 24/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6713 - accuracy: 0.6154 - val_loss: 0.6551 - val_accuracy: 0.6754\n",
      "Epoch 25/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6711 - accuracy: 0.6154 - val_loss: 0.6546 - val_accuracy: 0.6754\n",
      "Epoch 26/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6708 - accuracy: 0.6154 - val_loss: 0.6539 - val_accuracy: 0.6754\n",
      "Epoch 27/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6705 - accuracy: 0.6154 - val_loss: 0.6532 - val_accuracy: 0.6754\n",
      "Epoch 28/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6703 - accuracy: 0.6154 - val_loss: 0.6528 - val_accuracy: 0.6754\n",
      "Epoch 29/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6701 - accuracy: 0.6154 - val_loss: 0.6524 - val_accuracy: 0.6754\n",
      "Epoch 30/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6699 - accuracy: 0.6154 - val_loss: 0.6518 - val_accuracy: 0.6754\n",
      "Epoch 31/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6697 - accuracy: 0.6154 - val_loss: 0.6511 - val_accuracy: 0.6754\n",
      "Epoch 32/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6694 - accuracy: 0.6154 - val_loss: 0.6507 - val_accuracy: 0.6754\n",
      "Epoch 33/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6693 - accuracy: 0.6154 - val_loss: 0.6501 - val_accuracy: 0.6754\n",
      "Epoch 34/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6690 - accuracy: 0.6154 - val_loss: 0.6496 - val_accuracy: 0.6754\n",
      "Epoch 35/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6688 - accuracy: 0.6154 - val_loss: 0.6491 - val_accuracy: 0.6754\n",
      "Epoch 36/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6687 - accuracy: 0.6154 - val_loss: 0.6490 - val_accuracy: 0.6754\n",
      "Epoch 37/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6686 - accuracy: 0.6154 - val_loss: 0.6485 - val_accuracy: 0.6754\n",
      "Epoch 38/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6685 - accuracy: 0.6154 - val_loss: 0.6482 - val_accuracy: 0.6754\n",
      "Epoch 39/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6684 - accuracy: 0.6154 - val_loss: 0.6478 - val_accuracy: 0.6754\n",
      "Epoch 40/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6683 - accuracy: 0.6154 - val_loss: 0.6474 - val_accuracy: 0.6754\n",
      "Epoch 41/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6681 - accuracy: 0.6154 - val_loss: 0.6470 - val_accuracy: 0.6754\n",
      "Epoch 42/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6680 - accuracy: 0.6154 - val_loss: 0.6465 - val_accuracy: 0.6754\n",
      "Epoch 43/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6678 - accuracy: 0.6154 - val_loss: 0.6461 - val_accuracy: 0.6754\n",
      "Epoch 44/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6677 - accuracy: 0.6154 - val_loss: 0.6458 - val_accuracy: 0.6754\n",
      "Epoch 45/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6676 - accuracy: 0.6154 - val_loss: 0.6455 - val_accuracy: 0.6754\n",
      "Epoch 46/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.6154 - val_loss: 0.6451 - val_accuracy: 0.6754\n",
      "Epoch 47/200\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.6674 - accuracy: 0.6154 - val_loss: 0.6448 - val_accuracy: 0.6754\n",
      "Epoch 48/200\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.6673 - accuracy: 0.6154 - val_loss: 0.6444 - val_accuracy: 0.6754\n",
      "Epoch 49/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6672 - accuracy: 0.6154 - val_loss: 0.6440 - val_accuracy: 0.6754\n",
      "Epoch 50/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6671 - accuracy: 0.6154 - val_loss: 0.6439 - val_accuracy: 0.6754\n",
      "Epoch 51/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6671 - accuracy: 0.6154 - val_loss: 0.6436 - val_accuracy: 0.6754\n",
      "Epoch 52/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6671 - accuracy: 0.6154 - val_loss: 0.6434 - val_accuracy: 0.6754\n",
      "Epoch 53/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.6154 - val_loss: 0.6432 - val_accuracy: 0.6754\n",
      "Epoch 54/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.6154 - val_loss: 0.6431 - val_accuracy: 0.6754\n",
      "Epoch 55/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6669 - accuracy: 0.6154 - val_loss: 0.6429 - val_accuracy: 0.6754\n",
      "Epoch 56/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6669 - accuracy: 0.6154 - val_loss: 0.6428 - val_accuracy: 0.6754\n",
      "Epoch 57/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6669 - accuracy: 0.6154 - val_loss: 0.6425 - val_accuracy: 0.6754\n",
      "Epoch 58/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6669 - accuracy: 0.6154 - val_loss: 0.6423 - val_accuracy: 0.6754\n",
      "Epoch 59/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6668 - accuracy: 0.6154 - val_loss: 0.6420 - val_accuracy: 0.6754\n",
      "Epoch 60/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6667 - accuracy: 0.6154 - val_loss: 0.6417 - val_accuracy: 0.6754\n",
      "Epoch 61/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6415 - val_accuracy: 0.6754\n",
      "Epoch 62/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6413 - val_accuracy: 0.6754\n",
      "Epoch 63/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6411 - val_accuracy: 0.6754\n",
      "Epoch 64/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6409 - val_accuracy: 0.6754\n",
      "Epoch 65/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6409 - val_accuracy: 0.6754\n",
      "Epoch 66/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6410 - val_accuracy: 0.6754\n",
      "Epoch 67/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6666 - accuracy: 0.6154 - val_loss: 0.6409 - val_accuracy: 0.6754\n",
      "Epoch 68/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6408 - val_accuracy: 0.6754\n",
      "Epoch 69/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6407 - val_accuracy: 0.6754\n",
      "Epoch 70/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6406 - val_accuracy: 0.6754\n",
      "Epoch 71/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6407 - val_accuracy: 0.6754\n",
      "Epoch 72/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6408 - val_accuracy: 0.6754\n",
      "Epoch 73/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6408 - val_accuracy: 0.6754\n",
      "Epoch 74/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6406 - val_accuracy: 0.6754\n",
      "Epoch 75/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6406 - val_accuracy: 0.6754\n",
      "Epoch 76/200\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6406 - val_accuracy: 0.6754\n",
      "Epoch 77/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6405 - val_accuracy: 0.6754\n",
      "Epoch 78/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6665 - accuracy: 0.6154 - val_loss: 0.6402 - val_accuracy: 0.6754\n",
      "Epoch 79/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6402 - val_accuracy: 0.6754\n",
      "Epoch 80/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6400 - val_accuracy: 0.6754\n",
      "Epoch 81/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6400 - val_accuracy: 0.6754\n",
      "Epoch 82/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6400 - val_accuracy: 0.6754\n",
      "Epoch 83/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6397 - val_accuracy: 0.6754\n",
      "Epoch 84/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6397 - val_accuracy: 0.6754\n",
      "Epoch 85/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6397 - val_accuracy: 0.6754\n",
      "Epoch 86/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6396 - val_accuracy: 0.6754\n",
      "Epoch 87/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6396 - val_accuracy: 0.6754\n",
      "Epoch 88/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6396 - val_accuracy: 0.6754\n",
      "Epoch 89/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6395 - val_accuracy: 0.6754\n",
      "Epoch 90/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6395 - val_accuracy: 0.6754\n",
      "Epoch 91/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6395 - val_accuracy: 0.6754\n",
      "Epoch 92/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6394 - val_accuracy: 0.6754\n",
      "Epoch 93/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6392 - val_accuracy: 0.6754\n",
      "Epoch 94/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6391 - val_accuracy: 0.6754\n",
      "Epoch 95/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6390 - val_accuracy: 0.6754\n",
      "Epoch 96/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6664 - accuracy: 0.6154 - val_loss: 0.6389 - val_accuracy: 0.6754\n",
      "Epoch 97/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6389 - val_accuracy: 0.6754\n",
      "Epoch 98/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6387 - val_accuracy: 0.6754\n",
      "Epoch 99/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6386 - val_accuracy: 0.6754\n",
      "Epoch 100/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6387 - val_accuracy: 0.6754\n",
      "Epoch 101/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6386 - val_accuracy: 0.6754\n",
      "Epoch 102/200\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6386 - val_accuracy: 0.6754\n",
      "Epoch 103/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 104/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 105/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 106/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6385 - val_accuracy: 0.6754\n",
      "Epoch 107/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6385 - val_accuracy: 0.6754\n",
      "Epoch 108/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 109/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 110/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 111/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 112/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 113/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 114/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6380 - val_accuracy: 0.6754\n",
      "Epoch 115/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6380 - val_accuracy: 0.6754\n",
      "Epoch 116/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6380 - val_accuracy: 0.6754\n",
      "Epoch 117/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 118/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 119/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 120/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 121/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 122/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6380 - val_accuracy: 0.6754\n",
      "Epoch 123/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6379 - val_accuracy: 0.6754\n",
      "Epoch 124/200\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6379 - val_accuracy: 0.6754\n",
      "Epoch 125/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6380 - val_accuracy: 0.6754\n",
      "Epoch 126/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6379 - val_accuracy: 0.6754\n",
      "Epoch 127/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6379 - val_accuracy: 0.6754\n",
      "Epoch 128/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6378 - val_accuracy: 0.6754\n",
      "Epoch 129/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6378 - val_accuracy: 0.6754\n",
      "Epoch 130/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6379 - val_accuracy: 0.6754\n",
      "Epoch 131/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6379 - val_accuracy: 0.6754\n",
      "Epoch 132/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 133/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 134/200\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 135/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 136/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 137/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 138/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6385 - val_accuracy: 0.6754\n",
      "Epoch 139/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6385 - val_accuracy: 0.6754\n",
      "Epoch 140/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6385 - val_accuracy: 0.6754\n",
      "Epoch 141/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6385 - val_accuracy: 0.6754\n",
      "Epoch 142/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6386 - val_accuracy: 0.6754\n",
      "Epoch 143/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6386 - val_accuracy: 0.6754\n",
      "Epoch 144/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 145/200\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6386 - val_accuracy: 0.6754\n",
      "Epoch 146/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 147/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 148/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 149/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6385 - val_accuracy: 0.6754\n",
      "Epoch 150/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6386 - val_accuracy: 0.6754\n",
      "Epoch 151/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6386 - val_accuracy: 0.6754\n",
      "Epoch 152/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6387 - val_accuracy: 0.6754\n",
      "Epoch 153/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6387 - val_accuracy: 0.6754\n",
      "Epoch 154/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6387 - val_accuracy: 0.6754\n",
      "Epoch 155/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6388 - val_accuracy: 0.6754\n",
      "Epoch 156/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6387 - val_accuracy: 0.6754\n",
      "Epoch 157/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6387 - val_accuracy: 0.6754\n",
      "Epoch 158/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6387 - val_accuracy: 0.6754\n",
      "Epoch 159/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6386 - val_accuracy: 0.6754\n",
      "Epoch 160/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6385 - val_accuracy: 0.6754\n",
      "Epoch 161/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6385 - val_accuracy: 0.6754\n",
      "Epoch 162/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 163/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 164/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 165/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 166/200\n",
      "15/15 [==============================] - 0s 8ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 167/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 168/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 169/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 170/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 171/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 172/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 173/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 174/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 175/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 176/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6380 - val_accuracy: 0.6754\n",
      "Epoch 177/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 178/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6379 - val_accuracy: 0.6754\n",
      "Epoch 179/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6380 - val_accuracy: 0.6754\n",
      "Epoch 180/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6380 - val_accuracy: 0.6754\n",
      "Epoch 181/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6380 - val_accuracy: 0.6754\n",
      "Epoch 182/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 183/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 184/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 185/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 186/200\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 187/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 188/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 189/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 190/200\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 191/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 192/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 193/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 194/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 195/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6384 - val_accuracy: 0.6754\n",
      "Epoch 196/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 197/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6383 - val_accuracy: 0.6754\n",
      "Epoch 198/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6382 - val_accuracy: 0.6754\n",
      "Epoch 199/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6381 - val_accuracy: 0.6754\n",
      "Epoch 200/200\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.6663 - accuracy: 0.6154 - val_loss: 0.6380 - val_accuracy: 0.6754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe3c6a326a0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = tf.convert_to_tensor(x_train)\n",
    "x_test = tf.convert_to_tensor(x_test)\n",
    "\n",
    "model_encoded.compile(loss = \"binary_crossentropy\",\n",
    "                      optimizer = \"sgd\",\n",
    "                      metrics = [\"accuracy\"])\n",
    "model_encoded.fit(x_train, y_train,\n",
    "          epochs = 200,\n",
    "          validation_data = (x_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! We basically built the same neural nework, except this one does the pre-processing for us using the pre-trained encoder layer from our autoencoder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Convolutional AE\n",
    "\n",
    "Now let's build a de-noising autoencoder. First, we load in our data and add a little bit of noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from keras\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# rescale and reshape data\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
    "\n",
    "# add noise\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "# make sure noise doesnt give us an invalid value outside [0,1]\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's plot some of the noisy and non-noisy data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(1, n + 1):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAABwCAYAAACkaY2RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAb2ElEQVR4nO3de7zNVf7H8XVC5Z67jFsYkmuIamgoj8gtQhl0HSKadHGbeCBUM5TGSKRJIUmJEKGLSyo1jMu45aHGdVwjJZTL+f0xvz4+aznfbZ999nef7/7u1/Ov97LW+e716JzvPvt8W5+10tLT0w0AAAAAAACC5ZLsngAAAAAAAAAuxEMbAAAAAACAAOKhDQAAAAAAQADx0AYAAAAAACCAeGgDAAAAAAAQQDy0AQAAAAAACKCcmRmclpbG+eDZJD09PS0e1+F7mK0Op6enF4vHhfg+Zh/uxVDgXgwB7sVQ4F4MAe7FUOBeDAHuxVDI8F5kpQ2QODuzewIAjDHci0BQcC8CwcC9CARDhvciD20AAAAAAAACiIc2AAAAAAAAAcRDGwAAAAAAgADioQ0AAAAAAEAA8dAGAAAAAAAggHhoAwAAAAAAEEA8tAEAAAAAAAignNk9gVj07dtXcu7cua2+mjVrSu7QoYPnNSZMmCD5iy++sPqmTZuW1SkCAAAAAABkCSttAAAAAAAAAoiHNgAAAAAAAAHEQxsAAAAAAIAASpo9bWbOnCk50l412rlz5zz7evToIblp06ZW3/LlyyXv2rUr2ikiG1WuXNlqb926VXKfPn0kjxs3LmFzSnV58+aVPHr0aMn63jPGmDVr1kju2LGj1bdz506fZgcAAJB4hQoVkly2bNmovsb9PPTYY49J3rhxo+Rt27ZZ49avXx/LFIHAadiwodXWe9JWqVJFcqtWraxxLVu2lLxgwQLP63/++eeSV65cGfM8/cJKGwAAAAAAgADioQ0AAAAAAEAABbY8SpdDGRN9SZQui1m8eLHkChUqWONat24tuWLFilZfly5dJD/77LNRvS6y17XXXmu1dWncnj17Ej0dGGOuvPJKyd27d5fsli3WrVtXsrukcfz48T7NDr+qU6eO5NmzZ1t95cuX9+11b731Vqu9ZcsWybt37/btdREd/TvSGGPmzZsn+eGHH5Y8ceJEa9zZs2f9nVjIFC9eXPLbb78tWS/TNsaYSZMmSd6xY4fv8/pVwYIFrfZNN90kedGiRZJPnz6dsDkByUCXZLRp08bqa9y4seRKlSpFdT237KlcuXKSL7vsMs+vy5EjR1TXB4KiQIECkqdPny755ptvtsadPHlS8qWXXio5X758ntdu1KiRZ5++3okTJ6y+hx56SPKsWbM8r+EnVtoAAAAAAAAEEA9tAAAAAAAAAihQ5VH16tWT3K5dO89xmzZtkuwuOTx8+LDk48ePS9bLpowxZtWqVZJr1apl9RUpUiTKGSMoateubbV/+uknyXPmzEn0dFJSsWLFrPaUKVOyaSbIjGbNmkmOtMQ63tzymwceeEByp06dEjYPnKd/97300kue41588UXJkydPtvr08mJcSJ8aY4z9eUaXIh04cMAal10lUfp0P2Ps93ld2rp9+3b/J5aE9DJ/Y+yS++rVq0t2TzGl3Cy49JYKvXv3lqzLwI0xJnfu3JLT0tKy/LruKalAWP31r3+VrMsMXfoe0yX2hw4dssb98MMPntfQ96Z+LX1tY4x59dVXJbulihs2bPC8fjyx0gYAAAAAACCAeGgDAAAAAAAQQDy0AQAAAAAACKBA7Wmjjwh26z913bfeg2Hfvn1RXfuJJ56w2tdcc43n2AULFkR1TWQvXQ+uj6A1xphp06Ylejop6ZFHHpHctm1bq69+/fqZvp4+TtYYYy655Pxz5fXr10tesWJFpq+N83LmPP/W36JFi2yZg7tXxuOPPy45b968Vp/eowr+0fdf6dKlPcfNmDFD8qlTp3ydUxgULVpU8syZM62+woULS9b7CP3pT3/yf2IeBg8eLPmqq66y+nr06CGZfWwy1qVLF8lPP/201VemTJkMv8bd++a7776L/8QQF/q9sU+fPr6+1tatWyXrv4MQX/rYdf1+bYy9x6o+qt0YY86dOyd54sSJkj/77DNrHO+VkVWrVs1qd+jQIcNxe/bssdr33HOPZP3f+Pvvv7fG6T1uXfrvjCFDhkjWvweNsd+jhw4davV169ZN8tGjRz1fK6tYaQMAAAAAABBAPLQBAAAAAAAIoECVR82fP1+yXqpmjDE//vij5CNHjmT62u4Rsrly5cr0NRAsV199tWS3nMJdgg5/vPDCC5L1MtFY3XHHHZ7tnTt3Sr7rrruscW6pDSJr0qSJ5BtuuEHyqFGjEjYH9+hjXbKaJ08eq4/yKH+4R7wPGjQoqq/T5afp6elxnVMY1alTR7K7vF4bPnx4AmZzIXdpui4nnzNnjtXH79aM6ZKZv/3tb5KLFClijfO6X8aNG2e1dcl3LJ95cXFuGYwuddLlLYsWLbLG/fzzz5KPHTsm2f09pT+XLlmyxOrbuHGj5C+//FLy2rVrrXEnT570vD4yR2+pYIx9j+nPmu7PRbQaNGgg+cyZM1bf119/LXnlypVWn/65++WXX2J67WSXP39+q63fN/V7pj4K3Bhjli1bluXX1n+7DBs2TPKll15qjevbt69kXTJnjDGTJ0+W7OcWK6y0AQAAAAAACCAe2gAAAAAAAAQQD20AAAAAAAACKFB72mh6/4pY9evXT3LlypU9x+l60ozaCKb+/ftLdn9eVq9enejppIyFCxdK1kflxUofbeoey1euXDnJ+ujZr776yhqXI0eOLM8jzNxabn1k8zfffCP5mWeeSdicbr/99oS9FjJWo0YNq123bl3PsbpG/4MPPvBtTmFQvHhxq92+fXvPsX/84x8lHzp0yLc5ufQ+Nh999JHnOHdPG72/IM7T+x3oY9yj5e7T1rx5c8nuseF6/5tU3QMjVpH2malVq5Zkd88KbdWqVZL1flU7duywxpUtW1aye1RxPPYARMZq1qwpuXfv3pLde0wf4azt3bvXan/66aeS//Of/1h9+u8Qvbdi/fr1rXH6PaFFixZW3/r16yXrY8NTibu/njZlyhTJ48ePT8R0jDHGPPnkk1Zb//zov0eMsfdEYk8bAAAAAACAFMNDGwAAAAAAgAAKbHlUrFq1aiVZH5/pHt118OBByX/+85+tvhMnTvg0O2RF+fLlrXa9evUkb9u2zerjaMT4+f3vf2+1q1SpIlkv8Y12ua+7/FMvUdbHZxpjzM033yw50nHEDz30kOQJEyZENY9UMnjwYKutl4jrZfhueVq86SXC7s8Vy8UTL1LZjsstJYC3559/3mp37dpVsl5Cb4wx77zzTkLm5GrUqJHkEiVKWH2vv/665DfeeCNRU0oqunTXGGPuv//+DMdt2LDBah84cEBy06ZNPa9fsGBBybr0yhhjpk+fLnn//v0Xn2wKcz/7v/nmm5J1OZQxdnlwpJJBzS2J0nbt2hXVNZA1L7/8stXWpW2Rju/++OOPJf/73/+W7JbFnDp1yvMaN954o2T9OVQfAW2MMbVr15as3wOMsUt+3n33XcmJLJfNbiNGjPDsC8qWJYsXL5bcs2dPq+/6669PyBxYaQMAAAAAABBAPLQBAAAAAAAIoNCVR+mSGXdZpDZz5kzJy5cv93VOiA+3nEJLpWWEiaBL0d566y2rL9JyU02f6KWXfD711FPWuEjliPoaDz74oORixYpZ40aNGiX58ssvt/pefPFFyadPn77YtEOjQ4cOkt3TCrZv3y45kSet6RI3txxq2bJlkr///vtETSml3XTTTZ597qk0kcoTYUtPT7fa+mf9v//9r9Xn5+k/uXPnttp62X+vXr0ku/N94IEHfJtTWOhyB2OMyZ8/v2R92oz7uUX/fvrDH/4g2S3JqFixouSSJUtafXPnzpV82223ST5y5EhUcw+7fPnySXa3P9BbKBw+fNjqe+655ySzTUKwuJ/r9KlN3bp1s/rS0tIk678N3NL50aNHS451S4UiRYpI1qeYDhs2zBq3aNEiyW5pZaqqUKGC5FKlSll9eqsEXbqWnT755BPJbnlUorDSBgAAAAAAIIB4aAMAAAAAABBAPLQBAAAAAAAIoKTf0+a9996z2rfeemuG46ZOnWq13SNwEXw1atTw7NN7miDrcuY8/9YQ7R427t5QnTp1kuzWjkdL72nz7LPPSh4zZow1Lk+ePJLdn4V58+ZJ/uabb2KaRzLq2LGjZP3fxxhjXnrppYTNQ++P1KVLF8lnz561xo0cOVJyKu09lGj6iFKdXW6N/7p163ybUypp2bKl1dZHqeu9nNz9F6Kl91Bp3Lix1ed1LOmsWbNieq1Udtlll1ltvS/QCy+84Pl1+vjg1157TbJ+vzbG3u/Bpfdb8XNPpGTVtm1byQMHDrT69DHc+th7Y+x9NBAs7ntZv379JOs9bIwxZu/evZLbt28v+auvvorptfVeNWXKlLH69N+WCxculFyoUCHP67nznTZtmuRU2s+va9eukt33O70P5ueff56wOQUdK20AAAAAAAACiIc2AAAAAAAAAZSU5VFXXnmlZHd5t16yqksy9NJ7Y4w5fvy4T7NDPOnl3Pfff7/Vt3btWskffvhhwuaE8/Rx0e4xsbGWRHnRZU66zMYYY6677rq4vlYyKliwoNX2KoUwJvbSi1joo9p1qd2WLVuscUuXLk3YnFJZtPdKIn9Gwmbs2LFWu0mTJpLdo031set62XybNm1iem19Dfcob+3bb7+V7B43jYvTx3W7dAmcW8LvpV69elG/9qpVqyTzWfZCkco+9efGPXv2JGI6iANdomTMheXV2pkzZyQ3aNBAcocOHaxxV199dYZff/LkSatdtWrVDLMx9ufcEiVKeM5JO3DggNVO1dJwvYWCW5ro/g7F/7DSBgAAAAAAIIB4aAMAAAAAABBASVkepXeVLlKkiOe4N954Q3IqnRoTJk2bNpVcuHBhq2/RokWS9YkMiK9LLvF+tquXnvpNL/t35xRpjsOGDZN89913x31eQeGeZvKb3/xG8owZMxI9HVGxYsUM/33jxo0JngmMiVyGEY/Ti2DMmjVrrHbNmjUl165d2+pr3ry5ZH0iyqFDh6xxU6ZMieq19Ukk69ev9xynT+Tg81Hmue+pupxNlyC6JRj6FMx27dpJdk+b0fei29e9e3fJ+vu9efPmqOYedm4ZjKbvt6FDh1p9c+fOlcxpecHyySefWG1dTq3/TjDGmLJly0r++9//LjlSuagut3JLsSLxKok6d+6c1Z4zZ47kRx55xOrbt29f1K8XVlu3brXaK1euzKaZBBsrbQAAAAAAAAKIhzYAAAAAAAABxEMbAAAAAACAAEqaPW10vXCdOnU8xy1btkyyW6+K5FOrVi3Jbj3qrFmzEj2dlNGzZ0/Jbm1udmndurXka6+91urTc3Tnq/e0CbMff/zRauuafL2nhjH2/lBHjhyJ6zyKFy9utb32F6BmOXEaNmwouXPnzp7j9LGbHIcbP0ePHpXsHm2v2wMGDMjya1WoUEGy3gfMGPs9oW/fvll+rVT20UcfWW197+h9a9x9Zrz21XCv17t3b8nvv/++1ffb3/5Wst4fQ//eTmXFihWT7H4e0Hu/DRkyxOobPHiw5IkTJ0rWR6wbY++Zsn37dsmbNm3ynFO1atWs9hdffCGZ99qLc4/h1vtBXXHFFVbfwIEDJf/ud7+T/N1331njdu3aJVn/XOi/O4wxpn79+pme76RJk6z2k08+KVnvV5VK8ubNa7Vz5cqVTTNJXqy0AQAAAAAACCAe2gAAAAAAAARQYMuj3KO89dKySEuq9PLf48ePx39i8F3JkiUlN2rUSPLXX39tjdNH6CG+dClSIullzcYYc80110jW7wGRuEflnj59OusTSwLu8mF9jG/79u2tvgULFkgeM2ZMpl+revXqVluXZJQvX97q8yoHCErZXSrQv08vucT7/9V8+OGHiZgOfKRLPtx7T5dfue+TyBy3rPTOO++UrEu3CxYs6HmNcePGSXZL406dOiV59uzZVp8u/2jWrJnkihUrWuNS9Sj35557TvLjjz8e9dfp98ZevXplmONF3396W4dOnTrF/bXCzi030vdHLKZOnWq1I5VH6bJ0/bP2+uuvW+P0keKpSr9HGmO/Xx0+fDjR08k0vU2L68yZMwmZAyttAAAAAAAAAoiHNgAAAAAAAAHEQxsAAAAAAIAACuyeNk888YTVvu666zIc995771ltjvlOfvfdd59kfXzwBx98kA2zQSINGjTIautjTyPZsWOH5Hvvvdfq08c6phL9Xuge/duyZUvJM2bMyPS13fpjvXdG0aJFo7qGW/MN/3gdu+7uBfDyyy8nYjqIo44dO1rte+65R7Leb8GYC4+8RfzoI7v1/da5c2drnL7n9P5Deg8b14gRI6x21apVJet9FtwjrN3fhalC72kyc+ZMq+/NN9+UnDOn/SdQmTJlJEfa+yse9P59+udFHztujDEjR470dR74n/79+0vOzL5CPXv2lBzLZykEV926da12q1atPMdGu+dmVrHSBgAAAAAAIIB4aAMAAAAAABBAgS2PivaYvocffthqc8x38itXrlyG/3706NEEzwSJsHDhQslVqlSJ6RqbN2+WvHLlyizPKQy2bt0q2T1qsXbt2pIrVaqU6WvrI21dU6ZMsdpdunTJcJx7RDnip3Tp0lbbLdH41Z49e6z26tWrfZsT/HHbbbd59r3//vtW+1//+pff04GxS6V0jpX7XqlLfnR5VJMmTaxxhQsXluweUR5m+nhl9z2tcuXKnl93yy23SM6VK5fkYcOGWeO8tmuIlS5fdksy4J9u3bpJ1mVpbtmctmnTJqs9e/bs+E8M2Ubff+5ziCuuuELyZ599ZvUtXrzY34n9P1baAAAAAAAABBAPbQAAAAAAAAIosOVR0dLLP40x5vTp05m+xrFjxzyvoZdIFixY0PMaetmUMdGXd+llnAMGDLD6Tpw4EdU1wsZrh+758+cneCapSy/XjXSKQqSl+ZMmTZJcqlQpz3H6+ufOnYt2ipbWrVvH9HWpat26dRnmePj222+jGle9enWrvXHjxrjOI5XdeOONVtvrHnZPX0Tycd+Df/rpJ8nPP/98oqeDBHj77bcl6/Kou+66yxqntw8YPny4/xNLch9//HGG/67LiY2xy6POnDkj+bXXXrPGvfLKK5IfffRRq8+rZBX+qV+/vtXW74/58uXz/Dq97YY+LcoYY37++ec4zS789Cmvxlx4umF2yZEjh+S+fftKdt9P9+7dm+E4Y+z3AT+x0gYAAAAAACCAeGgDAAAAAAAQQDy0AQAAAAAACKCk39Nmw4YNWb7GO++8Y7X37dsnuUSJEpLd+rZ4279/v9V++umnfX29oGjYsKHVLlmyZDbNBL+aMGGC5FGjRnmO00fKRtqPJtq9aqIdN3HixKjGIfH0fkgZtX/FHjb+KVKkiGff4cOHJY8dOzYR00Gc6X0V9GcUY4w5ePCgZI74Dif9e1L/fr799tutcUOHDpX81ltvWX3btm3zaXbhs2TJEqutP5vr46G7d+9ujatUqZLkxo0bR/Vae/bsiWGGiIa792H+/PkzHKf3BTPG3jfKPeoZ0Vu6dKnV1nvEFChQwOorWrSoZP2ZJVY1a9aU3KtXL6uvTp06kuvVq+d5ja5du0r+8ssvszynWLDSBgAAAAAAIIB4aAMAAAAAABBAgS2PWrhwodV2l33GU8eOHWP6On3EV6Syjnnz5klevXq157hPP/00pnkku3bt2lltffza2rVrJa9YsSJhc0p1s2fPltyvXz+rr1ixYr697qFDh6z2li1bJD/44IOSdQkjgiU9PT1iG/5r1qyZZ9+uXbskHzt2LBHTQZzp8ij3/lqwYIHn1+lygEKFCknWPxNILuvWrZM8ZMgQq2/06NGSn3nmGavv7rvvlnzy5EmfZhcO+nOIMfaR63feeafn1zVp0sSz7+zZs5L1PTtw4MBYpggP+j2vf//+UX3N9OnTrfayZcviOSVkoGrVqlZ70aJFkuPxef/666+XHG35uP7b3Rhj/vnPf2Z5HlnFShsAAAAAAIAA4qENAAAAAABAAPHQBgAAAAAAIIACu6fNHXfcYbV1LWKuXLmiuka1atUkZ+a47smTJ0vesWOH57h3331X8tatW6O+PozJkyeP5BYtWniOmzVrlmRdAwx/7dy5U3KnTp2svrZt20ru06dPXF/XPeZ+/Pjxcb0+/Hf55Zd79rF3gn/078WKFSt6jjt16pTk06dP+zonJJ7+PdmlSxer77HHHpO8adMmyffee6//E4Pvpk6darV79Ogh2f1MPXz4cMkbNmzwd2JJzv299eijj0rOly+fZPe44OLFi0t2/5aYNm2a5GHDhsVhlviV/p5s3rxZcqS/HfU9oL+/8M+gQYMkDx482OrTx3DHm7sH7ZEjRySPGTNG8l/+8hff5hArVtoAAAAAAAAEEA9tAAAAAAAAAigtM8expqWlcXZrNklPT0+Lx3WC8j3UyxSXL19u9R08eFBy586dJZ84ccL/iflrTXp6er2LD7u4oHwfmzdvLlkfyW2MMa1bt5asj86bNGmSNS4t7fyPtl7Kakwwj6IN270Yb/v377faOXOer8IdMWKE5LFjxyZsThkI3b2YI0cOyf/4xz+svvvuu0+yLqFI9rKYVL0X9THPNWrUsPr0+6n7+e7VV1+VrO/F3bt3x3uKmRG6ezEoypYtK9ktz5kxY4Zkt4wuFql6L2r6GHVj7GOGn3rqKatPf84NkFDci23atJE8d+5cyZH+3r3lllskL1261J+JJUgy3oulSpWy2vrI7+rVq2f5+q+88orktWvXWn0TJ07M8vV9kOG9yEobAAAAAACAAOKhDQAAAAAAQABRHpUkknG5Gy4QiqWnqY57MbL58+dbbb0bf4CWHYf6XnSXGo8cOVLymjVrJCf76Wypei82bNhQsj4FyBhjVqxYIXnChAlW39GjRyX/8ssvPs0u00J9LwbFkiVLrPYNN9wguUGDBpLdEuVopeq9GDKhuBfXr18v2S0f1UaPHi15wIABvs4pkbgXQ4HyKAAAAAAAgGTBQxsAAAAAAIAA4qENAAAAAABAALGnTZKgRjEUQlEvnOq4F0OBezEEuBdDgXsxAQoUKGC19b4fffr0kTxv3ryYrs+9GAqhuBd3794tuXTp0pLdY9Zr164ted++ff5PLEG4F0OBPW0AAAAAAACSBQ9tAAAAAAAAAihndk8AAAAAgD9++OEHq33VVVdl00wAf40ZMybDPGLECGtcmEqikBpYaQMAAAAAABBAPLQBAAAAAAAIIB7aAAAAAAAABBBHficJjnALhVAcp5jquBdDgXsxBLgXQ4F7MQS4F0OBezEEuBdDgSO/AQAAAAAAkgUPbQAAAAAAAAIos0d+HzbG7PRjIoioXByvxfcw+/B9TH58D8OB72Py43sYDnwfkx/fw3Dg+5j8+B6GQ4bfx0ztaQMAAAAAAIDEoDwKAAAAAAAggHhoAwAAAAAAEEA8tAEAAAAAAAggHtoAAAAAAAAEEA9tAAAAAAAAAoiHNgAAAAAAAAHEQxsAAAAAAIAA4qENAAAAAABAAPHQBgAAAAAAIID+D4zkoyWoxrWtAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(1, n + 1):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the DAE\n",
    "Now we need to build our model. Remember, the goal of the encoding portion of the model is to compress our image (meaning we want it's output to be SMALLER than the image).\n",
    "\n",
    "Then we need to create some layers that learn to de-compress the image. We can use any combination of `UpSampling2D()` layers (the opposite of `MaxPooling2D()`), `Conv2D()` layers, or `Conv2DTranspose()` layers (the opposite of `Conv2D()`). What we want to keep in mind is that our **input** needs to be the same dimensions as our **output**. \n",
    "\n",
    "For this example (from Keras), you're going to use `UpSampling2D()` to upscale your compressed image, and a `Conv2D()` layer at the end to get our image back to 1 channel (rather than 32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 118s 62ms/step - loss: 0.1213 - val_loss: 0.1027\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe39d1c7070>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set input to be a 28x28 image\n",
    "input_img = kb.Input(shape=(28, 28, 1))\n",
    "\n",
    "# encoding\n",
    "x = kb.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = kb.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = kb.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = kb.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# At this point the representation is (7, 7, 32)\n",
    "\n",
    "# decoding\n",
    "x = kb.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = kb.layers.UpSampling2D((2, 2))(x)\n",
    "x = kb.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = kb.layers.UpSampling2D((2, 2))(x)\n",
    "decoded = kb.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = kb.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=1,\n",
    "                validation_data=(x_test_noisy, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 28, 28, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 7, 7, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 7, 7, 32)          9248      \n",
      "                                                                 \n",
      " up_sampling2d_4 (UpSampling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 14, 14, 32)        9248      \n",
      "                                                                 \n",
      " up_sampling2d_5 (UpSampling  (None, 28, 28, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 28, 28, 1)         289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,353\n",
      "Trainable params: 28,353\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're not going to use this today, but we COULD if we wanted...\n",
    "# create encoder model\n",
    "encoder_conv = kb.Model(inputs = autoencoder.input,\n",
    "                        outputs = encoded)\n",
    "\n",
    "# get compressed image\n",
    "x_example = encoder_conv(x_train_noisy[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot DAE\n",
    "Now that we've trained the denoising autoencoder, let's see how it does on a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 16s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "# denoise using DAE\n",
    "x_train_pred = autoencoder.predict(x_train_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot denoised images\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(1, n + 1):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(x_train_pred[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABG0AAABwCAYAAACkaY2RAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAaSUlEQVR4nO3df7zO9f3H8fcxOjFOhJVqYQ7jTCFskdFNRDKsRuR3a4xbp9UirQyFiWE7MSG/Ztwmm1/ZNKxEP3CjstvthIYVqfxackQdP3a+f3T7vnq93p3r+Jxzrh+f6zqP+1/Pz97v63O9c53P9eOz9+v9TisoKHAAAAAAAAAIl3KJHgAAAAAAAAC+jps2AAAAAAAAIcRNGwAAAAAAgBDipg0AAAAAAEAIcdMGAAAAAAAghLhpAwAAAAAAEELli9M5LS2N/cETpKCgIC0a5+E1TKgTBQUFNaNxIl7HxOFaTAlciymAazElcC2mAK7FlMC1mAK4FlNCodciM22A+DmY6AEAcM5xLQJhwbUIhAPXIhAOhV6L3LQBAAAAAAAIIW7aAAAAAAAAhBA3bQAAAAAAAEKImzYAAAAAAAAhxE0bAAAAAACAEOKmDQAAAAAAQAhx0wYAAAAAACCEuGkDAAAAAAAQQty0AQAAAAAACCFu2gAAAAAAAIQQN20AAAAAAABCqHyiB5AozZs3l/zAAw+YtgEDBkhevHix5BkzZph+b731VoxGBwAA8KWcnBzJDz74oOTc3FzTr2vXrpIPHjwY+4EBAErspZdekpyWlia5ffv2iRhOqGVlZZlj/Xk3ZMgQyTt27DD93n777Yjn/P3vfy/53LlzpR1iTDHTBgAAAAAAIIS4aQMAAAAAABBCZaY8qmnTpuZ448aNkjMyMkxbQUGB5P79+0vu1q2b6Ve9evVoDhEJcNttt0leunSpaWvXrp3kd999N25jQuFGjx4t+cknnzRt5cp9df/51ltvNW2bN2+O6biAVFGlShXJlStXNm133nmn5Jo1a0qePn266Zefnx+j0ZUtderUMcf9+vWT/L///U9yo0aNTL+GDRtKpjwq8Ro0aGCOK1SoILlt27aSZ82aZfrp17ik1qxZI7l3796mLexlAGGmX8PWrVtL/s1vfmP63XLLLXEbE5LH7373O3Os/4b0khz40tChQyVPnTrVtPnfU/5fvXr1zLH//qfpUqpNmzaVZIhxw0wbAAAAAACAEOKmDQAAAAAAQAildHnU97//fckrVqwwbVdccYVkXQ7lnHOnT5+WrKeQ+uVQN998s2R/J6lUm3qqp/Hqf4dVq1YlYjhR07JlS8n+auNIvEGDBkkeNWqU5KKmjvvXM4Cv6LIbfU0551yrVq0kN27cOND5atWqZY71zkYouePHj5vjLVu2SPZLtZF43/ve9yTrz62ePXuafrqU95prrpHsf6ZF43NM/53Mnj3btD300EOS8/LySv1cZYn+/aDLKY4cOWL6XX311RHbULY8/fTTkn/+85+btvPnz0vWO0nhS3/5y18kP/XUU6YtUnlUcaxcuVLyPffcI3nDhg2lPne0MdMGAAAAAAAghLhpAwAAAAAAEELctAEAAAAAAAihpF/TplKlSub4pptukrxkyRLJft19Ufbt2yd5ypQpkpctW2b6vf7665L1dsTOOTdp0qTAz5cM9DbK9evXl5xsa9roenLnnKtbt67k2rVrm7a0tLS4jAmR6dfk8ssvT+BIyqYf/OAHkvWWw+3atTP99HoOvhEjRkj+6KOPJLdp08b00+/X27dvL/5gIfS2z87Z9Sv69u0ruWLFiqaffs/74IMPTJte601vM92rVy/TT29dvHfv3uIMG8qZM2fMMdt3h5v+ztelS5cEjqRwAwYMMMfz58+XrL/LouT0Gjb+MWvalG16DVS9Zbxzzr322muSly9fHrcxJYtPPvlE8tixY03btGnTJOv7AYcOHTL9rr/++ojnr1q1quTOnTtLZk0bAAAAAAAABMJNGwAAAAAAgBBK+vKoOXPmmOM+ffqU+py6xEpvJ7Z582bTT5cM3XjjjaV+3jDTU2u3bt2awJGUjl8m97Of/UyyLs9wjqn9idChQwdznJ2dXWg//7Xp2rWr5KNHj0Z/YGWE3u7QOedycnIk16hRQ7JfOvjKK69Irlmzpmn77W9/W+hz+efQj+vdu3ewAZdxeuvZyZMnS/ZfxypVqgQ6ny4N7tSpk2nTU7r19af/Lgo7RsnoKdvOOdekSZMEjQRBbNy4UXJR5VHHjh2TrEuU/NJtfwtwrXXr1pL9UlUkDiX1yaVt27aSn3jiCcn+70hdnhOUf47GjRtLPnDggGnTJeQo2uzZs82x3j5df0bm5eWV6PwzZ84s2cDihJk2AAAAAAAAIcRNGwAAAAAAgBDipg0AAAAAAEAIJeWaNs2bN5d85513mrZINaX+ejRr166VPHXqVNOmt6V9++23JZ88edL0a9++/SWfN1X49dbJat68eRHb9HoOiB+99fPChQtNm16zQ/PXSWE73OIpX/6rt/4WLVpIfu6550w/vYXili1bJI8fP97001tWpqenmza9heXtt98ecUw7d+681LDh+fGPfyz5/vvvL/bj/dr6jh07Sva3/M7MzCz2+VFy+tpzrugtS7WWLVtK9tf+4n0ydp599lnJq1evjtjv/Pnzkku6DXRGRobk3Nxcyddcc03Ex/hj4v02+goKCszx5ZdfnqCRIIi5c+dKrl+/vuSsrCzTT3+/Cerxxx83x9WrV5es19J0zrl//etfxT4/vjRhwgTJel2ipk2bluh8l112WanHFEup8UscAAAAAAAgxXDTBgAAAAAAIISSpjxKT3XSWyvqaaLO2emJL774omR/+zW9TeLo0aNNmy6hOX78uGR/CpvektEv09Lbhr/11lsu2fhbmF911VUJGkl0RSq3cc7+XSF+Bg4cKLmo6d16W+nFixfHckgpr1+/fpKLKhnU14TeRrqo7RT97aYjlUQdPnzYHP/xj3+MeE4UrmfPnoH6vf/++5J37NghedSoUaafXxKlNWrUqHiDQ6noMm3nnFu0aJHkcePGRXycbvv0009NW9i3M01mFy5ckFzUdRQNnTp1klytWrVAj/Hfb/Pz86M6JnydLj3etm1bAkeCwpw9e1ay/u1Y0rI2/Tu1du3apk3/XqRsLnr++te/StZlbBs2bDD9brjhhkDn0+VWP/nJT0o5uuhjpg0AAAAAAEAIcdMGAAAAAAAghEJbHtWgQQNzPHLkSMm6xOXEiROm38cffyxZT7f/7LPPTL+///3vheaSqlixojl+5JFHJPft27fU54+3Ll26mGP/vy+Z6NKuunXrRuz34YcfxmM4ZV6NGjXM8X333SdZTyF1zk7v19MWUTz+bk96ZwM9LXjWrFmmny4dLaokStMr+BflwQcfNMe6FBXB6F0ohgwZItmfGrx//37Jx44dK9FzpUqJbLLS13BR5VFIPb179zbH+roP+t1szJgxUR1TWaZL4U6dOiXZL7+vV69e3MaES/O/B+mSmT179kguzm5O3/zmNyXrcmN/9z9dHqdLelA6+vd1kyZNJDdu3LhE5yvJTmHxxEwbAAAAAACAEOKmDQAAAAAAQAhx0wYAAAAAACCEQrWmTXp6uuSpU6eaNr3GyunTpyUPGDDA9Nu5c6fkRK7Dcv311yfsuaPhu9/9bsS2d955J44jKT39t+Svy/Dvf/9bsv67QnTVqVNH8ooVKwI/bsaMGZI3bdoUzSGlPL2GgV7Dxjnnzp07J3n9+vWS/S2gP//880LP7W9Zqbf19t/70tLSJOt1idasWRNx7AhGbwsd63VOWrVqFdPzI7hy5b76/9v8dcCQnPy1Dx977DHJmZmZpq1ChQqBzrlr1y7J58+fL8XooOm19l599VXJXbt2TcRwUIRvf/vbkvVaUM7ZtYkeeOABycVZX2/69OmSe/bsKVl/Njvn3C233BL4nLAaNmwoedWqVaZNvzeWL1/6WxovvPBCqc8RS8y0AQAAAAAACCFu2gAAAAAAAIRQqMqjmjVrJtnfclrr3r275M2bN8d0TPi6HTt2JHoIzjnnMjIyJHfu3Nm09evXT7Iu3fDpLQD1lFdEl359brzxxoj9XnrpJXOck5MTszGlmqpVq5rj4cOHS9bbejtnS6J69OgR6Px6GurSpUtNW/PmzSM+Tm9vOWXKlEDPhdjRW63r7UovRW+Pqr3xxhvmeOvWrSUbGALTJVH+tY3E0CXA/fv3l9yhQ4dAj2/Tpo05Dvq65uXlSdYlVc45t27dOsmRSl2BVKO3e9blNDVq1DD9dPl90N+SI0aMMMeDBg0qtN/EiRMDnQ+X1qhRI8l169Y1bdEoidIefvhhydnZ2VE9dzQw0wYAAAAAACCEuGkDAAAAAAAQQqEqj9KrcOsdR5yzU9fCUhJVVndwuPLKK0v0uCZNmkjWr68/ffi6666TfNlll0n2d1fQ//7+1N/t27dLzs/Pl+xPpXvzzTcDjR3Fp8tunn766Yj9XnvtNckDBw40badOnYr+wFKUvlac+/pUYE2XyHzrW9+SPHjwYNOvW7dukvWU48qVK5t+eiq/P61/yZIlks+cORNxTCidSpUqSc7KyjJtY8eOlVxU6XHQzzS9M4b/N3Px4sVLDxZIcvr90Dm760g8dw/VuxfNnTs3bs+LS6tevXqih5Cy9Hd5vRyCc87Nnz9fclGfaXpHxF/96leS9W9R5+xvHr1DlHP2t8zixYslz5kzp+j/AASmS9weffRR0zZ58mTJ/q6mJVGrVq1SnyOWmGkDAAAAAAAQQty0AQAAAAAACCFu2gAAAAAAAIRQQte06dq1qzlu2rSpZH9dBF0vHBZFbbu5a9eueA8nqvw1YvR/3+zZsyU//vjjgc+pt3rWdaAXLlww/c6ePSt59+7dkhcsWGD67dy5U7K/ztHRo0clHz58WHLFihVNv7179wYaOy5Nb3nqnHMrVqwI9Lj//Oc/kvXrhuI5d+6cOT5+/LjkmjVrmrb33ntPctCtZfU6JnqbWedsHfCJEydM29q1awOdH5dWoUIFc9ysWTPJ+nrz67L1+7l+Hf3tuTt37ixZr5Hj0+sJ3HXXXaYtJydHsv83CaQq/Z3GX5MxCL32hnPB10nU36PvuOMO0/biiy8WexyIHr0mHKKrd+/ekufNm2fa9HcafR3t37/f9GvRokWhuXv37qbftddeK9n/bNXfs+67775AY0fJPfPMM+Z43759kqtWrRrxcfo7y8yZM01bRkZGlEYXe8y0AQAAAAAACCFu2gAAAAAAAIRQQsuj/FIVvWXtsWPHTNvzzz8flzH50tPTJY8bNy5iv5dfftkc6+3jktHw4cPN8cGDByW3bt26ROc8dOiQ5NWrV0ves2eP6bdt27YSnV8bMmSIZF0aoktxEF2jRo0yx0Gndxe1HTiC+/TTT82x3nL9b3/7m2nTW1geOHBA8po1a0y/RYsWSf7kk08kL1u2zPTTU4b9NpSO/lzU5UvOObdy5cpCH/Pkk0+aY/359Prrr0vWfwd+P39LY02/p06aNMm0RXqfd865/Pz8iOdEcEG3Zm/btq059qeFo+Ryc3PN8a233ipZb0G8fv160++LL74o9nP99Kc/NcfZ2dnFPgdiY9OmTZL9JR8QPffcc485XrhwoeTz58+bNv1d6N5775V88uRJ02/atGmS27VrJ1mXSjlnyx39cvIaNWpI/uCDDyTr9wPn7PcsRE/QElD9GmZmZpq2MWPGSNbLtNSuXdv007+DE4WZNgAAAAAAACHETRsAAAAAAIAQ4qYNAAAAAABACCV0TZui+LXvH3/8cdyeW69jM3r0aMkjR440/fRW0ro20jnnPvvssxiNLjEmT56c6CEUy2233Vbo/x50G2oEo+s/b7/99kCP8ddNeffdd6M6Jnxp+/btkv0tv0tCr4+h67+ds+tqsG5U6fjbeuv1afzPIE3Xds+YMcO06Rp//bewbt060++GG26Q7G/XPWXKFMl6vRt/e9SlS5dK/uc//2na9OeIv76AtmvXrohtsNebv8aC5m/HnpWVJXn37t3RH1gZptc7mDhxYlTP7a+nyJo24aHX8PLp9/Iwro+RTIYOHWqO9b/7hAkTTJte76Yo+jqaM2eO5FatWgUel14rRa9vxBo24aLXBtRr2Pj0+kgXL16M6ZhKgpk2AAAAAAAAIcRNGwAAAAAAgBAKbXnUCy+8ELfn0iUeztkp6HqbOb+s4+67747twBB1q1atSvQQUsqGDRskV6tWLWI/vY37oEGDYjkkxEjFihUl+9sM6xINtvwuvm984xuSx48fb9pGjBgh+cyZM6btsccek6z/3f3t3/UWpnrb52bNmpl++/btkzxs2DDTpqd+Z2RkSG7durXp17dvX8ndunUzbRs3bnSF0VulOudc3bp1C+2HL82ePVuyXzZQlCFDhkh+6KGHojomxE6nTp0SPQREcOHChYhtunRGL7uA4vN/f61cuVKy//kRlN6uW5f8+vr06SM5Nzc3Yj+9ZAbCxS+hi2T+/PmSw/h6MtMGAAAAAAAghLhpAwAAAAAAEEIJLY/SUwf94x49epi2X/ziF1F97ocffljyr3/9a9N2xRVXSNY7YQwYMCCqYwCSXfXq1SX7JTParFmzJKfazmplxfr16xM9hJSly1Z0OZRzzp09e1ayXwqjyxNvvvlmyYMHDzb97rjjDsm6zO2pp54y/fSuG0VNOc/Ly5P8j3/8w7TpYz2t3Dnn7r333kLPpz+PcWl79+5N9BDKBH8nN71D4ssvv2zaPv/886g+t76Gc3JyonpuRI8u2/Gvy4YNG0r2yxGHDx8e24GlmGhcA/q3nXPO9ezZU7Iu+fV3flq+fHmpnxv294Jz9vvGn//8Z9PmHxdXrVq1zLH+jlUUXXYXRsy0AQAAAAAACCFu2gAAAAAAAIQQN20AAAAAAABCKKFr2uhtYv3jq6++2rQ988wzkhcsWCD5v//9r+mn6/r79+8vuUmTJqbfddddJ/nQoUOmTa/doNfiQHLSayU1aNDAtOmtqBGMrkMtVy7Yfd833ngjVsNBnLDtbOyMGTMmYpveDnzkyJGmbdy4cZIzMzMDPZd+zKRJk0zbxYsXA50jqGjXqeNLM2bMkJydnW3a6tWrF/Fxem1AfQ5/DYeyrE2bNpKfeOIJ09axY0fJ/rb0Jdl2+Morr5TcpUsX0zZ9+nTJlSpVingOvZbOF198UewxIHr0GmPOOXfttddK/uUvfxnv4cDjryM0bNgwyceOHZPcvn37uI2pLNG/451z7kc/+pFk/7fZRx99JPnDDz+UvH//ftOvefPmhZ7j0UcfNf30mkW+adOmFfq8YcRMGwAAAAAAgBDipg0AAAAAAEAIJbQ8qih6Srhzdlrb3XffLVlvPeqcc/Xr1w90fl2usWnTJtNW1FR1JB9ddhe0nAdfadq0qTnu0KGDZL3N97lz50y/P/zhD5KPHj0ao9EhXr7zne8keggp68iRI5Jr1qxp2tLT0yX7Zb7aunXrJG/ZssW0rV69WvL7778vOdrlUIi/d955xxwXdZ3q92sUbubMmZIbN24csZ8//f706dPFfi5dbnXTTTeZNn/5AO2VV16R/Oyzz0r2v8sisfRr6H8/QnzUrl1b8v3332/a9Oszd+5cyYcPH479wMogXZLrnC0xbdWqlWnT73H6O8vu3btNvx/+8IeSq1SpEvG59Wu9d+9e0zZ27FjJYS8x5RcsAAAAAABACHHTBgAAAAAAIIS4aQMAAAAAABBCCV3TZuvWreZ4x44dklu2bBnxcXo78KuuuipiP70d+LJly0yb3voSZYdfN7lo0aLEDCSJVK1a1Rzr60/T2/I559yIESNiNibE36uvvirZXxuKtTJKp23btpJ79Ohh2vRaF3pbUuecW7BggeSTJ09KZv2EskOvxeCc3UYVsaO3C44Ffa2vXbvWtOnvr2Ffg6Es09sMd+/e3bStWrUq3sMpkzZu3ChZr2/jnHNLliyRrNc1QWxs27bNHOt7AH/6059M26xZsyTXqVOn0Fwc+vtRVlZWic4RBsy0AQAAAAAACCFu2gAAAAAAAIRQQsuj/G3V7rrrLslDhw41baNHjw50zpycHMl6K8T9+/eXZIhIAWlpaYkeApD0cnNzJe/bt8+06W2G69WrZ9qOHz8e24GlAL1dsD9N2D8GNH8L1D179khu1KhRvIeT9AYNGiQ5OzvbtA0cOLDU5z9w4IDks2fPStblp87Zsjf93ovw6tWrlznOz8+XrK9LxM/ChQsljx8/3rStWbMm3sOB8sgjj0hOT083bZUrVy70Mc2aNTPHffr0KbTfqVOnzHHHjh1LMsTQYaYNAAAAAABACHHTBgAAAAAAIITSCgoKgndOSwveGVFVUFAQlRqfsvIa6inOeoeV5557zvTzy/Bi7M2CgoIW0ThRPF9Hf7eo559/XnKbNm0kv/fee6ZfZmZmbAeWIFyL9vpyzrl58+ZJ3rx5s2nTJQZ+KUcCJeW1CItrMSWE9lr0p+zr970JEyaYtmrVqklevXq1ZL17jXO2JOPIkSPRGGYocC1+fZdaXZ7YrVs303bw4MG4jKmYQnstIjiuxZRQ6LXITBsAAAAAAIAQ4qYNAAAAAABACHHTBgAAAAAAIIRY0yZJUKOYEqgXTgFci85lZGSY4+XLl0vu0KGDaVu5cqXkwYMHSz5z5kyMRhcI12IK4FpMCVyLKYBrMSVwLaYArsWUwJo2AAAAAAAAyYKbNgAAAAAAACFUPtEDAAAkl7y8PHPcq1cvyRMnTjRtw4YNkzxu3DjJIdr+GwAAAAgtZtoAAAAAAACEEDdtAAAAAAAAQoibNgAAAAAAACHElt9Jgi3cUgLbKaYArsWUwLWYArgWUwLXYgrgWkwJXIspgGsxJbDlNwAAAAAAQLLgpg0AAAAAAEAIFXfL7xPOuYOxGAiKVDuK5+I1TBxex+THa5gaeB2TH69hauB1TH68hqmB1zH58RqmhkJfx2KtaQMAAAAAAID4oDwKAAAAAAAghLhpAwAAAAAAEELctAEAAAAAAAghbtoAAAAAAACEEDdtAAAAAAAAQoibNgAAAAAAACHETRsAAAAAAIAQ4qYNAAAAAABACHHTBgAAAAAAIIT+D3Vp3aEqmZudAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1440x144 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot true images\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 2))\n",
    "for i in range(1, n + 1):\n",
    "    ax = plt.subplot(1, n, i)\n",
    "    plt.imshow(x_train[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Because we did this the simple way and added noise to our dataset, the noise for a given sample will never change. We also could add a `GaussianNoise()` layer to our model in order to change the noise each time, sorta like the data augmentation layers that we used before.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 96s 51ms/step - loss: 0.1244 - val_loss: 0.1118\n",
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_11 (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " gaussian_noise_10 (Gaussia  (None, 28, 28, 1)         0         \n",
      " nNoise)                                                         \n",
      "                                                                 \n",
      " conv2d_38 (Conv2D)          (None, 14, 14, 32)        160       \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 7, 7, 32)          4128      \n",
      "                                                                 \n",
      " conv2d_transpose_12 (Conv2  (None, 14, 14, 32)        9248      \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " conv2d_transpose_13 (Conv2  (None, 28, 28, 32)        9248      \n",
      " DTranspose)                                                     \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 28, 28, 1)         289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23073 (90.13 KB)\n",
      "Trainable params: 23073 (90.13 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# W = 28 # input height/width\n",
    "# K = 3 # kernel size\n",
    "# P = 0 # padding\n",
    "# S = 1 # stride\n",
    "\n",
    "# # output dimensions\n",
    "# ((W - K + P + S)/S)\n",
    "\n",
    "# set input to be a 28x28 image\n",
    "input_img = kb.Input(shape=(28, 28, 1))\n",
    "\n",
    "# encoding\n",
    "#28,28,1 --> 28,28,32 --> 14,14,32 --> 14,14,32 --> 7,7,32\n",
    "x = kb.layers.GaussianNoise(0.5)(input_img)\n",
    "x = kb.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = kb.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = kb.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = kb.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# # encoding alternate\n",
    "# #28,28,1 --> 14,14,32 --> 7,7,32\n",
    "# x = kb.layers.GaussianNoise(0.5)(input_img)\n",
    "# x = kb.layers.Conv2D(32, (2, 2), strides = (2,2), activation='relu', padding='valid')(x)\n",
    "# encoded = kb.layers.Conv2D(32, (2, 2), strides = (2,2), activation='relu', padding='valid')(x)\n",
    "\n",
    "\n",
    "# At this point the representation is (7, 7, 32)\n",
    "\n",
    "# # decoding\n",
    "# # 7,7,32 --> 7,7,32 --> 14,14,32 --> 14,14,32 --> 28,28,32 --> 28,28,1\n",
    "# x = kb.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "# x = kb.layers.UpSampling2D((2, 2))(x)\n",
    "# x = kb.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "# x = kb.layers.UpSampling2D((2, 2))(x)\n",
    "# decoded = kb.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# decoding alternate\n",
    "# 7,7,32 -->  14,14,32 --> 28,28,32 --> 28,28,1\n",
    "x = kb.layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(encoded)\n",
    "x = kb.layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "decoded = kb.layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "\n",
    "# Autoencoder\n",
    "\n",
    "autoencoder = kb.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=1,\n",
    "                validation_data=(x_test_noisy, x_test))\n",
    "\n",
    "autoencoder.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
