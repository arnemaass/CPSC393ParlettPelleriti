{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WewHwyGObcTB"
      },
      "source": [
        "# LSTM's and GRUs\n",
        "\n",
        "While Vanilla/Simple RNNs are a great way to understand the ideas behind Recurrent architechtures, they're not often used in real life. \n",
        "\n",
        "Rather, we use updated recurrent architechtures like the LSTM and GRU.\n",
        "\n",
        "\n",
        "## LSTM Architecture\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=17_HVigdnak1bEobOLS6Rbxq8jNYB_bLf\" alt=\"Q\" width = \"400\"/>\n",
        "\n",
        "- How many states does an LSTM output and take in at each step? How is this different from a simple RNN?\n",
        "- What range of values can a **gate** output?\n",
        "- What range of values can a **tanh** output?\n",
        "- What does the forget gate do, conceptually?\n",
        "- What does the input gate do, conceptually?\n",
        "- What does the output gate do, conceptually?\n",
        "- What does the input *node* do, conceptually?\n",
        "\n",
        "## GRU Architecture\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1cXxAo66BbhJ_Z4tni5JllfhkNZoNYcG6\" alt=\"Q\" width = \"400\"/>\n",
        "\n",
        "- How many states does a GRU output and take in at each step?\n",
        "- How many gates does a GRU have?\n",
        "- What does the reset gate do?\n",
        "- What does the update gate do?\n",
        "- How do we generate the proposed update to the hidden state?\n",
        "\n",
        "## The Vanishing Gradient\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=110Hz0GQdzxunzi0qIgGqLSxCzFftcApF\" alt=\"Q\" width = \"400\"/>\n",
        "\n",
        "- When a Gradient Vanishes, it approaches *what value*?\n",
        "- Why are deeper networks more susceptible to the vanishing gradient problem?\n",
        "- If a recurrent layer has only ONE CELL, why is IT susceptible to the vanishing gradient problem?\n",
        "- How does sequence length affect the vanishing gradient problem?\n",
        "- What range of values can the derivative of a sigmoid activation function take on? Why is this problematic?\n",
        "- For the simple NN in the image above, write out the different partial derivatives. I'll start you off with $$\\frac{\\partial Loss}{\\partial \\text{output}} = -2(y - \\text{output})$$ When our weights are very small (0<abs(weight)<1), what will happen to our gradient?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cgIO7PH7kNsa"
      },
      "source": [
        "# Building a Character Level LSTM and GRU\n",
        "\n",
        "Activity based on [this](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/) by Jason Brownlee."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_9B1Xc4jVv28"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XUxzISQkSdw"
      },
      "source": [
        "## Load in Pride and Prejudice\n",
        "\n",
        "ðŸ“š First, let's load in a txt file of the book **Pride and Prejudice**\n",
        "\n",
        "(if you'd like to use your own book, try looking here on [Project Gutenberg](https://www.gutenberg.org/ebooks/). Make sure you set `FileType` to be a Plain Text (.txt) file to make your life easier, and delete the Header and License at the top and bottom of the file.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TTizYdB5VyWY"
      },
      "outputs": [],
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"pandp.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WxufAunBV5iH",
        "outputId": "db11279c-8bc5-47de-c95e-2c2d3964c8a4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'chapter 1\\n\\n      it is a truth universally acknowledged, that a single man in\\n      possession of a '"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_text[0:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bspQCJvdkYZI"
      },
      "source": [
        "## Map Characters to One Hot Vectors\n",
        "\n",
        "As we'll discuss in the next lecture, our NN's cannot understand text, it needs numbers as input, so we're going to change our characters (e.g. 'a', 'b', 'c') to one hot encoded vectors\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1nyNRjEBl99luyPbF-ZCw03_Ki_6h4u0W\" alt=\"Q\" width = \"400\"/>\n",
        "\n",
        "As we talked about in our Math Review lecture, One Hot Encoded Vectors are sparse (they have a lot of 0's) vectors where we use 1's to indicate group membership. Very similar to dummy variables we learned about in CPSC 392."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "7j_9BY2OV_dD"
      },
      "outputs": [],
      "source": [
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZyHXwf5WC9e",
        "outputId": "90b8bbe8-ce80-41b0-968b-4ec533db5782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Characters:  754870\n",
            "Total Vocab:  56\n"
          ]
        }
      ],
      "source": [
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWXZB-cVWMoT",
        "outputId": "e3c1e8dc-611f-4428-b246-68ddef626dfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Patterns:  754770\n"
          ]
        }
      ],
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100 # 100 characters as input\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        " seq_in = raw_text[i:i + seq_length] # generate 100 character input\n",
        " seq_out = raw_text[i + seq_length] # grab next character\n",
        " \n",
        " dataX.append([char_to_int[char] for char in seq_in])\n",
        " dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MNra9bwKWaNT"
      },
      "outputs": [],
      "source": [
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKaxure5Woka",
        "outputId": "76fb7fe8-285c-412b-ca42-ddd3e27d9dea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "print(y[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmC_K3vxmGdY"
      },
      "source": [
        "## Build Recurrent Model (LSTM)\n",
        "\n",
        "[ðŸ’» Server Docs](https://docs.google.com/document/d/1YuMv84E6mYLyPFiVCarKAMbMFT4H0rEqEspcGTCQv28/edit?usp=sharing)\n",
        "\n",
        "The code below builds and saves an LSTM model. However, this model takes a really long time to train, so I reccomend either:\n",
        "\n",
        "1. Load my Pre-Trained Model (available on Canvas) **if** you're using Pride and Prejudice data. \n",
        "\n",
        "2. Train your Model on the Server\n",
        "\n",
        "  - **Change** your model path so that you replace `pandpmodel` with whatever you want to call your model\n",
        "  - **Download** this notebook as a .py file, then delete any code in the file that happens AFTER the line where you save your model\n",
        "  - **Move** your .py file and your book .txt file to the server and put it in your run directory (the file you list in your .yaml file)\n",
        "  - Using a docker container, **run** your python file\n",
        "  - When finished, **exit** the container. Your saved model files should be in your home directory, in whichever folder you use as your run directory. Use `scp` or VSCode to download the file locally. \n",
        "  - **Zip** your file\n",
        "  - **Upload** to Colab!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "3qIQJGd2WqiW"
      },
      "outputs": [],
      "source": [
        "# LSTM model\n",
        "inputs = Input(shape = (X.shape[1], X.shape[2]))\n",
        "x = LSTM(256)(inputs)\n",
        "x = Dropout(0.2)(x)\n",
        "output = Dense(y.shape[1], activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs = inputs, outputs = output)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj6k5lcmagRG"
      },
      "source": [
        "### Saving a Model when Running on the Server\n",
        "When saving a model that will be run on the server, your filepath in `model.save()` needs to be a filepath that will be local to your *container* (not your local computer, nor your home directory on the server). The way our docker containers are set up, if you want to write to the run directory, you'll need to write to the folder `'/app/rundir/...'` where `...` is your file or directory name. \n",
        "\n",
        "When saving a keras model, you provide the name of a *directory* that will be created and that all your model files will save to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "3HWQ9NoIW_SW"
      },
      "outputs": [],
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128)\n",
        "model.save('/app/rundir/pandpmodel') # save model to file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZsOD4PemaoC"
      },
      "source": [
        "### Loading a Pre-Trained Model into Colab\n",
        "If uploading a pre-trained model, upload the .zip to Colab, and replace `pandpmodel.zip` with your model name.\n",
        "\n",
        "Then Load the model by replacing `'pandpmodel/` with your model file name in `tf.keras.models.load_model()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp5GO5hXiW6o",
        "outputId": "15ae0a0d-82ed-4389-f606-19418b0c47e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  pandpmodel.zip\n",
            "replace __MACOSX/._pandpmodel? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/._pandpmodel   \n",
            "replace __MACOSX/pandpmodel/._variables? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: __MACOSX/pandpmodel/._variables  \n",
            "replace pandpmodel/saved_model.pb? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: pandpmodel/saved_model.pb  \n",
            "  inflating: __MACOSX/pandpmodel/._assets  \n",
            "  inflating: pandpmodel/variables/variables.data-00000-of-00001  \n",
            "  inflating: pandpmodel/variables/variables.index  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"
          ]
        }
      ],
      "source": [
        "# unzips model that you uploaded to colab\n",
        "!unzip pandpmodel.zip\n",
        "\n",
        "# loads model from files\n",
        "model = tf.keras.models.load_model('pandpmodel/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab2e16l2oKdR"
      },
      "source": [
        "### Using a Pre-Trained Model to Make Predictions\n",
        "We'll need a dictionary to map the one hot encoded vectors BACK to characters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "hx2wJ9LlXc1s"
      },
      "outputs": [],
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WBE6YBKoRep"
      },
      "source": [
        "Next, we'll generate data!\n",
        "\n",
        "You can change `n_chars` to determine how many individual characters our model should product (remember we used *characters* as our tokens rather than words for this model).\n",
        "\n",
        "We then pick a random section of text to be our input sequence, and we'll feed it to the trained model in order to get our generated text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffRkvW5kXgGx",
        "outputId": "990d142e-4bb4-4500-dbe1-761521b350d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed:\n",
            "\" erfect\n",
            "      unconcern, â€œoh! but there were two or three much uglier in the\n",
            "      shop; and when i h \"\n",
            "------------------------------------------------------------\n",
            "ave not been to be ao aotaos of the mott\n",
            "      conpenting oo the sore of the matter of the matter wh\n",
            "------------------------------------------------------------\n",
            "\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "n_chars = 100\n",
        "\n",
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "print(\"------------------------------------------------------------\")\n",
        "\n",
        "# generate characters\n",
        "for i in range(n_chars):\n",
        " x = np.reshape(pattern, (1, len(pattern), 1))\n",
        " x = x / float(n_vocab)\n",
        "\n",
        " # get model's prediction (as a one hot vec)\n",
        " prediction = model.predict(x, verbose=0) # predicted probs\n",
        " index = np.argmax(prediction) # find highest prop\n",
        "\n",
        " # use one hots to grab actual characters\n",
        " result = int_to_char[index]\n",
        "\n",
        " # string sequence together\n",
        " seq_in = [int_to_char[value] for value in pattern]\n",
        "\n",
        " # write sequence to console\n",
        " print(result, end = \"\")\n",
        "\n",
        " # store pattern\n",
        " pattern.append(index)\n",
        " pattern = pattern[1:len(pattern)]\n",
        "\n",
        "print(\"\\n------------------------------------------------------------\")\n",
        "print(\"\\nDone.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErjbRC_Nb5tv"
      },
      "source": [
        "# On Your Own\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
        "\n",
        "Now that you know how to build a Character Level LSTM, try implementing any of updates:\n",
        "\n",
        "- Try training your model with a different book, or a set of books (e.g. the tiny shakespeare data set, or the Illiad, or the bible, or frankenstein!)\n",
        "- Add DEEP LSTM architechture by adding another (or a few) extra LSTM layers. Remember you need to put the `return_sequence = True` argument in ALL Recurrent Layers except the last one.\n",
        "- Replace the LSTM layer with a GRU layer and compare the performance\n",
        "- Generate a looooong string of output using your model (make `n_char` big, like `2000`). Do you see any weird behavior?\n",
        "- Discuss with your group some of the benefits, and some of the drawbacks of training a recurrent model on *character tokens* rather than *word tokens*."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
