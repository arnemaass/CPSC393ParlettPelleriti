{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import tensorflow.keras as kb\n",
    "from tensorflow.keras import backend\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression # Linear Regression Model\n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n",
    "from sklearn.model_selection import LeaveOneOut #LOO cv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "In the lecture we talked about different methods for optimizing our loss function\n",
    "\n",
    "- **Gradient Descent**\n",
    "- Gradient Descent with **Momentum**\n",
    "- **AdaGrad**\n",
    "- **RMSP**\n",
    "- **Adam**\n",
    "\n",
    "All of these are (or are based off of) the basic updating rule from Gradient Desent:\n",
    "\n",
    "$$ w_t = w_{t-1} - \\alpha * g_t $$\n",
    "\n",
    "Which says that the new weights ($w_t$) are the old weights ($w_{t-1}$) minus some adjustment which is the product of the learning rate ($\\alpha$) and the gradient ($g_t$). \n",
    "\n",
    "\n",
    "**Momentum** affects the gradient part of the update rule. Rather than updating based on just the current gradient, we update based on the *moving average* of past gradients. This allows us to \"build up\" speed as we \"roll\" down the gradient, but also smooths out any osscilating steps that we might take. \n",
    "\n",
    "$$ w_t = w_{t-1} - \\alpha * m_t $$\n",
    "$$ m_t = \\beta * m_{t-1} + (1 - \\beta) * g_t$$\n",
    "\n",
    "**AdaGrad** and **RMSP** both affect the learning rate part of the update rule. Both optimizers allow us to\n",
    "\n",
    "1. use different learning rates for different weights/parameters\n",
    "2. **ada**pt the learning rate throughout the process\n",
    "\n",
    "AdaGrad:\n",
    "$$ w_t = w_{t-1} - \\frac{\\alpha}{\\sqrt{\\epsilon + \\sum g_t^2}} * g_t $$\n",
    "\n",
    "RMSP:\n",
    "$$ w_t = w_{t-1} - \\frac{\\alpha}{\\sqrt{\\epsilon + \\nu_t}} * g_t $$\n",
    "$$ \\nu_t = \\beta * \\nu_{t-1} + (1 - \\beta) * g_t^2$$\n",
    "AdaGrad updates the learning rate based on the sum of the squared past gradients, whereas RMSP updates the learning rate based on the moving average of the past squared gradients. \n",
    "\n",
    "**Adam** combines the changes Momentum makes to the gradient part of the update rule, and the changes RMSP makes to the learning rate. It also unbiases the momentum and learning rate parameters so that they are not as strongly affected by the fact that we initialize the past gradients to be 0. \n",
    "\n",
    "$$ w_t = w_{t-1} - \\frac{\\alpha}{\\sqrt{\\epsilon + \\hat{\\nu_t}}} * \\hat{m_t} $$\n",
    "$$ \\nu_t = \\beta_2 * \\nu_{t-1} + (1 - \\beta_2) * g_t^2$$\n",
    "$$m_t = \\beta_1 * m_{t-1} + (1 - \\beta_1) * g_t$$\n",
    "$$ \\hat{\\nu_t} = \\frac{\\nu_t}{1 - \\beta_2^t},\\; \\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "\n",
    "## Using Different Optimizers in Keras/Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previously\n",
    "\n",
    "In a past lecture, we looked at a python implementation of Gradient Descent for a simple linear regression model using a Sum of Square Errors loss function. The function below, `stepGradient()` takes in four arguments:\n",
    "- `b0_current`: the current value for the `b0` intercept parameter\n",
    "- `b1_current`: the current value for the `b1` slope parameter\n",
    "- `point` a DataFrame of the points we're using to claculate the gradient\n",
    "- `learningRate` a constant value representing the learning rate (how big of a step we should take at each step)\n",
    "\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "Take a moment to study this function and map it to the process that we learned about for gradient descent. Call this function using `b0_current` = 0, `b1_current` = 0, `points` = `df`, and `learningRate` = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepGradient(b0_current, b1_current, points, learningRate):\n",
    "    # initialize gradient to 0\n",
    "    b0_gradient = 0\n",
    "    b1_gradient = 0\n",
    "\n",
    "    # for each data point, calculate gradient and add \n",
    "    for i in range(0, len(points)):\n",
    "        b0_gradient += -2 * (points.iloc[i].y - ((b1_current*points.iloc[i].x) + b0_current))\n",
    "        b1_gradient += -2 * points.iloc[i].x * (points.iloc[i].y - ((b1_current * points.iloc[i].x) + b0_current))\n",
    "\n",
    "    b0_gradient = b0_gradient/points.shape[0]\n",
    "    b1_gradient = b1_gradient/points.shape[0]\n",
    "    # update parameter values\n",
    "    new_b0 = b0_current - (learningRate * b0_gradient)\n",
    "    new_b1 = b1_current - (learningRate * b1_gradient)\n",
    "    return [np.round(new_b0,5), np.round(new_b1,5)]\n",
    "\n",
    "#based on https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame \n",
    "np.random.seed(1234)\n",
    "x = np.random.normal(loc = 0, scale = 1, size = 100)\n",
    "y = 1.67 + 0.67*x + np.random.normal(loc = 0, scale = 0.2, size = 100)\n",
    "df = pd.DataFrame({\"x\": x, \"y\": y})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "# run 500 updates\n",
    "for i in range(0,500):\n",
    "  a,b = stepGradient(a,b, df, 0.1)\n",
    "\n",
    "  # every 10 updates, print the current parameter values\n",
    "  if i%10 == 0:\n",
    "    print(a,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "Now that we've learned about momentum, let's modify our `stepGradient()` function to incorporate momentum. \n",
    "\n",
    "Remember momentum is calculated based on a decaying sum of the past gradients\n",
    "\n",
    "$$w_t = w_{t-1} - \\alpha * m_t$$\n",
    "$$m_t = \\beta * m_{t-1} + (1-\\beta)* g_t$$\n",
    "\n",
    "This new `stepMomentum()` function will need to take in **three** additional arguments:\n",
    "\n",
    "- `b0_mt`: $m_t$; the sum of the gradients for `b0` from the previous step\n",
    "- `b1_mt`: $m_t$; the sum of the gradients for `b1` from the previous step\n",
    "- `beta`: moving average parameter that controlls how much of the previous gradient is remembered (set to a default value of `0.9`)\n",
    "\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "I have added these arguments to the function definition for you. Take a moment to look at the lecture slides and understand the difference between Gradient Descent and Gradient Descent *with* momentum. \n",
    "\n",
    "Then, modify the function below (which contains the same code as the `stepGradient()` function) to implement Gradient Descent *with* momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# Change this function so it does GD with momentum, right now it just does GD\n",
    "\n",
    "def stepMomentum(b0_current, b1_current, points, learningRate):\n",
    "    # initialize gradient to 0\n",
    "    b0_gradient = 0\n",
    "    b1_gradient = 0\n",
    "\n",
    "    # for each data point, calculate gradient and add \n",
    "    for i in range(0, len(points)):\n",
    "        b0_gradient += -2 * (points.iloc[i].y - ((b1_current*points.iloc[i].x) + b0_current))\n",
    "        b1_gradient += -2 * points.iloc[i].x * (points.iloc[i].y - ((b1_current * points.iloc[i].x) + b0_current))\n",
    "\n",
    "    b0_gradient = b0_gradient/points.shape[0]\n",
    "    b1_gradient = b1_gradient/points.shape[0]\n",
    "    # update parameter values\n",
    "    new_b0 = b0_current - (learningRate * b0_gradient)\n",
    "    new_b1 = b1_current - (learningRate * b1_gradient)\n",
    "    return [np.round(new_b0,5), np.round(new_b1,5)]\n",
    "\n",
    "#based on https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it Out\n",
    "\n",
    "Once you've created and tested your `stepMomentum()` function, try it out with this dataset, `df` and compare it to the output of the `stepGradient()` function on the same data.\n",
    "\n",
    "Initialize `b0_mt` and `b1_mt` both to 0.\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "What's different about the updates you got?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "What do you think the potential downside of initializing `b0_mt` and `b1_mt` to `0` is?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "\n",
    "We also learned about an upgraded version of Gradient Descent called **AdaGrad**. AdaGrad updates the learning rate for each parameter separately by scaling a baseline learning rate, $\\alpha$ by the square root of the sum of the previous gradients.\n",
    "\n",
    "Similarly to what we did with momentum, update the code from the `stepGradient()` function below to implement AdaGrad. \n",
    "\n",
    "The AdaGrad will need two extra arguments:\n",
    "- `b0_squared_gradient_sum`: the sum of the previous squared gradients for `b0`\n",
    "- `b1_squared_gradient_sum`: the sum of the previous squared gradients for `b1`\n",
    "\n",
    "(Set $\\epsilon$ to be 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepAda(b0_current, b1_current, points, learningRate,b0_squared_gradient_sum, b1_squared_gradient_sum ):\n",
    "    pass\n",
    "    # COPY THE GRADIENT DESCENT FUNCTION AND ALTER IT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "\n",
    "We also learned about another algorithm, RMSProp which:\n",
    "\n",
    "- scales the learning rate by the moving average of the squared past gradients\n",
    "- tailors the learning rate for each parameter separately\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepRMSP(b0_current, b1_current, points, learningRate,b0_squared_gradient_sum, b1_squared_gradient_sum, beta ):\n",
    "    # initialize gradient to 0\n",
    "    b0_gradient = 0\n",
    "    b1_gradient = 0\n",
    "\n",
    "    # for each data point, calculate gradient and add \n",
    "    for i in range(0, len(points)):\n",
    "        b0_gradient += (1/10000) * -2 * (points.iloc[i].y - ((b1_current*points.iloc[i].x) + b0_current))\n",
    "        b1_gradient += (1/10000) * -2 * points.iloc[i].x * (points.iloc[i].y - ((b1_current * points.iloc[i].x) + b0_current))\n",
    "    b0_gradient = b0_gradient/points.shape[0]\n",
    "    b1_gradient = b1_gradient/points.shape[0]\n",
    "    \n",
    "    # new learning rates\n",
    "    b0_squared_gradient_sum = (beta *b0_squared_gradient_sum) + ((1-beta) * b0_gradient**2)\n",
    "    b1_squared_gradient_sum = (beta *b1_squared_gradient_sum) + ((1-beta) * b1_gradient**2)\n",
    "\n",
    "    b0_learningRate = learningRate/(np.sqrt(0.0001 + b0_squared_gradient_sum))\n",
    "    b1_learningRate = learningRate/(np.sqrt(0.0001 + b1_squared_gradient_sum))\n",
    "\n",
    "    \n",
    "    # update parameter values\n",
    "    new_b0 = b0_current - (b0_learningRate * b0_gradient)\n",
    "    new_b1 = b1_current - (b1_learningRate * b1_gradient)\n",
    "    return [np.round(new_b0,5), np.round(new_b1,5), b0_squared_gradient_sum, b1_squared_gradient_sum]\n",
    "#based on https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "Finally, we learned about Adam, which combines Momentum and RMSP.\n",
    "\n",
    "I won't make you figure this one out on your own, but the code is below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepAdam(b0_current, b1_current, points, learningRate,\n",
    "             b0_squared_gradient_sum, b1_squared_gradient_sum,\n",
    "             b0_mt, b1_mt, beta1, beta2,t ):\n",
    "    # initialize gradient to 0\n",
    "    b0_gradient = 0\n",
    "    b1_gradient = 0\n",
    "\n",
    "    # for each data point, calculate gradient and add \n",
    "    for i in range(0, len(points)):\n",
    "        b0_gradient += (1/10000) * -2 * (points.iloc[i].y - ((b1_current*points.iloc[i].x) + b0_current))\n",
    "        b1_gradient += (1/10000) * -2 * points.iloc[i].x * (points.iloc[i].y - ((b1_current * points.iloc[i].x) + b0_current))\n",
    "    \n",
    "    b0_gradient = b0_gradient/points.shape[0]\n",
    "    b1_gradient = b1_gradient/points.shape[0]\n",
    "\n",
    "    # calculate mts\n",
    "    b0_mt = (beta1*b0_mt) + ((1-beta1)*b0_gradient)\n",
    "    b1_mt = (beta1*b1_mt) + ((1-beta1)*b1_gradient)\n",
    "\n",
    "    # unbias these estimates\n",
    "    b0_mt_unbiased = b0_mt/(1-beta1**t)\n",
    "    b1_mt_unbiased = b1_mt/(1-beta1**t)\n",
    "\n",
    "    # new learning rates\n",
    "    b0_squared_gradient_sum = (beta2 *b0_squared_gradient_sum) + ((1-beta2) * b0_gradient**2)\n",
    "    b1_squared_gradient_sum = (beta2 *b1_squared_gradient_sum) + ((1-beta2) * b1_gradient**2)\n",
    "\n",
    "    # unbias these estimates\n",
    "    b0_squared_gradient_sum_unbiased = b0_squared_gradient_sum/(1-beta2**t)\n",
    "    b1_squared_gradient_sum_unbiased = b1_squared_gradient_sum/(1-beta2**t)\n",
    "\n",
    "    b0_learningRate = learningRate/(np.sqrt(0.0001 + b0_squared_gradient_sum_unbiased))\n",
    "    b1_learningRate = learningRate/(np.sqrt(0.0001 + b1_squared_gradient_sum_unbiased))\n",
    "\n",
    "    \n",
    "    # update parameter values\n",
    "    new_b0 = b0_current - (b0_learningRate * b0_mt_unbiased)\n",
    "    new_b1 = b1_current - (b1_learningRate * b1_mt_unbiased)\n",
    "    return [np.round(new_b0,5), np.round(new_b1,5), b0_squared_gradient_sum, b1_squared_gradient_sum, b0_mt, b1_mt]\n",
    "#based on https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (Comparison)\n",
    "\n",
    "Just for comparison, since this is a simple model, let's see what Linear Regression comes up with for this model!\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "How close did our various methods of Gradient Descent get to Linear Regression's parameter estimates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(df[[\"x\"]], df[\"y\"])\n",
    "print(lr.intercept_, lr.coef_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
