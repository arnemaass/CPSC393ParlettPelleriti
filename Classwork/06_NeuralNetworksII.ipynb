{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 09:50:41.465234: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.datasets import mnist\n",
    "import tensorflow.keras as kb\n",
    "from tensorflow.keras import backend\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression # Linear Regression Model\n",
    "from sklearn.preprocessing import StandardScaler #Z-score variables\n",
    "\n",
    "from sklearn.model_selection import train_test_split # simple TT split cv\n",
    "from sklearn.model_selection import KFold # k-fold cv\n",
    "from sklearn.model_selection import LeaveOneOut #LOO cv\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "In the lecture we talked about different methods for optimizing our loss function\n",
    "\n",
    "- **Gradient Descent**\n",
    "- Gradient Descent with **Momentum**\n",
    "- **AdaGrad**\n",
    "- **RMSP**\n",
    "- **Adam**\n",
    "\n",
    "All of these are (or are based off of) the basic updating rule from Gradient Desent:\n",
    "\n",
    "$$ w_t = w_{t-1} - \\alpha * g_t $$\n",
    "\n",
    "Which says that the new weights ($w_t$) are the old weights ($w_{t-1}$) minus some adjustment which is the product of the learning rate ($\\alpha$) and the gradient ($g_t$). \n",
    "\n",
    "\n",
    "**Momentum** affects the gradient part of the update rule. Rather than updating based on just the current gradient, we update based on the *moving average* of past gradients. This allows us to \"build up\" speed as we \"roll\" down the gradient, but also smooths out any osscilating steps that we might take. \n",
    "\n",
    "$$ w_t = w_{t-1} - \\alpha * m_t $$\n",
    "$$ m_t = \\beta * m_{t-1} + (1 - \\beta) * g_t$$\n",
    "\n",
    "**AdaGrad** and **RMSP** both affect the learning rate part of the update rule. Both optimizers allow us to\n",
    "\n",
    "1. use different learning rates for different weights/parameters\n",
    "2. **ada**pt the learning rate throughout the process\n",
    "\n",
    "AdaGrad:\n",
    "$$ w_t = w_{t-1} - \\frac{\\alpha}{\\sqrt{\\epsilon + \\sum g_t^2}} * g_t $$\n",
    "\n",
    "RMSP:\n",
    "$$ w_t = w_{t-1} - \\frac{\\alpha}{\\sqrt{\\epsilon + \\nu_t}} * g_t $$\n",
    "$$ \\nu_t = \\beta * \\nu_{t-1} + (1 - \\beta) * g_t^2$$\n",
    "AdaGrad updates the learning rate based on the sum of the squared past gradients, whereas RMSP updates the learning rate based on the moving average of the past squared gradients. \n",
    "\n",
    "**Adam** combines the changes Momentum makes to the gradient part of the update rule, and the changes RMSP makes to the learning rate. It also unbiases the momentum and learning rate parameters so that they are not as strongly affected by the fact that we initialize the past gradients to be 0. \n",
    "\n",
    "$$ w_t = w_{t-1} - \\frac{\\alpha}{\\sqrt{\\epsilon + \\hat{\\nu_t}}} * \\hat{m_t} $$\n",
    "$$ \\nu_t = \\beta_2 * \\nu_{t-1} + (1 - \\beta_2) * g_t^2$$\n",
    "$$m_t = \\beta_1 * m_{t-1} + (1 - \\beta_1) * g_t$$\n",
    "$$ \\hat{\\nu_t} = \\frac{\\nu_t}{1 - \\beta_2^t},\\; \\hat{m_t} = \\frac{m_t}{1 - \\beta_1^t}$$\n",
    "\n",
    "## Using Different Optimizers in Keras/Tensorflow\n",
    "\n",
    "Luckily, using different optimizers in Keras is simple! In our `model.compile()` step, we just use the `optimizer = ` argument to choose the optimizer we want to use. In the past we've done things like `kb.optimizers.SGD()` which uses `keras`' Stochastic Gradient Descent optimizer (annoyingly, thought it's CALLED SGD, it actually does minibatch GD by default). We can also give the `optimizer = ` argument a string like `\"SGD\"` or `\"adam\"` instead of giving it an optimizer object. This will use all the default settings (e.g. learning rate, or the $\\beta$ parameter for calculating the decaying average), but typically those will be fine. If you do want more control over the hyperparameters for your optimizer, then definitely use an optimizer object like `kb.optimizers.SGD()` or `kb.optimizers.Adam()`. [Learn more here](https://keras.io/api/optimizers/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2553, 14)\n",
      "Epoch 1/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.9185\n",
      "Epoch 2/100\n",
      "64/64 [==============================] - 0s 929us/step - loss: 0.1249\n",
      "Epoch 3/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0702\n",
      "Epoch 4/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0525\n",
      "Epoch 5/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0444\n",
      "Epoch 6/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0401\n",
      "Epoch 7/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0378\n",
      "Epoch 8/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0363\n",
      "Epoch 9/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0354\n",
      "Epoch 10/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0349\n",
      "Epoch 11/100\n",
      "64/64 [==============================] - 0s 959us/step - loss: 0.0346\n",
      "Epoch 12/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0344\n",
      "Epoch 13/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0342\n",
      "Epoch 14/100\n",
      "64/64 [==============================] - 0s 974us/step - loss: 0.0341\n",
      "Epoch 15/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 16/100\n",
      "64/64 [==============================] - 0s 971us/step - loss: 0.0340\n",
      "Epoch 17/100\n",
      "64/64 [==============================] - 0s 937us/step - loss: 0.0340\n",
      "Epoch 18/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 19/100\n",
      "64/64 [==============================] - 0s 944us/step - loss: 0.0340\n",
      "Epoch 20/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 21/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 22/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 23/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 24/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 25/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 26/100\n",
      "64/64 [==============================] - 0s 989us/step - loss: 0.0339\n",
      "Epoch 27/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 28/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 29/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 30/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 31/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 32/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 33/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 34/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 35/100\n",
      "64/64 [==============================] - 0s 850us/step - loss: 0.0339\n",
      "Epoch 36/100\n",
      "64/64 [==============================] - 0s 858us/step - loss: 0.0340\n",
      "Epoch 37/100\n",
      "64/64 [==============================] - 0s 919us/step - loss: 0.0339\n",
      "Epoch 38/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 39/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 40/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 41/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 42/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 43/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 44/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 45/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 46/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 47/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 48/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 49/100\n",
      "64/64 [==============================] - 0s 970us/step - loss: 0.0339\n",
      "Epoch 50/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 51/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 52/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 53/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 54/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 55/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 56/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 57/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 58/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 59/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 60/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 61/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 62/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 63/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 64/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 65/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 66/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 67/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 68/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 69/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 70/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 71/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 72/100\n",
      "64/64 [==============================] - 0s 991us/step - loss: 0.0339\n",
      "Epoch 73/100\n",
      "64/64 [==============================] - 0s 956us/step - loss: 0.0339\n",
      "Epoch 74/100\n",
      "64/64 [==============================] - 0s 978us/step - loss: 0.0339\n",
      "Epoch 75/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 76/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 77/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 78/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 79/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 80/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 81/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 82/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 83/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 84/100\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 85/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 86/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 87/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0340\n",
      "Epoch 88/100\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0339\n",
      "Epoch 89/100\n",
      "64/64 [==============================] - 0s 998us/step - loss: 0.0339\n",
      "Epoch 90/100\n",
      "64/64 [==============================] - 0s 868us/step - loss: 0.0339\n",
      "Epoch 91/100\n",
      "64/64 [==============================] - 0s 800us/step - loss: 0.0339\n",
      "Epoch 92/100\n",
      "64/64 [==============================] - 0s 907us/step - loss: 0.0339\n",
      "Epoch 93/100\n",
      "64/64 [==============================] - 0s 982us/step - loss: 0.0339\n",
      "Epoch 94/100\n",
      "64/64 [==============================] - 0s 881us/step - loss: 0.0339\n",
      "Epoch 95/100\n",
      "64/64 [==============================] - 0s 850us/step - loss: 0.0340\n",
      "Epoch 96/100\n",
      "64/64 [==============================] - 0s 851us/step - loss: 0.0340\n",
      "Epoch 97/100\n",
      "64/64 [==============================] - 0s 842us/step - loss: 0.0339\n",
      "Epoch 98/100\n",
      "64/64 [==============================] - 0s 894us/step - loss: 0.0339\n",
      "Epoch 99/100\n",
      "64/64 [==============================] - 0s 867us/step - loss: 0.0339\n",
      "Epoch 100/100\n",
      "64/64 [==============================] - 0s 821us/step - loss: 0.0339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f935cae5c40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From last class\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/cmparlettpelleriti/CPSC392ParlettPelleriti/master/Data/Music_data.csv\")\n",
    "feats = [\"danceability\", \"energy\", \"loudness\",\"acousticness\", \"instrumentalness\", \"liveness\", \"duration_ms\"]\n",
    "predict = \"valence\"\n",
    "\n",
    "print(df.shape)\n",
    "X = df[feats]\n",
    "y = df[predict]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2)\n",
    "\n",
    "z = StandardScaler()\n",
    "X_train[feats] = z.fit_transform(X_train[feats])\n",
    "X_test[feats] = z.transform(X_test[feats])\n",
    "\n",
    "#structure of the model\n",
    "model = kb.Sequential([\n",
    "    kb.layers.Dense(1, input_shape =[7]), #input\n",
    "])\n",
    "\n",
    "#how to train the model\n",
    "model.compile(loss = \"mean_squared_error\",\n",
    "              optimizer = kb.optimizers.SGD())\n",
    "\n",
    "#fit the model (same as SKlearn)\n",
    "model.fit(X_train,y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tried a few different optimizers, Use the documentation linked [here](https://keras.io/api/optimizers/) to figure out how to use RMSProp and AdaGrad as optimizers for your model. \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previously\n",
    "\n",
    "In a past lecture, we looked at a python implementation of Gradient Descent for a simple linear regression model using a Sum of Square Errors loss function. The function below, `stepGradient()` takes in four arguments:\n",
    "- `b0_current`: the current value for the `b0` intercept parameter\n",
    "- `b1_current`: the current value for the `b1` slope parameter\n",
    "- `point` a DataFrame of the points we're using to claculate the gradient\n",
    "- `learningRate` a constant value representing the learning rate (how big of a step we should take at each step)\n",
    "\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "Take a moment to study this function and map it to the process that we learned about for gradient descent. Call this function using `b0_current` = 0, `b1_current` = 0, `points` = `df`, and `learningRate` = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepGradient(b0_current, b1_current, points, learningRate):\n",
    "    # initialize gradient to 0\n",
    "    b0_gradient = 0\n",
    "    b1_gradient = 0\n",
    "\n",
    "    # for each data point, calculate gradient and add \n",
    "    for i in range(0, len(points)):\n",
    "        b0_gradient += -2 * (points.iloc[i].y - ((b1_current*points.iloc[i].x) + b0_current))\n",
    "        b1_gradient += -2 * points.iloc[i].x * (points.iloc[i].y - ((b1_current * points.iloc[i].x) + b0_current))\n",
    "\n",
    "    b0_gradient = b0_gradient/points.shape[0]\n",
    "    b1_gradient = b1_gradient/points.shape[0]\n",
    "    # update parameter values\n",
    "    new_b0 = b0_current - (learningRate * b0_gradient)\n",
    "    new_b1 = b1_current - (learningRate * b1_gradient)\n",
    "    return [np.round(new_b0,5), np.round(new_b1,5)]\n",
    "\n",
    "#based on https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame \n",
    "np.random.seed(1234)\n",
    "x = np.random.normal(loc = 0, scale = 1, size = 100)\n",
    "y = 1.67 + 0.67*x + np.random.normal(loc = 0, scale = 0.2, size = 100)\n",
    "df = pd.DataFrame({\"x\": x, \"y\": y})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "# run 500 updates\n",
    "for i in range(0,500):\n",
    "  a,b = stepGradient(a,b, df, 0.1)\n",
    "\n",
    "  # every 10 updates, print the current parameter values\n",
    "  if i%10 == 0:\n",
    "    print(a,b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "Now that we've learned about momentum, let's modify our `stepGradient()` function to incorporate momentum. \n",
    "\n",
    "Remember momentum is calculated based on a decaying sum of the past gradients\n",
    "\n",
    "$$w_t = w_{t-1} - \\alpha * m_t$$\n",
    "$$m_t = \\beta * m_{t-1} + (1-\\beta)* g_t$$\n",
    "\n",
    "This new `stepMomentum()` function will need to take in **three** additional arguments:\n",
    "\n",
    "- `b0_mt`: $m_t$; the sum of the gradients for `b0` from the previous step\n",
    "- `b1_mt`: $m_t$; the sum of the gradients for `b1` from the previous step\n",
    "- `beta`: moving average parameter that controlls how much of the previous gradient is remembered (set to a default value of `0.9`)\n",
    "\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "I have added these arguments to the function definition for you. Take a moment to look at the lecture slides and understand the difference between Gradient Descent and Gradient Descent *with* momentum. \n",
    "\n",
    "Then, modify the function below (which contains the same code as the `stepGradient()` function) to implement Gradient Descent *with* momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# Change this function so it does GD with momentum, right now it just does GD\n",
    "\n",
    "def stepMomentum(b0_current, b1_current, points, learningRate):\n",
    "    # initialize gradient to 0\n",
    "    b0_gradient = 0\n",
    "    b1_gradient = 0\n",
    "\n",
    "    # for each data point, calculate gradient and add \n",
    "    for i in range(0, len(points)):\n",
    "        b0_gradient += -2 * (points.iloc[i].y - ((b1_current*points.iloc[i].x) + b0_current))\n",
    "        b1_gradient += -2 * points.iloc[i].x * (points.iloc[i].y - ((b1_current * points.iloc[i].x) + b0_current))\n",
    "\n",
    "    b0_gradient = b0_gradient/points.shape[0]\n",
    "    b1_gradient = b1_gradient/points.shape[0]\n",
    "    # update parameter values\n",
    "    new_b0 = b0_current - (learningRate * b0_gradient)\n",
    "    new_b1 = b1_current - (learningRate * b1_gradient)\n",
    "    return [np.round(new_b0,5), np.round(new_b1,5)]\n",
    "\n",
    "#based on https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it Out\n",
    "\n",
    "Once you've created and tested your `stepMomentum()` function, try it out with this dataset, `df` and compare it to the output of the `stepGradient()` function on the same data.\n",
    "\n",
    "Initialize `b0_mt` and `b1_mt` both to 0.\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "What's different about the updates you got?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "What do you think the potential downside of initializing `b0_mt` and `b1_mt` to `0` is?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "\n",
    "We also learned about an upgraded version of Gradient Descent called **AdaGrad**. AdaGrad updates the learning rate for each parameter separately by scaling a baseline learning rate, $\\alpha$ by the square root of the sum of the previous gradients.\n",
    "\n",
    "Similarly to what we did with momentum, update the code from the `stepGradient()` function below to implement AdaGrad. \n",
    "\n",
    "The AdaGrad will need two extra arguments:\n",
    "- `b0_squared_gradient_sum`: the sum of the previous squared gradients for `b0`\n",
    "- `b1_squared_gradient_sum`: the sum of the previous squared gradients for `b1`\n",
    "\n",
    "(Set $\\epsilon$ to be 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepAda(b0_current, b1_current, points, learningRate,b0_squared_gradient_sum, b1_squared_gradient_sum ):\n",
    "    pass\n",
    "    # COPY THE GRADIENT DESCENT FUNCTION AND ALTER IT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "\n",
    "We also learned about another algorithm, RMSProp which:\n",
    "\n",
    "- scales the learning rate by the moving average of the squared past gradients\n",
    "- tailors the learning rate for each parameter separately\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepRMSP(b0_current, b1_current, points, learningRate,b0_squared_gradient_sum, b1_squared_gradient_sum, beta ):\n",
    "    # initialize gradient to 0\n",
    "    b0_gradient = 0\n",
    "    b1_gradient = 0\n",
    "\n",
    "    # for each data point, calculate gradient and add \n",
    "    for i in range(0, len(points)):\n",
    "        b0_gradient += (1/10000) * -2 * (points.iloc[i].y - ((b1_current*points.iloc[i].x) + b0_current))\n",
    "        b1_gradient += (1/10000) * -2 * points.iloc[i].x * (points.iloc[i].y - ((b1_current * points.iloc[i].x) + b0_current))\n",
    "    b0_gradient = b0_gradient/points.shape[0]\n",
    "    b1_gradient = b1_gradient/points.shape[0]\n",
    "    \n",
    "    # new learning rates\n",
    "    b0_squared_gradient_sum = (beta *b0_squared_gradient_sum) + ((1-beta) * b0_gradient**2)\n",
    "    b1_squared_gradient_sum = (beta *b1_squared_gradient_sum) + ((1-beta) * b1_gradient**2)\n",
    "\n",
    "    b0_learningRate = learningRate/(np.sqrt(0.0001 + b0_squared_gradient_sum))\n",
    "    b1_learningRate = learningRate/(np.sqrt(0.0001 + b1_squared_gradient_sum))\n",
    "\n",
    "    \n",
    "    # update parameter values\n",
    "    new_b0 = b0_current - (b0_learningRate * b0_gradient)\n",
    "    new_b1 = b1_current - (b1_learningRate * b1_gradient)\n",
    "    return [np.round(new_b0,5), np.round(new_b1,5), b0_squared_gradient_sum, b1_squared_gradient_sum]\n",
    "#based on https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "Finally, we learned about Adam, which combines Momentum and RMSP.\n",
    "\n",
    "I won't make you figure this one out on your own, but the code is below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepAdam(b0_current, b1_current, points, learningRate,\n",
    "             b0_squared_gradient_sum, b1_squared_gradient_sum,\n",
    "             b0_mt, b1_mt, beta1, beta2,t ):\n",
    "    # initialize gradient to 0\n",
    "    b0_gradient = 0\n",
    "    b1_gradient = 0\n",
    "\n",
    "    # for each data point, calculate gradient and add \n",
    "    for i in range(0, len(points)):\n",
    "        b0_gradient += (1/10000) * -2 * (points.iloc[i].y - ((b1_current*points.iloc[i].x) + b0_current))\n",
    "        b1_gradient += (1/10000) * -2 * points.iloc[i].x * (points.iloc[i].y - ((b1_current * points.iloc[i].x) + b0_current))\n",
    "    \n",
    "    b0_gradient = b0_gradient/points.shape[0]\n",
    "    b1_gradient = b1_gradient/points.shape[0]\n",
    "\n",
    "    # calculate mts\n",
    "    b0_mt = (beta1*b0_mt) + ((1-beta1)*b0_gradient)\n",
    "    b1_mt = (beta1*b1_mt) + ((1-beta1)*b1_gradient)\n",
    "\n",
    "    # unbias these estimates\n",
    "    b0_mt_unbiased = b0_mt/(1-beta1**t)\n",
    "    b1_mt_unbiased = b1_mt/(1-beta1**t)\n",
    "\n",
    "    # new learning rates\n",
    "    b0_squared_gradient_sum = (beta2 *b0_squared_gradient_sum) + ((1-beta2) * b0_gradient**2)\n",
    "    b1_squared_gradient_sum = (beta2 *b1_squared_gradient_sum) + ((1-beta2) * b1_gradient**2)\n",
    "\n",
    "    # unbias these estimates\n",
    "    b0_squared_gradient_sum_unbiased = b0_squared_gradient_sum/(1-beta2**t)\n",
    "    b1_squared_gradient_sum_unbiased = b1_squared_gradient_sum/(1-beta2**t)\n",
    "\n",
    "    b0_learningRate = learningRate/(np.sqrt(0.0001 + b0_squared_gradient_sum_unbiased))\n",
    "    b1_learningRate = learningRate/(np.sqrt(0.0001 + b1_squared_gradient_sum_unbiased))\n",
    "\n",
    "    \n",
    "    # update parameter values\n",
    "    new_b0 = b0_current - (b0_learningRate * b0_mt_unbiased)\n",
    "    new_b1 = b1_current - (b1_learningRate * b1_mt_unbiased)\n",
    "    return [np.round(new_b0,5), np.round(new_b1,5), b0_squared_gradient_sum, b1_squared_gradient_sum, b0_mt, b1_mt]\n",
    "#based on https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression (Comparison)\n",
    "\n",
    "Just for comparison, since this is a simple model, let's see what Linear Regression comes up with for this model!\n",
    "\n",
    "### Question\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1ghyQPx1N8dmU3MV4TrANvqNhGwnLni72\" alt=\"Q\" width = \"200\"/>\n",
    "\n",
    "How close did our various methods of Gradient Descent get to Linear Regression's parameter estimates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(df[[\"x\"]], df[\"y\"])\n",
    "print(lr.intercept_, lr.coef_[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
